> [!note] 
> - 不知道搞什么方向
> - 看了一下前沿的论文都在干什么

# CVPR-2025

## Reconstructing Humans with a Biomechanically Accurate Skeleton

方向：人体重建

主要贡献：

- 提出了 HSMR，这是首个端到端的方法 (作者说的)，能够通过估计生物力学骨骼模型 SKEL 的参数，从单张图像中重建人体的三维结构。
- 给出了一种在没有任何图像与 SKEL 真值配对数据集的情况下，如何生成训练所需的数据，并引入了一种迭代优化伪真值质量的流程。
- 揭示了基于简化人体模型（如 SMPL）参数回归方法的局限性：这类方法往往预测出不自然的关节旋转，导致生物力学上不准确的重建结果。

## CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos

方向：具身智能

主要工作： We leverage thousands of hours of online city walking and driving videos to train autonomous agents for robust, generalizable navigation in urban environments through scalable, data-driven imitation learning.  

我们利用数千小时的在线城市步行和驾驶视频，通过可扩展、数据驱动的模仿学习，训练自主智能体在城市环境中实现稳健、可通用的导航。

Pipeline：

1. 从互联网获取视频数据，利用 visual odometry 获取帧间相对位姿。
2. 在每个时间步，模型接收过去的观测数据、过去的轨迹和目标数据位置作为输入。
3. 输入经过冻结的图片编码器和可训练的**坐标编码器** (Coordinate Encoder) 被编码。
4. Transformer 处理这些输入来生成预测的 Tokens。 
5. Action Head 和 Arrival Head 将 Tokens 解码为预测的 Action 和到达的状态。
6. 在训练过程中，来自未来帧的真实帧 token 用于引导 Transformer 生成对应的未来 token。

主要贡献：

- 提出了一种可扩展的、数据驱动的解决方案，利用网络上大规模的城市步行与驾驶视频。
- 引入了一种简单而可扩展的数据处理范式，使得在无需大量人工标注的情况下，能够对城市导航任务进行大规模的模仿学习。
- 通过实验证明，在网络大规模数据上进行训练显著提升了真实场景中的导航性能，使具身智能体能够应对城市环境的复杂性。文章还展示了模型性能随训练数据规模的增加而良好地提升。

## SpiritSight Agent: Advanced GUI Agent with One Look

方向：多模态

简单描述一下这篇文章做了什么：微调 VLM(20B/26B) 实现较高精度的 GUI 互动和导航。

# AAAI-2025

## PixelMan: Consistent Object Editing with Diffusion Models via Pixel Manipulation and Generation

**方向**：图像编辑 / 扩散模型 / 无需训练与反演的高效编辑方法

**主要工作**：We propose PixelMan, an inversion-free and training-free method that achieves consistent object editing by directly manipulating pixels and leveraging a three-branched sampling framework to guide diffusion models. Our approach enables high-fidelity, harmonized edits within just 16 inference steps while preserving object identity and background coherence.

我们提出 PixelMan，一种无需反演（inversion-free）且无需额外训练（training-free）的方法，通过像素级操作与三分支采样机制引导预训练扩散模型，在仅 16 步内实现高保真、视觉协调的对象编辑，同时保持对象身份与背景一致性。

链接：[[03-Paper/PixelMan|PixelMan]]

> [!note] 巧思型，没有复杂训练没有新架构，但是想法很好，效果也很好

# ICCV-2025

## SpatialCrafter: Unleashing the Imagination of Video Diffusion Models for Scene Reconstruction from Limited Observations

**方向**：3D 场景重建 / 视频扩散模型 / 稀疏视角新视角合成

主要工作：We present SpatialCrafter, a framework that leverages the rich physical-world priors embedded in pre-trained video diffusion models to enable high-quality 3D scene reconstruction and novel view synthesis from extremely limited observations—even a single or just two input views. By integrating geometric constraints, depth priors, and a hybrid Mamba-Transformer architecture, our method significantly reduces ambiguity in sparse-view reconstruction while maintaining photorealism and 3D consistency.

我们提出 SpatialCrafter，一种利用预训练视频扩散模型中蕴含的物理世界先验知识的框架，仅需极稀疏观测（甚至单视角或首尾两帧）即可实现高质量 3D 场景重建与逼真新视角合成。通过融合几何约束、单目深度先验与 Mamba-Transformer 混合架构，显著降低稀疏重建的歧义性，同时保证视觉真实感与三维一致性。

链接：[[03-Paper/SpatialCrafter|SpatialCrafter]]

> [!note] 这个算力夸张，没法参考

# 综述

- A Survey on 3D Gaussian Splatting (2025)
- A Survey of Multimodal Learning: Methods, Applications, and Future (2025)
- 3D Gaussian Splatting: Survey, Technologies, Challenges, and Opportunities (2025)