---
tags:
  - 深度学习
---
# 注意力机制

灵长类动物的视觉系统接受了大量的感官输入，这些感官输入远远超过了大脑能够完全处理的程度。然而，并非所有刺激的影响都是相等的。意识的聚集和专注使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体，例如猎物和天敌。只关注一小部分信息的能力对进化更加有意义，使人类得以生存和成功。

## 注意力提示

注意力是一种稀缺的资源，而环境中的干扰注意力的信息不少。人类的视觉神经系统大约每秒接受 $10^8$ 位信息，这远远超出了大脑能够完全处理的能力。但是人类的祖先已经能够从经验中认识到并非所有的输入都是一样的。这种只将注意力引向一小部分感兴趣的信息的能力，是人类的大脑能够更加明治的分配资源来生存的能力。

### 生物学中的注意力提示

现今广泛采用的双组件（two-component）框架是注意力研究的基础，其起源可以追溯到19世纪90年代的心理学家威廉·詹姆斯，他被认为是美国心理学之父。在这个框架中，受试者通过非自主性和自主性提示，有选择地引导注意力的焦点。

非自主性提示是基于环境中物体的突出性和易见性。例如，我们通常会自然地将注意力集中在环境中最突出的物体上。

自主性提示是受到了认识和意识的控制，因此注意力在基于自主性提示来进行辅助选择时将会更加谨慎。在受试者的主观意志推动下，选择的作用也就更大。

### 查询、键和值

自主性提示与非自主性提示解释了人类的注意力的方式，下面来看看神经网络如何实现注意力的实现。

在以往的神经网络中，卷积、全连接、池化等操作都是非自主性提示的实现方式。而注意力机制则是考虑了自主性提示与非自主性提示。非自主性提示被称为查询 (query)，在给定任意查询，注意力机制通过注意力汇聚 (attention pooling) 将选择引导至感官输入 (sensory input)，例如中间特征表示。在注意力机制中，这些感官输入被称为值 (value)。更加通俗的解释，每个值都与一个键 (key) 匹配，这可以想象为感官输入的非自主性提示。可以通过注意力汇聚的方式，便于给定的查询 (自主性提示) 与键 (非自主性提示) 进行匹配，这将引导得出最匹配的值 (感官输入)。

上述框架是注意力机制的中心，但是我们也可以使用其他的方式来设计注意力机制。例如我们可以设计一个不可微的注意力模型，然后使用强化学习的方法进行训练。

## 注意力机制的本质

上述描述的注意力机制过于抽象了，下面从另一种方式来描述注意力机制。注意力机制主要是针对 Encoder-Decoder 架构提出。以机器翻译为例，在 Encoder-Decoder 架构中，由于传递的仅仅为一个上下文向量，序列较长的时候所有的语义仅仅由一个语义向量表示，而单词自身的信息已经损失过多，因此很多的细节信息会被丢失。此时我们就需要注意力机制。

在注意力机制中，主要有三个元素，分别是
- 查询 (自主性提示)：查询 Q 是一个向量，可以理解为用于查询的向量。通常可以将当前时间步的隐状态或者输出作为 Q 来使用。在 self-attention 中，Q 通过输入的线性变换得到。
- 键 (非自主性提示)：键 k 通常通过输入的线性变换得到，用于衡量查询 Q 与值 V 的相似度，从而计算注意力权重。
- 值 (感官输入)：值 V 也是通过输入的线性变换得到的，我们通过计算每一个时间步中输入的值 V，最后通过与注意力权重 $\alpha$ 相乘相加并得到最终输出。

这几个元素的关系可以用下面的图来说明：
![[Pasted image 20230716153135.png]]

对于经典的注意力机制模型，我们有下面的结构：
![[Pasted image 20230716153613.png]]


### 注意力的可视化

我们可以使用热力图可视化注意力权重，其中热力图的 y 轴为查询 (自主性提示)，x 轴为键 (非自主性提示)。这里定义的函数的输入为 (子图行数、子图列数、Q 数、K 数)，使用的是 `d2l.torch` 包中的函数。

```python
def show_heatmap(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5), cmap='Reds'):
    """Show the heatmap of matrices"""
    num_rows, num_cols, _, _ = matrices.shape
    fig = plt.figure()
    if num_cols == 1 and num_rows == 1:
        axes = [[fig.subplots(num_rows, num_cols)]]
    else:
        axes = fig.subplots(num_rows, num_cols)
    for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):
        for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):
            pcm = ax.imshow(matrix.detach().numpy(), cmap=cmap)
            if i == num_rows - 1:
                ax.set_xlabel(xlabel)
            if j == num_cols - 1:
                ax.set_ylabel(ylabel)
            if titles:
                ax.set_title(titles[j])
    fig.colorbar(pcm, ax=axes, shrink=0.6)
```

## Nadaraya-Watson 核回归

1964 年提出的 Nadaraya-Watson 核回归模型是一个简单而完整的具有注意力机制的机器学习的例子。通过这个例子我们可以更好的理解注意力机制的运作。

首先从一个最简单的回归例子开始，对于一组数据，我们可以使用它们的平均值来作为回归结果计算其他的输入。
$$
f(x)=\frac{1}{n}\sum_{i=1}^ny_i
$$

但是很明显的，对于大多数数据来说，这样简单的处理不能得到一个好的结果，因为上式忽略了数据中 $x$ 的影响。Nadaraya-Watson 核回归使用下面的式子来对数据进行回归：
$$
f(x)=\sum_{i=1}^n\frac{K(x-x_i)}{\sum_{j=1}^nK(x-x_j)}y_i
$$
其中 $K$ 是核函数。上式核函数取常数时，就相当于直接计算平均值。使用核函数相当于将数据映射到高纬空间中，使得可以在高纬空间中找到一个平均直线作为估计的结果。下面是使用高斯核得到的结果，可以看到使用核函数得到的结果明显优于直接使用平均汇聚的结果。![[output.svg]]

### 非参数注意力汇聚

受到 Nadaraya-Watson 核回归的启发，我们可以将注意力机制框架重写，得到更加通用的注意力汇聚层 (attention pooling) 公式
$$
f(x)=\sum_{i=1}^n\alpha(x,x_i)y_i
$$
其中 $x$ 是查询，$(x_i,y_i)$ 是键值对，$\alpha(x,x_i)$ 表示查询 $x$ 与键 $x_i$ 之间的注意力权重 (attention weight)。每个权重都对应一个值 $y_i$，且对于任何查询，注意力权重都是一个概率分布，和为 1。我们可以使用核函数来计算权重，这样得到的注意力权重在查询 $x$ 与键 $x_i$ 越接近时越大，这时有
$$
\alpha(x,x_i)=\frac{K(x-x_i)}{\sum_{j=1}^nK(x-x_j)}
$$

需要注意的是，Nadaraya-Watson 核回归是一个非参数模型，因此我们通过它得到的注意力汇聚层是非参数注意力汇聚模型，这样得到的模型数据越多表现应当越好，但是实际上在实践中发现模型过于简单，表达能力不足，容易欠拟合。

### 带参数的注意力汇聚

我们可以很简单的将非参数的注意力汇聚修改为带参数的注意力汇聚，只需要使用带有参数的核函数，我们便可以通过梯度下降等方法学习核函数中的参数。以高斯核为例，有
$$
f(x)=\sum_{i=1}^n\frac{\exp(-\Vert x-x_i\Vert^2 w)}{\sum_{j=1}^n\exp(-\Vert x-x_j\Vert^2 w)}y_i
$$
其中 $w$ 为高斯核的带宽的倒数，是一个可以学习的参数。

## 注意力评分函数

到目前为止，注意力框架可以被建模为下面的形式：
![[Pasted image 20230717131249.png]]

其中查询通过与每个键通过注意力函数进行评分，得到分数通过 softmax 层转换为了注意力权重，最后每个值通过与注意力权重加权求和得到最终的输出。在 [[#Nadaraya-Watson 核回归]] 中，我们使用高斯核的指数部分作为注意力评分函数。实际上注意力评分函数有更多的选择。在上述的框架中，注意力权重通过
$$
\alpha(q,k_i)=\mathrm{softmax}(a(q,k_i))=\frac{\exp(a(q,k_i))}{\sum_{j=1}^n\exp(a(q,k_j))}
$$
得到，其中 $a$ 就是注意力评分函数。不同的注意力函数会导致不同的注意力汇聚操作，在注意力机制框架中，有两种流行的评分函数。

### 掩蔽 softmax 操作

在介绍两种流行的评分函数之前，先介绍掩蔽 softmax 操作，实际上这个操作在循环神经网络中就已经使用过了。softmax 操作用于将注意力评分转变为由概率分布表示的注意力权重，但是在某些情况下，不是所有的值都应该被纳入评分中的，例如在文本序列中，会填充一些没有意义的特殊词元 (空白词元等)。为了仅将有意义的词元作为值来获取注意力权重，可以指定一个序列长度，将超出这个长度的注意力权重置为 0。下面实现的是一个 masked_softmax 操作：
```python
def masked_softmax(X, valid_lens):
    """通过在最后一个轴上掩蔽元素来执行softmax操作"""
    # X:3D张量，valid_lens:1D或2D张量
    if valid_lens is None:
        return nn.functional.softmax(X, dim=-1)
    else:
        shape = X.shape
        if valid_lens.dim() == 1:
            valid_lens = torch.repeat_interleave(valid_lens, shape[1])
        else:
            valid_lens = valid_lens.reshape(-1)
        # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0
        X = d2l.sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)
        return nn.functional.softmax(X.reshape(shape), dim=-1)
```

### 加性注意力

一般而言，当查询和键是不同长度的向量时，可以使用加性注意力作为评分函数。给定查询 $q\in\mathbb R^q$ 与键 $k\in \mathbb R^k$，加性注意力 (additive attention) 评分函数定义为
$$
a(q, k)=w_v^T\tanh(W_qq+W_kk)\in \mathbb R
$$
其中可学习的参数为 $W_q\in\mathbb R^{h\times q}$、$W_k\in\mathbb R^{h\times k}$ 和 $w_v\in R^h$。该式将查询与键连接后输入一个隐藏层有 $h$ 个单元，激活函数为 $\tanh$ ，输出一个值的多层感知机中，并且禁用了偏置项。

### 缩放点积注意力

当查询和键是长度相同的向量时，使用点积可以更加快速的计算。假设查询和键的所有元素都是独立的随机变量，并且满足零均值和单位方差，那么他们的点积也满足零均值和单位方差。为了保证无论向量的长度如何，点积的方差在不考虑向量长度的情况下仍然是 1，我们将点积除以 $\sqrt{d}$，即缩放点积注意力 (scaled dot-product attention) 评分函数为：
$$
a(q,k)=q^Tk/\sqrt{d}
$$
在实践中，我们通常考虑从小批量的角度来提高效率，例如对于 n 个查询和 m 个键值对计算注意力，其中查询的长度为 $d$，值的长度为 $v$，查询 $Q\in\mathbb R^{n\times d}$、键 $K\in\mathbb R^{m\times d}$ 和值 $V\in\mathbb R^{m\times v}$ 的缩放点积注意力为
$$
\mathrm{softmax}(\frac{QK^T}{\sqrt{d}})V\in\mathbb R^{n\times v}
$$
## Bahdanau 注意力模型

Bahdanau 是一种将注意力运用在 seq2seq 上的模型。该模型是基于 [[现代循环神经网络#从序列到序列学习]] 与注意力机制的模型。Bahdanau 模型在编码阶段没有变化，但是在解码阶段，每一步的上下文变量都使用
$$
c_t'=\sum_{t=1}^T\alpha(s_{t'-1},h_t)h_t
$$
来进行计算，其中解码器隐状态 $s_{t'-1}$ 是查询，编码器隐状态 $h_t$ 即是键也是值。注意力权重使用加性注意力打分函数进行计算。

换句话说，在解码阶段，每一次都将上一时刻的解码器隐状态 $s_{t'-1}$ 作为查询，将编码器最终的隐状态 $h_t$ 作为键和值。该模型成功的将注意力加入了之前的编码器-解码器架构，我们可以使用相同的方式对其进行训练。

## 多头注意力

像卷积神经网络中的多个通道一样，我们希望当给定相同的查询、键和值的集合时，模型可以基于相同的注意力机制学习到不同的行为，然后将不同的行为组合起来，捕获序列内各种范围的依赖关系（如长距离依赖关系与短距离依赖关系）。多头注意力就是一种解决方式。

多头注意力 (multihead attention) 指独立学习得到 $h$ 组不同的线性投影来变化查询、键和值，然后并行的将这些查询、键和值送入注意力汇聚层中，最后将这 $h$ 组不同的线性投影拼接起来，并通过另一个可以学习的线性投影变换，最后得到输出。对于 $h$ 组不同的线性投影，称为头 (head)。
![[Pasted image 20230717212614.png]]

### 模型

下面使用数学的语言对其进行描述。给定查询 $q\in\mathbb R^{d_q}$、键 $k\in\mathbb R^{d_k}$ 和值 $v\in\mathbb R^{d_v}$，每个注意力头 $h_i$ 的计算方式为
$$
h_i=f(W_i^{(q)}q+W_i^{(k)}k+W_i^{(v)}v)\in\mathbb R^{p_v}
$$
其中包含了可学习的参数 $W_i^{(q)},W_i^{(k)},W_i^{(v)}$ 和注意力汇聚函数 $f$ 。$f$ 可以是加性注意力或者点积注意力。最后通过另一个线性变化得到输出
$$
W_o\begin{bmatrix}h_1\\h_2\\\vdots\\h_h\end{bmatrix}\in\mathbb R^{p_o}
$$

这样的设计，每个头都有可能关注输入不同的部分，可以表示比简单加权平均值更加复杂的函数。

## 自注意力与位置编码

在深度学习中，我们可以使用 [[卷积神经网络]] 与 [[循环神经网络]] 对序列进行编码。在引入注意力机制后，我们可以同时将序列直接输入注意力池化中，将一个词元同时充当查询、键和值。由于查询、键和值来自于同一个输入，因此称为自注意力 (self-attention)，也称为内部注意力 (intra-attention)。

#### 自注意力

下面给出自注意力形式化的定义。给定一个由词元组成的输入序列 $x_1,\cdots,x_n$，其中 $x_i\in\mathbb R^{d}$，该序列的输出为一个长度相同的自注意力序列 $y_1,\cdots,y_n$，其中
$$
y_i=f(x_i,(x_1,x_1),\cdots,(x_n,x_n))\in \mathbb R^d
$$
其中 $f$ 是注意力汇聚，$x_i$ 是 query，$(x_i,x_i)$ 是 (key, value)。

### 自注意力与 CNN 和 RNN

下面比较 CNN、RNN 与自注意力的一些属性：

|            | CNN        | RNN       | 自注意力  |
| ---------- | ---------- | --------- | --------- |
| 计算复杂度 | $O(knd^2)$ | $O(nd^2)$ | $O(n^2d)$ |
| 并行度     | $O(n)$     | $O(1)$    | $O(n)$    |
| 最长路径   | $O(n/k)$   | $O(n)$    | $O(1)$    |

自注意力有着高并行、可以处理长序列的优势，但是计算复杂度较高。

### 位置编码

与 RNN 不同，自注意力的并行处理导致无法获得位置的信息，一种解决手段是将位置信息编码 (positional encoding)，作为输入送入神经网络中。位置编码可以通过学习得到也可以直接固定得到，下面描述的基于正弦函数和余弦函数的固定位置编码。

假设输入 $X\in\mathbb R^{n\times d}$ 表示一个序列中 $n$ 个词元的 $d$ 维嵌入表示，位置编码使用相同形状的位置嵌入矩阵 $P\in\mathbb R^{n\times d}$ 输出 $X+P$，矩阵的第 $i$ 行第 $2j$ 列和 $2j+1$ 列上的元素为
$$
\begin{aligned}
p_{i,2j}&=\sin\left(\frac{i}{10000^{2j/d}}\right)\\
p_{i,2j+1}&=\cos\left(\frac{i}{10000^{2j/d}}\right)
\end{aligned}
$$

#### 绝对位置编码

在二进制表示中，较高位置的交替频率低于较低次位，使用上面的三角函数可以模拟这样的变化，由于此类连续表示的输出是浮点数，因此二进制表更加节省空间。

#### 相对位置信息

除了捕获绝对位置信息，上述的位置编码还允许模型学习得到输入序列中的相对位置信息。因为对于任意确定位置的偏移 $\delta$，位置 $i+\delta$ 处的位置编码可以使用线性投影位置 $i$ 处的位置编码表示。

具体来说，可以得到：
$$
\small
\begin{aligned}
\begin{bmatrix}\cos(\delta\omega_j)&\sin(\delta\omega_j)\\-\sin(\delta\omega_j)&\cos(\delta\omega_j)\end{bmatrix}\begin{bmatrix}p_{i,2j}\\p_{i,2j+1}\end{bmatrix}&=\begin{bmatrix}\cos(\delta\omega_j)\sin(i\omega_j)+\sin(\delta\omega_j)\cos(i\omega_j)\\-\sin(\delta\omega_j)\sin(i\omega_j)+\cos(\delta\omega_j)\cos(i\omega_j)\end{bmatrix}\\&=\begin{bmatrix}\sin((i+\delta)\omega_j)\\\cos((i+\delta)\omega_j)\end{bmatrix}\\&=\begin{bmatrix}p_{i+\delta,2j}\\p_{i+\delta,2j+1}\end{bmatrix}
\end{aligned}
$$

## Transformer

Transformer 是完全基于自注意的架构，没有任何的卷积层或者循环神经网络。尽管 Transformer 最初应用于文本数据的序列到序列学习，但是现在已经推广到各种现代深度学习中。

### 模型结构

Transformer 作为编码器-解码器架构的一个实例，其结构如下。
![[Pasted image 20230729163348.png]]

与基于 Bahdanau 注意力实现的序列到序列学习相比，Transformer 的编码器和解码器是基于自注意力的模块叠加而成，输入序列和目标序列的嵌入 (embedding) 表示将加上位置编码，再分别输入到编码器和解码器中。

Transformer 的编码器与解码器在不同的任务中有着不同的应用。