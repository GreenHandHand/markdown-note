---
tags:
  - 深度学习
---
# 现代循环神经网络

[[循环神经网络]] 可以很好的处理序列序列数据，但是对于现代中各种各样的序列数据，简单的循环神经网络可能不够用。例如，循环神经网络在实践的一个常见的问题是数值不稳定，虽然使用梯度截断等技巧可以缓解这个问题，但是仍然需要通过设计更加复杂的序列模型才可以进一步法处理它。现代循环神经网络中有两个使用广泛的概念用于解决这些问题，即门控循环单元 (gated recurrent unit, GRU)和长短时记忆单元 (long short-term memory, LSTM)。

解决数值不稳定的问题后，我们将基于一个单向隐藏层来拓展循环神经网络架构，即描述具有多个隐藏层的深层架构，且讨论基于正向和后循环计算的双向设计。

语言建模仅仅是序列学习能力的冰山一角，在各种序列学习问题中，如自动语音识别，文本到语音的转换与机器翻译，输入输出都是任意长度的序列。为了描述如何拟合这种类型的数据，我们将以机器翻译为例子介绍基于循环神经网络的“编码器-译码器”架构和束搜索，并使用他们来生成序列。

## 门控循环单元 (GRU)

循环神经网络容易出现梯度消失或者梯度爆炸的问题，学术界已经提出了许多的方法来解决这个问题，其中最早的方法是长短时记忆网络 (LSTM)。门控循环单元是一个稍微简化的变体，通常可以提供同等的效果，并且计算速度明显更快。

### 门控隐状态

门控循环单元与普通的循环神经网络之间的关键区别在于，前者支持隐状态的门控。这意味这模型有专门的机制来确定应该何时更新隐状态，何时重置隐状态，且这些机制是可以学习的。

### 重置门和更新门

重置门 (reset gate) 允许我们控制可能还想记住什么的过去状态的数量；更新门 (update gate) 允许我们控制新状态中有多少个是旧状态的副本。我们将他们设置为 $(0,1)$ 区间内的向量，这样我们就可以将他们进行凸组合。

设上一个时间步的隐状态为 $H_{t-1}$，输入为 $X_t$，我们想要得到的结构是通过上一个时间步的隐状态与输入得到当前时间步的隐状态的结构。使用重置门和更新门可以得到如下
![[Pasted image 20230703114031.png]]
我们使用简单的全连接层与 Sigmoid 函数进行连接，可以得到重置门与更新门的输出为
$$
\begin{aligned}
R_t&=\sigma(X_tW_{xr}+H_{t-1}W_{hr}+b_r)\\
Z_t&=\sigma(X_tW_{xz}+H_{t-1}W_{hz}+b_z)
\end{aligned}
$$
使用 Sigmoid 函数是因为重置门与更新门的输出为 $(0,1)$ 区间内的向量，使用简单的加权求和来拟合输出与输入的关系。

### 候选隐状态

接下来将重置门 $R_t$ 与常规隐状态更新机制集成，从而得到在时间步 $t$ 的候选隐状态 $\widetilde H_t\in \mathbb R^{n\times h}$，
$$
\widetilde H_t=\tanh(X_tW_{xh}+(R_t\odot H_{t-1})W_{hh}+b_{h})
$$
其中 $W_{xh}$ 与 $W_{hh}$ 是权重参数，$b_h$ 是偏置，符号 $\odot$ 为哈达玛积。在这里，我们使用非线性激活函数 $\tanh$ 来保证候选隐状态中的值在 $(-1,1)$ 区间内。

由于重置门 $R_t$ 的输出为 $(0,1)$ 的向量，因此当重置门中的项接近与 1 时，上式恢复为一个普通的循环神经网络。对于重置门 $R_t$ 中所有接近于 0 的项，候选隐状态为 $X_t$ 为输入的多层感知机的结果。因此，任何预先存在的隐状态都会被重置为默认值，且这个值是可以被学习的。

下图展示了将重置门与隐状态更新机制机集成后的结构
![[Pasted image 20230703114058.png]]

### 隐状态

上述计算只结合了重置门，我们仍然需要结合更新门 $Z_t$ 的效果。这一步确定新的隐状态 $H_t\in\mathbb R^{n\times h}$ 在多大程度上来自旧的隐状态 $H_{t-1}$ 和新的候选隐状态 $\widetilde H_{t}$。更新门 $Z_t$ 仅需要在 $H_{t-1}$ 和 $\widetilde H_t$ 之间进行按元素的凸组合就可以实现这个目标，即
$$
H_t=Z_t\odot H_{t-1}+(1-Z_t)\odot \widetilde H_t
$$
同样的，由于更新门的输出为 $(0,1)$ 区间的向量，因此当更新门 $Z_t$ 接近与 1 时，模型会倾向于保留旧的隐状态，此时来自 $X_t$ 的信息基本被忽略，从而有效的跳过了依赖链中的时间步 $t$；当更新门 $Z_t$ 接近于 0 时，性的隐状态 $H_t$ 就会接近于候选隐状态。

这样的设计可以帮助我们处理循环神经网络中的梯度消失问题，并更好的捕获时间步很长的序列依赖关系。最终可以得到门控循环单元 GRU 的结构如下
![[Pasted image 20230703114826.png]]

总结一下，门控循环单元具有以下两个显著特征：
- 重置门有助于捕获序列中的短期依赖关系
- 更新门有助于捕获序列中的长期依赖关系

在 pytorch 中，我们可以通过 `nn.GRU(num_inputs, num_hiddens)` 来初始化一个 GRU 块。

## 长短期记忆网络 (LSTM)

长期以来，隐变量模型存在着长期信息保存和短期输入缺失的问题，解决这一问题的最早方法之一是长短期记忆网络 (LSTM)，它与门控循环单元有着一样的属性，但是较 GRU 更为复杂。有趣的是，LSTM 的出现比 GRU 要早 20 年。

### 门控记忆元

可以认为长短期记忆网络的设计灵感来自计算机的逻辑门，LSTM 引入了记忆元 (memory cell)，简称为单元 (cell)。一些文献认为记忆元是隐状态的一种特殊类型，它们与隐状态有着相同的形状，其设计目的是用于记录附加的信息。长短期记忆网络的门控记忆元有三种基本门：
- 输出门 (output gate)：从记忆元中输出条目
- 输入门 (input gate)：决定何时将数据读入记忆元
- 遗忘门 (forget gate)：决定何时重置记忆元的内容

#### 输入门、遗忘门和输出门

与 GRU 部分相同，先将数据输入遗忘门、输入门与输出门中，我们需要的是输入观测 $X_t$ 与上一时间步的隐状态 $H_{t-1}$，输出当前隐状态的结构。同样的，我们限制遗忘门、输入门、输出门的输出为 $(0,1)$ 区间向量。使用 $I_t\in\mathbb R^{n\times h},F_t\in\mathbb R^{n\times h}$ 和 $O_t\in\mathbb R^{n\times h}$ 分别代表输入门、遗忘门和输出门，则它们的计算方式如下：
$$
\begin{aligned}
I_t&=\sigma(X_tW_{xi}+H_{t-1}W_{hi}+b_i)\\
F_t&=\sigma(X_tW_{xf}+H_{t-1}W_{hf}+b_f)\\
O_t&=\sigma(X_tW_{xo}+H_{t-1}W_{ho}+b_o)
\end{aligned}
$$
于是得到输入结构如下：
![[Pasted image 20230703142009.png]]

#### 候选记忆元

候选记忆元 (candidate memory cell) $\widetilde C_t\in\mathbb R^{n\times h}$ 的计算与前面的三个门类似，但是使用 $\tanh$ 作为激活函数，其计算方式为
$$
\widetilde C_t=\tanh(X_tW_{xc}+H_{t-1}W_{hc}+b_c)
$$
在长短期记忆网络中，我们使用记忆元来储存来自过去的信息，而候选记忆元就是作为当前观测得到的信息。加入候选记忆元后的结构变为
![[Pasted image 20230703143025.png]]

#### 记忆元

长短期记忆网络中，我们使用输入门控制采用多少来自 $\widetilde C_t$ 中的新数据，使用遗忘门控制保留多少过去的记忆元 $C_{t-1}\in\mathbb R^{n\times h}$ 的内容。使用按元素乘法，得到
$$
C_t=F_t\odot C_{t-1}+I_t\odot \widetilde C_t
$$
如果遗忘门始终为 1 输入门始终为 0，那么过去的记忆元将被一直保留并传递到当前时间步。引入这种设计是为了缓解梯度消失问题，并更好的捕获序列中的长距离依赖关系。将记忆元的计算加入结构中，得到
![[Pasted image 20230703143311.png]]

#### 隐状态

最后定义如何计算隐状态 $H_t\in\mathbb R^{n\times h}$。我们在使用记忆元之前先对其进行 $\tanh$ 操作，使其值被限制在 $(-1,1)$ 区间中，此时我们可以使用
$$
H_t=O_t\odot\tanh(C_t)
$$
计算下一状态的隐状态。当输出门接近 1,，我们可以有效地将所有记忆信息传递给预测部分，而对于输出门接近 0，我们只保留记忆元内的所有信息，而不需要更新隐状态。最终可以得到 LSTM 单元的结构如下：
![[Pasted image 20230703143817.png]]

### 高级 API

在 pytorch 中，使用 `nn.LSTM(num_inputs, num_hiddens)` 即可定义一个长短期记忆单元。

长短期记忆网络时典型的具有重要状态控制的隐变量自回归模型，多年以来已经提出了许多变体。然而，由于序列的长距离依赖性，训练长短期记忆网络和其他序列模型 (如 GRU) 的成本是相当高的。transformer 可以作为其更高级的替代。

## 深度循环神经网络

目前为止讨论的都是单向隐藏层的神经网络。在实际中，因隐变量与观测值和具体的函数形式的交互方式是相当随意的，这对于单层的网络而言是具有相当的挑战。在线性模型中，我们可以通过增加层数来解决这个问题。而在循环神经网络中，我们首先要确定如何添加更多的层，以及在哪里添加额外的非线性层。

事实上，我们可以将多层循环神经网络堆叠在一起，通过对几个简单层的组合，产生一种灵活的机制。特别是，数据可能与不同的堆叠层有关。一个深度的循环神经网络可以是下面的形式
![[Pasted image 20230703162000.png]]

### 函数依赖关系

我们可以将深度架构中的函数依赖关系形式化，这个架构由上图中的 $L$ 个隐藏层构成。后面我们主要讨论经典的循环神经网络，但是这些讨论同样适用于其他序列模型。

假设在时间步 $t$ 有一个小批量输入数据 $X_t\in\mathbb R^{n\times d}$，同时，将第 $l$ 个隐藏层的隐状态设为 $H_t^{(l)}$，输出层变量设置为 $O_t\in\mathbb R^{n\times q}$。设置 $H_t^{(0)}=X_t$，第 $l$ 个隐藏层的隐状态使用激活函数 $\phi_l$，则
$$
H_t^{(l)}=\phi_1(H_t^{(l-1)}W_{xh}^{(l)}+H_{t-1}^{(l)}W_{hh}^{(l)}+b_h^{(l)})
$$
在最后，输出层的计算仅基于第 $l$ 个隐藏层最终的隐状态：
$$
O_t=H_t^{(l)}W_{hq}+b_q
$$

与多层感知机相同，隐藏层的层数 $L$ 和隐藏单元数 $h$ 都是超参数。另外，使用门控循环单元或者长短期记忆网络的隐状态来替代上面的式子，可以很容易的得到深度门控循环神经网络或者深度长短期记忆网络。

### 高级 API

实现多层循环神经网络的许多逻辑在高级 API 中都是现成的，最常用的堆叠层数，只需要在创建循环神经网络的模块时在最后添加 num_layers 参数即可将其堆叠。

## 双向循环神经网络

双向循环神经网络的灵感来自于 [[隐马尔可夫模型]] 中的前向算法与后向算法，考虑一个挖空填词的问题，我们不仅需要考虑空位之前的文本，还需要考虑空位之后的文本。单向的循环神经网络只会考虑前文序列的内容，因此在这样的任务下表现不佳。

### 双向模型

我们希望在神经网络中有一种机制，使之能够有与隐马尔可夫模型类似的前瞻能力，我们需要修改循环神经网络的设计。这在概念上时十分容易的，只需要增加一个从最后一个词元开始从前向后运行的循环神经网络。双向循环神经网络 (bidirectional RNN)添加了反向传递信息的隐藏层，以便灵活的处理此类信息。

事实上，这与隐马尔可夫模型中的前向算法与后向算法没有多大的区别。其主要区别是，隐马尔科夫模型中的方程具有特定的统计意义，而双向循环神经网络没有这么容易理解，我们只能将其作为通用的、可学习的函数。这种转变体现了现代深度网络的设计原则：首先使用经典的统计模型的函数依赖类型，然后将其参数化为通用形式。

#### 定义

对于任意时间步 $t$，给定一个小批量的输入数据 $X_t\in\mathbb R^{n\times d}$，并令隐藏层激活函数 $\phi$，在双向架构中，我们设该时间步的前向隐状态和反向隐状态分别为 $\overrightarrow H_t\in\mathbb R^{n\times h}$ 和 $\overleftarrow H_t\in\mathbb R^{n\times h}$，其中 $h$ 是隐藏单元数。前向隐藏状态与后向隐藏状态的更新分别如下：
$$
\begin{aligned}
\overrightarrow H_t &= \phi(X_tW_{xh}^{(f)}+\overrightarrow H_{t-1}W_{hh}^{(f)}+b_h^{(f)})\\
\overleftarrow H_t &= \phi(X_tW_{xh}^{(b)}+\overleftarrow H_{t-1}W_{hh}^{(b)}+b_h^{(b)})
\end{aligned}
$$
然后将前向隐状态与后向隐状态连起来，获得需要送入输出层的隐状态 $H_t\in\mathbb R^{n\times 2h}$。在具有多个隐藏层的双向神经网络中，该信息作为输入传递到下一个双向层，最后，在输出层使用使用
$$
O_t=H_tW_{hq}+b_q
$$
计算得到最终的输出。

#### 模型的计算成本及其应用

双向循环神经网络的计算成本非常高，因为网络的前向传播需要在双向层中进行向前和向后递归，并且网络的反向传播还依赖于前向传播的结果，因此，梯度求解将会经历一个非常长的链。

同时，双向循环神经网络在运用中也有很多的限制。双向循环神经网络的一个关键特性是我们使用来自过去和未来的观测信息来预测当前的观测。在预测任务中，我们可以使用未来的数据与过去的数据进行训练来预测当前的观测，但是在测试集中我们只有过去的观测数据，因此精度将会变得很低。

双向循环神经网络在实践中运用得非常少，且仅能应用于部分场景。例如，填充缺失的单词，词元注释以及作为序列处理流水线中的一个步骤对序列进行编码（如机器翻译）。

### 高级 API

在 pytorch 中，循环神经网络的结构（如 LSTM、RNN、GRU）都可以通过传入参数 `bidirectional=True` 来设置为双向的。

## 机器翻译与数据集

语言模型是自然语言处理的关键，而机器翻译时语言模型最成功的基准测试，因为机器翻译正是将输入序列转换为输出序列的序列转换模型 (sequence transduction model) 的核心问题。

机器翻译 (machine translation) 指的是将序列从一种语言自动翻译成另一种语言，事实上，这个领域可以追溯到数字计算机发明后不久的 20 世纪 40 年代。在使用神经网络进行端到端学习兴起之前，统计学法在这一领域一直占据了主导地位，因为统计机器翻译 (statistical machine translation) 涉及翻译模型和语言模型等组成部分的统计分析，而基于神经网络的方法通常被称为神经网络机器翻译 (neural machine translation)。

与语言模型问题不同，机器翻译的数据集是由源语言到目标语言的文本序列对组成的。

## 编码器-解码器架构

机器翻译是序列转换模型的一个核心问题，这种模型的输入与输出都是长度可变的序列。为了处理这种类型的输入和输出，我们可以设计一种包含两个组件的架构，包括编码器 (encoder) 与解码器 (decoder)。
- 编码器接受一个可变长度的序列，输出一个一定形状的编码状态。
- 解码器将固定形状的编码状态映射到长度可变的序列。
![[Pasted image 20230706152929.png]]

对于一个输入，编码器先将其编码为一定形状的状态变量，然后在通过解码器将其解码为输出。

下面是编码器-解码器的通用接口：
```python
class Encoder(nn.Module):
	"""编码器-解码器架构的基本编码器接口"""
	def __init__(self, **kwargs):
		super(Encoder, self).__init__(**kwargs)

	def forward(self, x, *args):
		raise NotImplementedError

class Decoder(nn.Module):
	"""编码器-解码器架构的基本解码器接口"""
	def __init__(self, **kwargs):
		super(Decoder, self).__init__(**kwargs)

	def init_state(self, enc_outputs, *args):
		raise NotImplementedError

	def forward(self, x, state):
		raise NotImplementedError

class EncoderDecoder(nn.Module):
	"""编码器-解码器架构的基类"""
	def __init__(self, encoder, decoder, **kwargs):
		super(EncoderDecoder, self).__init__(**kwargs)
		self.encoder = encoder
		self.decoder = decoder

	def forward(self, enc_x, dec_x, *args):
		enc_ouputs = self.encoder(enc_x, *args)
		dec_state = self.decoder.init_state(enc_outputs, *args)
		return self.decoder(dec_x, dec_state)
```

## 从序列到序列学习

下面详细说明如何使用编码器-解码器架构与循环神经网络实现从序列到序列 (sequence to sequence, seq2seq) 学习。根据编码器-解码器架构，循环神经网络编码器使用长度可变的序列作为输入，将其转换为固定长度的隐状态编码。换言之，输入序列的信息被编码到了神经网络的隐状态中。为了连续输出序列的词元，神经网络解码器根据输入序列的编码信息和输出序列可见或者生成的词元来预测下一个词元。简单来说，从输入到输出的步骤如下：
1. 编码器按照时间步的顺序接受一组编码器输入，这个输入在机器翻译中一般为源语言，最后得到的输出就是状态编码
2. 解码器通过 init_state 函数，根据编码器输出的状态编码得到解码器的初始状态
3. 解码器接受之前得到的初始状态与解码器输入，解码器输入一般为开始词元或者上一次预测得到的词元，不断的预测输入词元的下一个词元。

下面是一个例子：
![[Pasted image 20230706180609.png]]
其中 `<eos>` 表示结束词元，当读到这个词元时，编码器停止编码，转而将输入传入解码器中。`<bos>` 表示开始词元，它是解码器输入序列的第一个词元。其次，每次都使用编码器最后得到的隐状态作为解码器的隐状态来初始化解码器的隐状态，或者说，编码器最终的隐状态在每一个时间步都作为解码器的输入的一部分。

下面针对机器翻译问题对编码器与解码器进行解释。

### 编码器

从技术上来将，编码器将长度可变的输入序列转变成形状固定的上下文变量 $c$，并将序列的信息在该上下文变量中进行编码。因此可以使用循环神经网络来设计编码器。

考虑一个由序列组成的样本，假设输入序列为 $x_1,\cdots,x_T$，其中 $x_t$ 是输出文本中第 $t$ 个词元。在时间步 $t$，循环神经网络将词元 $x_t$ 的输入特征向量 $x_t$ 与 $h_{t-1}$ 转换为 $h_t$。使用一个函数来描述这一过程就是
$$
h_t=f(x_t,h_{t-1})
$$
得到所有的隐变量后，循环神经网络通过选定函数 $q$ 将所有时间步的隐状态转换为了上下文变量
$$
c=q(h_1,\cdots,h_T)
$$
当选定 $q(h_1,\cdots,h_T)=h_T$ 时，上下文变量就是最后的隐状态。

使用单向神经循环神经网络来设计编码器，其中隐状态只依赖输入子序列，这个子序列为输入序列的开始位置到隐状态所在时间步的位置。也可以使用双向循环神经网络来设计编码器，其中隐状态依赖两个输入子序列，这两个子序列分别是隐状态所在的时间步的位置之前的序列和之后的序列，因此隐状态对整个序列的信息都进行了编码。

#### 嵌入层

嵌入层 (embedding layer) 是自然语言处理中常用的神经网络层。该层的权重是一个矩阵，通过该矩阵将输入向量映射到其他空间中。这个矩阵是可以学习的。嵌入层常见的作用有两个：
1. 特征降维：通过一个行数小于列数的矩阵，可以将输入向量映射到另一个维度更小的空间，得到维度更小的向量。
2. 特征升维：将数据维度升高，可以将数据的一些特征放大，或者使得一些特征更加明显。

在机器翻译中，嵌入层的大小为 (vocab_size, embed_size)，其中 vocab_size 为词表大小，embed_size 为输出特征向量的维度。由于在自然语言处理中，词表大小一般都非常大，使用 one-hot 编码得到的向量中包含了太多的无效信息，使用嵌入层可以每个 one-hot 向量映射到一个唯一的，长度为 embed_size 的向量上，从而大大减少数据的数量。

### 解码器

编码器输出的上下文变量 $c$ 对整个输入序列 $x_1,\cdots,x_T$ 进行编码。来自数据集的输出序列为 $y_1,\cdots,y_T$，对于每一个时间步 $t'$，解码器输出 $y_{t'}$ 的概率取决于之前的输出子序列 $y_1,\cdots,y_{t'-1}$ 和上下文变量 $c$，即 $P(y_{t'}|y_{t'-1},\cdots,y_1,c)$。

为了在序列上模型化这种条件概率，我们可以使用另一个循环神经网络作为解码器，在输出序列上的任意时间步 $t'$，循环神经网络将来自上一时间步的输出 $y_{t'-1}$ 和上下文变量 $c$ 作为其输入，然后在当前时间步将它们和上一隐状态 $s_{t'-1}$ 转换为隐状态 $s_{t'}$。因此可以使用 $g$ 来表示解码器的隐藏层变换：
$$
s_{t'}=g(y_{t'-1},c,s_{t'-1})
$$
在获得解码器的隐状态后，我们可以使用输出层和 softmax 操作来计算时间步 $t'$ 时的输出 $y_{t'}$ 的条件概率分布。在实际中，我们通常直接将当前时间步的 $y_{t'-1}$ 与上下文变量 $c$ 进行 concatenate 操作再作为输入。

### 损失函数

在每个时间步，解码器预测了输出词元的概率分布，类似语言模型，可以使用 softmax 来获得分布，并通过计算交叉熵损失函数来进行优化。但是由于一些特殊的填充词 (如结束词、开始词等) 我们可以将不同长度的序列以相同的形状进行加载，但是在计算损失时我们应该将他们去除。

在计算损失函数时，我们需要根据有效长度来将其他的全部清零。

### 训练

在该模型的训练中，有一种策略称为强制教学 (teacher forcing)，这种策略是指在特定的序列开始词元 (`<bos>`) 和原始输出序列连接在一起作为解码器的输入。在训练过程中，使用真实的标签作为上一时间步预测值输出，而不使用上一时间步的预测值作为输入。这样做的优势是可以加速模型的收敛，特别是在任务较为困难或输出序列较长的情况下。它通过提供准确的目标序列信息来帮助模型学习到正确的生成规律和语言结构。但是这样做可能会导致误差累积，因为在预测中，我们只能使用上一次预测的结果，这与训练过程中是不同的。

### 预测

预测与训练过程不同点在于预测只能使用上一时间步得到的预测值作为输入，所以与训练时的前向传播函数相对比，得到预测的过程应当为
![[Pasted image 20230706213804.png]]

### 预测序列的评估

我们可以通过与真实的标签序列进行比较来评估预测序列。虽然 BLEU (bilingual evaluation understudy) 方法最优被运用在评估机器翻译的结果，但是现在它已经被广泛用于度量许多应用的输出序列的质量。原则上来说，对于预测序列中任意 n 元语法，BLEU 都能够评估这个 n 元语法是否出现在标签序列中。

我们将 BLEU 定义为
$$
\exp\left(\min\left(0,1-\frac{\mathrm{len_{label}}}{\mathrm{len_{pred}}}\right)\right)\prod_{n=1}^kp_n^{1/2^n}
$$
其中，$\mathrm{len_{label}}$ 表示标签中的词元数，$\mathrm{len_{pred}}$ 表示预测序列中的词元数，$k$ 是用于匹配的最长 $n$ 元语法。另外，使用 $p_n$ 表示 $n$ 元语法的精确率，他是两个数量的比值。根据定义，当预测序列与标签序列完全一样时，BLEU 为 1。

## 束搜索

在 [[#从序列到序列学习#预测]] 中，我们逐个预测词元，直到出现结束词元 `<eos>`。每一步我们可以通过模型得到目标词元的概率分布，通过这个分布得到目标输出的方法有三种：贪心搜索、穷举搜索和束搜索。

在正式介绍之前，我们需要先明确目标。在任意时间步 $t'$ 中，解码器输出 $y_t'$ 的概率取决于时间步 $t'$ 之前的输出 $y_{1},y_{2},\cdots,y_{t'-1}$ 和对输入序列的信息进行编码得到的上下文变量 $c$。为了量化计算代价，使用 $\mathcal Y$ 表示输出词表，其中包含 `<eos>`，所以这个词汇集合的基数 $|\mathcal Y|$ 就是词表的大小。我们还将输出序列的最大词元数指定为 $T'$。因此，我们的目标是从所有的 $\mathcal O(|\mathcal Y^{T'}|)$ 个可能的输出序列中寻找理想的输出。对于所有的序列，在 `<eos>` 后的部分在实际输出中都将丢弃。

### 贪心搜索

贪心策略是一种简单的策略。对于输出序列的每一个时间步 $t'$，我们都将基于贪心搜索从 $\mathcal Y$ 中找到具有最高概率的词元，即:
$$
y_{t'}=\arg\max_{y\in\mathcal Y}P(y|y_1,y_2,\cdots,y_{t'-1},c)
$$
一旦输出序列包含了 `<eos>` 或者到达最大长度 $T'$，则输出完成。

实际最优序列应当是最大化 $\prod_{t'=1}^{T'}P(y_{t'}|y_1,\cdots,y_{t'-1},c)$ 的序列。而贪心搜索无法保证最优序列。

### 穷举搜索

一种简单的手段是使用穷举搜索 (exhaustive search)：穷举所有可能的输出序列及其概率，然后计算条件概率最高的一个。穷举搜索虽然可以得到正确的序列，但是时间复杂度得惊人，在现实中几乎不能使用。

### 束搜索

贪心搜索追求速度但是精度低，而穷举搜索精度高但是速度慢。束搜索 (beam search)是一种折中的方案。束搜索是贪心搜索的一种改进版本，它有一个超参数束宽 (beam size) $k$。在时间步 1，我们选择具有最高条件概率的几个词元，然后这 $k$ 个词元将分布是 $k$ 个候选输出的第 1 个词元。在随后的时间步中，基于上一个时间步的 $k$ 个输出，我们将继续从 $k|\mathcal Y|$ 中个可能的选择中挑选具有最高概率的 $k$ 个候选序列。

下面是一个例子：
![[Pasted image 20230708103803.png]]
设束宽为 2，词表为 $\mathcal Y=\{A,B,C,D,E\}$，下面描述上图中的束搜索过程：
1. $t=1$，通过开始词元预测得到了第一个词的概率分布，然后选择其中条件概率最高的 $k$ 个词元作为候选词元。
2. $t=t'$，将上一时间步的 $k$ 个词元作为输入，从输出的 $k|\mathcal Y|$ 个选择中选择其中条件概率最高的 $k$ 个词元作为候选词元
3. 最后，我们会得到 $kT'$ 个候选输出，在本例中为 $\{A,C,AB,CE,ABD,CED\}$
4. 在候选输出中，我们选择其中条件概率乘积最高的序列作为输出序列
$$
\frac{1}{L^{\alpha}}\log P(y_1,y_2,\cdots,y_L|c)=\frac{1}{L^\alpha}\sum_{t'=1}^L\log P(y_{t'}|y_1,y_2,\cdots,y_{t'-1},c)
$$
其中 $L$ 为最终候选序列的长度，$\alpha$ 通常为 0.75。由于一个长序列中会有更多的对数项，我们使用 $L^{\alpha}$ 来惩罚长序列。束搜索的计算量为 $\mathcal O(k|\mathcal Y|T')$，这个结果介于贪心搜索和穷举之间，束搜索可以在计算代价和精确度之间进行权衡。