---
aliases:
  - 线性回归
  - 逻辑回归
  - linear regression
  - logistic regression
tags:
  - 机器学习
  - 监督学习
  - 线性
---

# 线性模型

## 基本形式

给定由d个属性描述的实例：$\boldsymbol{x}=(x_1,x_2,\cdots,x_d)$，线性模型试图获得一个通过属性的线性组合来进行预测的模型：
$$
f(x) = w_1x_1 + w_2x_2 + \cdots + w_dx_d + b\tag{1}
$$
一般用向量形式写为：
$$
f(\boldsymbol{x})=\boldsymbol{w}^T\boldsymbol{x}+b\tag{2}
$$

当$\boldsymbol{w}$和$b$确定后，模型就得以确定。
> 线性模型形式简单、易于建模，许多强大的非线性模型可在线性模型的基础上通过引入层级结构或者高维映射而得。此外，线性模型具有良好的可解释性，$w$ 直观地表示了各个属性的重要性。


## 线性回归

给定数据集$D=\{(\boldsymbol{x_1},y_1),(\boldsymbol{x_2},y_2),\cdots,(\boldsymbol{x_n},y_n)\}$，**线性回归**(**linear regression**)试图学得一个线性模型以尽可能准确地预测实值输出标记。
- 在线性回归中，如果 $x$ 为离散类型的属性，有一下两种方法将其转换为可以使用的数据：
- 若属性值间存在“序(order)”的关系，则可以通过连续化将其转换为连续值，例如身高的高矮取值为 $\{1.0,0.0\}$，三值属性高、中、低取值 $\{1.0,0.5,0.0\}$
- 若属性值间不存在有序关系，则转换为k维向量，例如西瓜、南瓜、黄瓜可以取值为$\{(1,0,0),(0,1,0),(0,0,1)\}$
线性回归试图解决：

$$
f(x_i)=wx_i+b,使得f(x_i)\simeq y_i
$$

如何计算$w,b$就成为了线性回归的首要目标，均方误差是回归认为中常用的性能度量，因此我们通过将均方误差最小化来得到目标模型。

线性回归要解决的问题可以描述如下：
$$
\begin{aligned}(w^*,b^*)=&\arg_{(w,b)}\min\sum_{i=1}^m(f(x_i)-y_i)^2\\=&\arg_{(w,b)}min\sum_{i=1}^m(y_i-wx_i-b)^2\end{aligned}\tag{3}
$$
基于均方误差来求解的方法称为**最小二乘法**(**least square method**)，在线性回归中，最小二乘法的意义就是找到一条直线使得所有的点到目标直线的欧式距离最短。

我们可以证明，线性回归得到的函数一定是凸函数，所以可以使用凸函数最优化的方法对线性回归的参数进化拟合，包括梯度下降法、牛顿法、最小二乘法等，在已知线性回归模型一定是凸函数的情况下，我们知道梯度下降法和牛顿法一定可以计算得到最优值。

### 最小二乘法

最小二乘法是基于均方误差最小化来进行模型求解的方法。最小二乘法在线性模型中的应用可以概括如下：
对于线性回归模型 $f(\boldsymbol{x_i})=\boldsymbol{w}^T\boldsymbol{x_i}+b$，可以将其写作矩阵的形式
$$
 f(\boldsymbol{X})=\boldsymbol{\hat{w}}^T\boldsymbol{X}\tag{4}
 $$
其中 $\boldsymbol{\hat{w}}=\{\boldsymbol{w};b\}$，$\begin{align}X=\begin{pmatrix}\boldsymbol{x_1}^T & 1\\ \boldsymbol{x_2}^T & 1 \\ \vdots & \vdots \\ \boldsymbol{x}_m^T & 1  \end{pmatrix}\end{align}$ 可以得到类似 $(3)$ 式的矩阵形式：
$$
\boldsymbol{\hat{w}}^*=\arg_{\boldsymbol{\hat{w}}}\min(Y-\boldsymbol{X}\boldsymbol{\hat{w}})^T(Y-\boldsymbol{X}\boldsymbol{\hat{w}})\tag{5}
$$
求导可以得到：
$$
\frac{\partial E_{\hat{w}}}{\partial\hat{w}}=2\boldsymbol{X}^T(\boldsymbol{X\hat{w}-Y})\tag{6}
$$
令上式等于零可以得到：
$$
\boldsymbol{\hat{w}}^*=(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{Y}\tag{7}
$$
上式是在假设 $\boldsymbol{X^TX}$ 为满秩矩阵或者正定矩阵时计算得到的结果，然而在现实中并不总能得到满秩矩阵和正定矩阵，在许多任务中，我们会遇到大量的变量，甚至数目超过样例的数量，在这种情况下我们将会解得多个满足条件的值，他们都能使的均方误差最小化，选择哪一个解作为输出将由学习算法的归纳偏好决定，常见的做法是引入正则化（这里不详细说明）。

> [!note]
> 在线性代数的角度来看，对于一般的 [[矩阵分析/线性方程组|线性方程组]] $Ax=\beta$ 在 $r(A\mid \beta)\neq r(A)$ 时，线性方程组是无解的，也就是我们无法找到一个超平面同时经过所有的样本点。
> 
> 在线性回归问题中，一般有数据数量远大于特征的数量，因此这类问题直接求解线性方程组都是无解的。但是对于这样无解的方程，我们可以在方程的两边同时左乘 $A^{T}$ 实现，这在线性代数中称为最小二乘解，含义为距离所有样本最近的超平面。所以实际上我们在机器学习中使用的线性回归模型是在求解线性方程组
> $$
A^TA\vec{w}=A^{T}\beta
> $$
> 在 $A^{T}A$ 可逆时，该方程组有唯一解 $\vec{x}=(A^{T}A)^{-1}A^{T}\beta$。

## 非线性回归

线性回归虽然简单，但是有丰富的变化，这里举出一个简单的例子。考虑单调可微函数$g(\cdot)$，令：
  
$$
y=g^{-1}(\boldsymbol{w}^T\boldsymbol{x} + b)\tag{8}
$$
  
这样得到的模型称为广义线性模型，其中函数$g(\cdot)$称为**联系函数**。当联系函数取$g(\cdot)=ln(\cdot)$时，就是对数线性回归。
  
## 对数几率回归

前面的内容是关于使用线性模型进行回归学习，下面介绍如何通过广义线性模型进行分类任务。这里的思想很简单，我们只需要找到一个单调可微函数将分类任务的真实标记$y$与线性回归模型中的预测值联系起来就可以得到分类模型。

考虑**二分类**，其输出标记为$y\in \{0, 1\}$，而线性回归模型产生的预测值$z=\boldsymbol{w^Tx}+b$是实值，于是我们将实值z转换成0/1值，这里最理想的函数是单位阶跃函数：

$$
y=\begin{cases}0, & z < 0 \\ 0.5, & z = 0\\ 1, & z > 0\end{cases}\tag{9}
$$

但是实际模型要求函数连续可微，我们可以使用机器学习中常用的**Sigmoid函数**，该函数可以在一定程度上近似单位阶跃函数，称为**替代函数**。 将Sigmoid函数带入$(8)$式中可以得到：

$$
y=\frac{1}{1+e^{-(\boldsymbol{w^Tx} + b)}}\tag{10}
$$

变化一下形式有：

$$
\ln\frac{y}{1-y}=\boldsymbol{w^Tx}+b\tag{11}
$$

若将$y$视为样本$x$作为正例的可能性，则$1-y$是其反例可能性，两者的比$\frac{y}{1-y}$称为**几率(odds)**，反映了$\boldsymbol{x}$作为正例的相对可能性，对几率取对数就可以得到**对数几率(log odds，也称为logit)**。
由此看出，式$(10)$实际上是在使用线性回归模型预测结果去逼近真实标记的对数几率，因此，对应的模型称为**对数几率回归(logistic regression)**，它不是仅预测出类别，而是可以得到近似概率的预测。

接下来给出确定式中$\boldsymbol{w}$与$b$的方法。将$(11)$中的$y$视为类后验概率估计$p(y=1\,|\,x)$，则式$(11)$可以重写为：
$$
\ln\frac{p(y=1\,|\,x)}{p(y=0\,|\,x)}=\boldsymbol{w^Tx}+b\tag{12}
$$
解得：
$$
p(y=1\,|\,x)=\frac{e^{\boldsymbol{w^Tx}+b}}{1+e^{\boldsymbol{w^Tx}+b}}\tag{13}
$$

$$
p(y=0\,|\,x)=\frac{1}{1+e^{\boldsymbol{w^Tx}+b}}\tag{14}

$$
于是我们通过极大似然法来估计$\boldsymbol{w}$与$b$。对于给定数据集$\{\boldsymbol{x}_i,y_i\}_{i=1}^m$，对数概率回归模型的对数似然函数如下：
$$
\mathscr{l}(\boldsymbol{w},b)=\sum_{i=1}^m\ln p(y_i\,|\,\boldsymbol{x}_i;\boldsymbol{w},b)\tag{15}
$$
令$\boldsymbol{\beta}=(\boldsymbol{w};b),\hat{\boldsymbol{x}}=(\boldsymbol{x};1)$，则$\boldsymbol{w^Tx}+b$可以简写为$\boldsymbol{\beta}^T \hat{\boldsymbol{x}}$，再令$p_1(\hat{\boldsymbol{x}_i};\boldsymbol{\beta})=p(y=1\,|\,\hat{\boldsymbol{x}_i};\boldsymbol{\beta}),p_0(\hat{\boldsymbol{x}_i};\boldsymbol{\beta})=p(y=0\,|\,\hat{\boldsymbol{x}_i};\boldsymbol{\beta})=1-p_1(\hat{\boldsymbol{x}_i};\boldsymbol{\beta})$，则$(15)$可以写为：
$$
\mathscr{l}(\boldsymbol{\beta})=\sum_{i=1}^m(-y_i\boldsymbol{\beta^T}\hat{\boldsymbol{x}}_i+\ln(1+e^{\boldsymbol{\beta}^T\hat{\boldsymbol{x}}_i}))\tag{16}
$$
上式是关于$\boldsymbol{\beta}$的高阶可导连续凸函数，根据凸优化理论，经典的数值优化算法如梯度下降法、牛顿法都可以求得其最优解：
$$
\boldsymbol{\beta}^*=\arg_{\boldsymbol{\beta}}\min \mathscr{l}(\boldsymbol{\beta})
$$
以牛顿法为例，更新公式为：
$$
\left\{\quad\begin{aligned}&\boldsymbol{\beta}'=\boldsymbol{\beta}-(\frac{\partial^2 \mathscr{l}(\boldsymbol{\beta})}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^T})^{-1}\frac{\partial \mathscr{l}(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}
\\[4mm]
&\frac{\partial \mathscr{l}(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}=-\sum_{i=1}^m\hat{\boldsymbol{x}_i}(y_i-p_1(\hat{\boldsymbol{x}_i};\boldsymbol{\beta}))
\\[4mm]
&\frac{\partial^2 \mathscr{l}(\boldsymbol{\beta})}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^T}=\sum_{i=1}^m\hat{\boldsymbol{x}_i}\hat{\boldsymbol{x}}^T_ip_1(\hat{\boldsymbol{x}_i};\boldsymbol{\beta})(1-p_1(\hat{\boldsymbol{x}_i};\boldsymbol{\beta}))\end{aligned}\right.\tag{17}
$$