---
aliases:
  - clustering
tags:
  - 机器学习
  - 无监督学习
  - 概述
---

# 聚类

聚类是一类无监督学习算法，根据他们的特征的相似度或者距离、将其归类到若干类，一个类是给定样本的一个子集。这里介绍一些常用的聚类算法，例如层次聚类、k-means、谱聚类、DBScan 等。

## 聚类的基本概念

下面介绍一些聚类的基本概念，包括样本之间的距离或者相似度、类或簇，类与类之间的距离。

### 相似度或者距离

聚类的对象是观测数据，或者样本集合，假设有 $n$ 个样本，每个样本由 $m$ 个属性的特征向量组成，样本集合可以使用矩阵表示，矩阵中第 $j$ 列表示第 $j$ 个元素，第 $i$ 行表示第 $i$ 个属性。

聚类的核心是相似度 (similarity) 或者距离 (distance)，有多种相似度或距离的定义，因为相似度直接影响了聚类的结果，所以其选择时聚类的根本问题。下面给出一些常见的距离度量：

#### 闵可夫斯基距离

在聚类中的可以将样本看作向量空间中的点的集合，以该空间的距离表示样本之间的相似度，常用的距离是闵可夫斯基距离，特别是欧氏距离，闵可夫斯基距离越大相似度越小，距离越小相似度越大。

给定样本集合 $X$，$X$ 是 $m$ 维实数向量空间 $R^m$ 中点的集合，其中 $x_i,x_j\in X$，$x_i\in(x_{1i},x_{2i},\cdots,x_{mi})^T,x_j\in(x_{1j},x_{2j},\cdots,x_{mj})^T$，样本 $x_i$ 与样本 $x_j$ 之间的闵可夫斯基距离 (Minkowski distance) 定义为
$$
d_{ij}=\left(\sum_{k=1}^m|x_{ki}-x_{kj}|^p\right)^{\frac{1}{p}}
$$
这里 $p\geqslant 1$，当 $p=2$ 时称为欧式距离，当 $p=1$ 时称为马哈顿距距离，当 $p=\infty$ 时称为切比雪夫距离 (Chebyshev distance)。

#### 马哈拉诺比距离

马哈拉诺比距离 (Mahalanobis distance)，简称马氏距离，也是一种常用的相似度，考虑各个分量之间的相关性与各个分量的尺度无关。马哈拉诺比距离越大相似度越小，距离越小相似度越大。

给定一个样本集合 $X$，$X=[x_{ij}]_{m\times n}$，其协方差矩阵记作 $S$，样本 $x_i$ 与样本 $x_j$ 之间的马哈拉诺比距离 $d_{ij}$ 定义为
$$
d_{ij}=[(x_i-x_j)^TS^{-1}(x_i-x_j)]^\frac{1}{2}
$$
其中
$$
x_i\in(x_{1i},x_{2i},\cdots,x_{mi})^T,x_j\in(x_{1j},x_{2j},\cdots,x_{mj})^T
$$
当 $S$ 为单位矩阵时，即样本数据的各个分量互相独立且各个分量的方差为 1 时，可以得到马氏距离是欧式距离的推广。

#### 相关系数

样本之间的相似度也可以使用相关系数 (correlation coefficient)来表示。相关系数的绝对值接近于 1，表示样本相似，越接近 0 表示样本越不相似。

样本 $x_i$ 与样本 $x_j$ 之间的相关系数定义为
$$
r_{ij}=\frac{\displaystyle\sum_{k=1}^m(x_{ki}-\bar{x}_i)(x_{kj}-\bar{x}_j)}{\displaystyle\left[\sum_{k=1}^m(x_{ki}-\bar x_i)^2\sum_{k=1}^m(x_{kj}-\bar x_j)^2\right]}
$$
其中
$$
\bar x_i=\frac{1}{m}\sum_{k=1}^mx_{ki},\quad x_j=\frac{1}{m}\sum_{k=1}^mx_{kj}
$$

#### 夹角余弦

样本之间的相似度也可以使用夹角余弦 (cosine) 来表示，夹角余弦越接近 1，表示样本越相似，越接近与 0，表示样本越不相似。

样本 $x_i$ 与样本 $x_j$ 之间的夹角余弦定义为
$$
s_{ij}=\frac{\displaystyle\sum_{k=1}^mx_{ki}x_{kj}}{\displaystyle\left[\sum_{k=1}^mx_{ki}^2\sum_{k=1}^mx_{kj}^2\right]^\frac{1}{2}}
$$

根据上述定义可以看出，使用距离度量相似度时，距离越小样本越相似；用相关系数度量相似度时，相关系数越大样本越相似。

### 类或簇

通过聚类得到的类或簇，本质是样本的子集，如果一个聚类方法假定一个样本只能属于一个类，或者类的交集为空集，那么该方法为硬聚类 (hard clustering)，否则，一个样本可以属于多个类，或类的交集不为空集，那么该方法称为软聚类 (soft clustering)。

使用 $G$ 表示类或者簇 (cluster)，用 $x_i,x_j$ 表示类中的样本，用 $n_G$ 表示 $G$ 中样本的个数，用 $d_{ij}$ 表示样本 $x_i$ 与 $x_j$ 之间的距离，类或簇有多种定义，下面给出几个常见的定义：
1. 给定 $T$ 为正数，若集合 $G$ 中任意两个样本都满足 $d_{ij}\leqslant T$，则称 $G$ 为一个簇
2. 给定 $T$ 为正数，若集合任意样本 $x_i$ 一定存在另一个样本 $x_j$ 满足 $d_{ij}\leqslant T$，则称 $G$ 为一个簇
3. 给定 $T$ 为正数，若对集合中任意样本 $x_i$, $G$ 中另一样本 $x_j$ 满足
   $$
\frac{1}{n_G-1}\sum_{x_j\in G}d_{ij}\leqslant T
$$
其中 $n_G$ 为 $G$ 中样本的个数，则称 $G$ 为一个簇
4. 设 $T$ 和 $V$ 为给定的两个正数，如果集合 $G$ 中任意两个样本的距离满足
   $$
\frac{1}{n_G(n_G-1)}\sum_{x_i\in G}\sum_{x_j\in G}d_{ij}\leqslant T
$$
则称 $G$ 为一个簇

类的特征可以通过不同的角度来刻画，常用的特征有以下三种：
1. 类的均值 $\bar x_G$，又称为类的中心
   $$
\bar x_G=\frac{1}{n_G}\sum_{i=1}^{n_G}x_i
$$
2. 类的直径 (diameter)
$$
D_G=\max_{x_i,x_j\in G}d_{ij}
$$
3. 类的样本散布矩阵 (scatter matrix) $A_G$ 与样本协方差矩阵 (covariance matrix) $S_G$
   $$
\begin{aligned}
A_G&=\sum_{i=1}^{n_G}(x_i-\bar x_G)(x_i-\bar x_G)^T\\
S_G&=\frac{1}{n_G-1}A_G\\
&=\frac{1}{n_G-1}\sum_{i=1}^{n_G}(x_i-\bar x_G)(x_i-\bar x_G)^T
\end{aligned}
$$

### 类与类之间的距离

下面考虑类 $G_p$ 与类 $G_q$ 之间的距离 $D(p, q)$，也称为连接 (linkage)，类与类之间的距离也有多种定义。设类 $G_p$ 包含 $n_p$ 个样本，$G_q$ 包含 $n_q$ 个样本，分别使用 $\bar x_p$ 和 $\bar x_q$ 表示 $G_p$ 和 $G_q$ 的均值，即类的中心。
1. 最短距离或者单连接 (single linkage)
   定义类 $G_p$ 的样本与 $G_q$ 的样本之间的最短距离为两类之间的距离
   $$
D_{pq}=\min\{d_{ij}|x_i\in G_p,x_j\in G_q\}
$$
2. 最长距离或完全连接 (complete linkage)
   定义类 $G_p$ 的样本与 $G_q$ 的样本之间的最长距离为两类之间的距离
   $$
D_{pq}=\max\{d_{ij}|x_i\in G_p,x_j\in G_q\}
$$
3. 中心距离
定义类 $G_p$ 与 $G_q$ 的中心 $\bar x_p$ 与 $\bar x_q$ 之间的距离为两类之间的距离
$$
D_{pq}=d_{\bar x_p\bar x_q}
$$
4. 平均距离
定义类 $G_p$ 与类 $G_q$ 任意两个样本之间距离的平均值为两类之间的距离
$$
D_{pq}=\frac{1}{n_pn_q}\sum_{x_i\in G_p}\sum_{x_j\in G_q}d_{ij}
$$

## 层次聚类

层次聚类假设类别之间存在层次结构，将样本聚到层次化的类中，层次聚类又有聚合 (agglomerative) 或自上而下 (bottom-up) 聚类，分裂 (divisive) 或自上而下 (top-down) 聚类两种方法。因为每个样本只属于一个类，所以层次聚类属于硬聚类。

聚合聚类开始将每个样本分到一个类中，之后将每个相距最近的类合并，建立一个新的类，重复此操作直到满足停止条件。分裂聚类开始将所有样本分到一个类中，之后将已有类中相距最远的样本分到两个新的类，重复此操作直到满足停止条件。

由此可以知道，聚合聚类需要预先确定下面的三个要素：
1. 距离或者相似度
2. 合并规则
3. 停止条件

根据这些要素的不同组合，就可以构成不同的聚类方法。

## k 均值聚类

k 均值聚类是基于样本合集划分的聚类方法，k 均值聚类将样本集合划分为 k 个子集，构成 k 个类，将 n 个样本分到 k 个类中，每个样本到其所属中心的距离小，每个样本只能属于一个类，所以 k 均值是硬聚类方法。

k 均值采用欧式距离的平方作为样本之间的距离度量。即
$$
d(x_i,x_j)=\sum_{k=1}^m(x_{ki}-x_{kj})^2=\Vert x_i-x_j\Vert^2
$$
然后定义样本与其所属的类的中心之间的距离的总和为损失函数
$$
W(C)=\sum_{l=1}^k\sum_{C(i)=l}\Vert x_i-\bar x_l\Vert^2
$$
该式也称为能量，表示相同类中的样本相似程度，k 均值就是求解最优化问题
$$
\begin{aligned}
C^*&=\arg\min_C W(C)\\
&=\arg\min_C\sum_{l=1}^k\sum_{C(i)=l}\Vert x_i-\bar x_l\Vert^2
\end{aligned}
$$

相似的样本被聚到同类时，损失函数就最小，但是这是个组合优化问题，其分类复杂度数指数级。事实上，k 均值聚类的最优解求解问题是 NP 困难问题。在现实中我们采用迭代的方式可以求得局部最优解，即 [[机器学习/EM 算法|EM 算法]]的思想。

具体来说，步骤如下：
1. 初始化，令 $t=0$，随机选择 $k$ 个样本点作为初始聚类中心 $m^{(0)}=(m^{(0)}_1,\cdots,m^{(0)}_k)$
2. 对样本进行聚类。对固定的类中心 $m^{(t)}$，计算每个样本到类中心的距离，将每个样本指派到与其最近的中心的类，构成距离结果 $C^{(t)}$
3. 计算新的类的中心，对聚类结果 $C^{(t)}$，计算当前各个类中的样本均值，作为新的类中心 $m^{(t+1)}$
4. 如果迭代结果收敛，则输出结果，否则令 $t=t+1$，返回 2

k-mean 聚类算法的复杂度为 $O(mnk)$，其中 $m$ 是样本维数，$n$ 是样本个数，$k$ 是类别个数。

k 均值算法有以下特点：
1. 基于划分的聚类方法
2. 类别 k 事先指定
3. 以欧式距离的平方表示样本之间的距离，以中心或者样本的均值表示样本的类别
4. 以样本和其所属的类的中心之间的距离总和作为最优化的目标函数
5. 得到的类别是凸的
6. 算法是迭代算法，不能保证得到全局最优值

类别数 k 需要实现指定，一般使用不同的 k 值尝试，观测结果的优劣，推测最优的 k 值。聚类的结果可以使用类的平均直径来评估，一般的，当类别数变小时，平均直径会增加；当类别数变大超过一定的值后，平均直径会不变，而这个值就是最优的 k 值。可以采用二分查找快速的找到最优的 k 值。

## 密度聚类算法

这里介绍典型的密度聚类算法 DBSCAN。DBSCAN (Density-Based Spatial Clustering of Applications with Noise) 全名具有噪声的基于密度的聚类方法，与 [[#k 均值聚类]] 方法不同，DBSCAN 可以发现任意形状的簇，可以用于非凸的数据集。

DBSCAN 是基于一组邻域来描述样本集的紧密程度的，参数($\varepsilon$, MinPts)用来描述邻域的样本分布紧密程度。其中，$\varepsilon$ 描述了某一样本的邻域距离阈值，MinPts 描述了某一样本的距离为 $\varepsilon$ 的邻域中样本个数的阈值。对于样本集 $D=\{x_1,x_2,\cdots,x_N\}$，DBSCAN 的密度描述如下：
1. $\varepsilon$ 邻域：对于 $x_j\in D$，其邻域定义为距离 $x_j$ 的距离不大于 $\varepsilon$ 的样本子集
2. 核心对象：如果对象的 $\varepsilon$ 邻域内至少包含最小数目 MinPts 的对象，则这个对象称为核心对象
3. 直接密度可达：如果 $x_i$ 在 $x_j$ 的邻域内，而 $x_j$ 是一个核心对象，则成对象 $x_i$ 从对象 $x_j$ 出发时直接密度可达的
4. 密度可达：对于样本 $x_i$ 和 $x_j$，如果存在一个对象链 $p_1,p_2,\cdots,p_T$，满足 $p_1=x_i,p_n=x_j$，且对于任意的 $p_{t+1}$ 是 $p_t$ 直接密度可达的，则成 $x_j$ 由 $x_i$ 密度可达
5. 密度相连：对于 $x_i$ 和 $x_j$，如果存在 $x_z$ 使得 $x_i$ 与 $x_j$ 由 $x_z$ 密度可达，则称 $x_i$ 与 $x_j$ 密度相连

根据定义，可以得到密度可达是直接密度可达的传递闭包，是非对称的，只有核心对象之间密度互相密度可达。而密度相连则是一种对称的关系，这是基于密度聚类算法运算的基础。

DBSCAN 聚类算法的思想很简单，即：**由密度可达关系导出的最大密度相连的样本集合，即为我们最终聚类的一个类别，或者说一个簇**。DBSCAN 的簇里面可以有一个或者多个核心对象。如果只有一个核心对象，则簇里其他的非核心对象样本都在这个核心对象的 $\varepsilon$ -邻域里；如果有多个核心对象，则簇里的任意一个核心对象的 $\varepsilon$ -邻域中一定有一个其他的核心对象，否则这两个核心对象无法密度可达。这些核心对象的 $\varepsilon$ -邻域里所有的样本的集合组成的一个 DBSCAN 聚类簇。

DBSCAN 算法可以描绘如下：
1. 初始化核心对象集合 $\Omega=\varnothing$，初始化聚类簇数 $k=0$，初始化未访问样本集合 $\Gamma=D$，簇划分 $C=\varnothing$
2. 对于 $j=1,2,\cdots,m$ 按下面的步骤找出所有核心对象
	1. 通过距离度量方式，找到样本 $x_j$ 的 $\varepsilon$ 邻域样本子集
	2. 如果子样本集个数大于 MinPts，将该样本 $x_j$ 加入核心对象样本集合中，即 $\Omega=\Omega\cup \{x_j\}$
3. 如果核心对象集合为 $\varnothing$，则算法结束
4. 在核心对象集合 $\Omega$ 中，随机选择一个核心对象 $o$，初始化当前簇核心对象队列 $\Omega_{\text{cur}}=\{o\}$，初始化类别序号 $k=k+1$，初始化当前簇样本集合 $C_k=\{o\}$，更新未访问样本集合 $\Gamma=\Gamma-\{o\}$
5. 如果当前簇核心队列 $\Omega_{\text{cur}}=\varnothing$，则当前聚类簇 $C_k$ 生成完毕，更新簇划分集合 $C=\{C_1,C_2,\cdots,C_k\}$，更新核心对象集合 $\Omega=\Omega-C_k$，转入步骤 3。否则更新核心对象集合 $\Omega=\Omega-C_k$
6. 在当前簇核心对象队列 $\Omega_{\text{cur}}$ 中取出一个核心对象 $o'$，通过邻域聚类阈值 $\varepsilon$ 找出所有 $\varepsilon$ 邻域子样本集，更新当前样本集合，更新未访问样本集合，转入步骤 5

## 谱聚类

[[谱聚类]]是一种基于图论的无监督聚类算法，它通过将数据集表示成一个图，然后对这个图进行划分，将相似的数据点分到同一个簇中。其实现的主要过程包括两个步骤：首先，对原始数据进行相似度计算，并基于计算结果构造相应的相似度矩阵；其次，通过对相似度矩阵进行谱分解，从而获得数据集的特征向量，进而对数据点进行划分。

谱聚类不仅可以用于聚类分析，还可用于特征提取、图像分割、社区发现等领域。相较于传统的聚类算法，谱聚类具有优秀的性能和可扩展性，并且对于复杂的数据结构有着出色的适应性。

## 聚类评价指标

常用的聚类评价方法有内部评价法和外部评价法。

### 内部评价法

内部评价法指仅使用聚类的结果进行评价，评价时不知道真实的分类标签。

#### 轮廓系数

轮廓系数是常用的聚类评价指标。当聚类的目标类别未知时，可以使用轮廓系数评价聚类结果。轮廓系数的取值范围是 $[-1,1]$，轮廓系数越大，聚类效果越好。相反，轮廓系数越接近-1，聚类效果越差。对于一个样本，轮廓系数的计算方式为
$$
s = \frac{a+b}{\max(a,b)}
$$
其中 $a$ 是该样本与簇内其他样本的平均距离，$b$ 是该样本与其它簇样本的平均距离。对于每一个样本点，我们都可以计算其轮廓系数，对于一个聚类结果，我们使用所有数据的轮廓系数的均值来表示整个聚类的轮廓系数。

轮廓系数更加偏向于凸的簇，因此对于 k-means 等寻找凸集的聚类算法时，轮廓系数是一个较好的评价指标。但是对于 DBSCAN 等可以发现非凸的聚类算法时，轮廓系数就不能作为一个较优的选择。

#### Calinski-Harabasz 指数

Calinski-Harabasz 指数是一种运算非常快的评价指标。Calinski-Harabasz 指数通过计算类中各点与类中心的距离平方和来度量类内的紧密度，通过计算各类中心点与数据集中心点距离平方和来度量数据集的分离度，CH 指标由分离度与紧密度的比值得到。从而，CH 越大代表着类自身越紧密，类与类之间越分散，即更优的聚类结果。计算方式为
$$
s=\frac{SS_B/(k-1)}{SS_W/(N-k)}
$$
其中 $SS_B$ 是类间方差，$SS_W$ 是类内方差，$k$ 代表聚类类别数，$N$ 代表全部样本数。$SS_B$ 的计算方式为
$$
\begin{aligned}
SS_B&=\mathrm{tr}(B_k)\\
B_k&=\sum_{q=1}^kn_q(c_q-c_E)(c_q-c_E)^T
\end{aligned}
$$
其中 $c_q$ 是第 $q$ 类的质点，$c_E$ 是所有类的中心点，$n_q$ 是类 $q$ 数据点的总数。$SS_W$ 的计算方式为
$$
\begin{aligned}
SS_W&=\mathrm{tr}(W_k)\\
W_k&=\sum_{q=1}^k\sum_{x\in C_q}(x-c_q)(x-c_q)^T
\end{aligned}
$$
其中 $C_q$ 是类 $q$ 所有数据点的集合，$c_q$ 是类 $q$ 的中心点。

### 外部评价法

外部评价法指在已知标签的情况下对模型进行评价的方法。一般用于在拥有少量标签数据时，通过外部评价法对少量数据得到的结果进行评价，得到最好的模型再用于其他数据。
#### 混淆矩阵

下面几个由混淆矩阵定义得到的评价指标，包括纯度、兰德系数、F 值、调整兰德系数。聚类结果得到的混淆矩阵定义如下

|      | 同簇 | 非同簇 |
| :---- :|: ----: | :------: |
| 同类 |   TP   | FN       |
| 非同类     |   FP   |  TN      |

根据上述的混淆矩阵，我们可以得到下面的几个指标
$$
\begin{aligned}
RI&=\frac{TN+TP}{TP+FP+FN+TN}\\
Precision&=\frac{TP}{TP+FP}\\
Recall&=\frac{TP}{TP+FN}\\
F_\beta&=(1+\beta^2)\frac{Precision + Recall}{\beta^2\cdot Precision+ Recall}
\end{aligned}
$$

### 调整兰德系数

调整兰德系数是对于兰德系数的优化，其计算方式为
$$
ARI=\frac{RI-E[RI]}{\max(RI)-E[RI]}
$$
调整兰德系数对簇的形状不做要求，所以可以用于不同聚类方法性能的比较。