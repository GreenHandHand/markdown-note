---
aliases:
  - maximum entropy model
tag:
  - 机器学习
  - 监督学习
---

# 最大熵模型

最大熵模型 (maximum entropy model) 由最大熵原理推导实现，这里首先叙述一般的最大熵原理，然后讲解最大熵模型的推导，最后给出最大熵模型学习的形式。

## 最大熵原理 

最大熵原理是概率模型学习的一个准则，最大熵原理认为，学习概率模型时，在所有可能的概率模型中，熵最大的模型就是最好的模型。

假设离散变量 $X$ 的概率分布为 $P(X)$，则其熵为
$$
H(P)=-\sum_xP(x)\log P(x)
$$
熵值满足不等式
$$
0\leqslant H(P)\leqslant \log|X|
$$
其中 $|X|$ 是 $X$ 取值的个数，当且仅当 $X$ 的分布是均匀分布时右边的等号成立，即当 $X$ 服从均匀分布时，熵最大。直观的理解，最大熵原理认为要选择的模型需要满足约束条件，在没有更多信息的情况下，这些不确定的部分都是等可能的，最大熵原理通过最大化来表示等可能性。

## 最大熵模型

最大熵原理是统计学习的一般原理，将它应用到分类问题中得到最大熵模型。最大熵模型是建立在离散的空间中的，对于连续变量的特征函数，我们需要将其离散化。

假设分类模型是一个条件概率分布 $P(Y|X)$，$X\in\mathcal X$ 表示输入，$Y\in\mathcal Y$ 表示输出，这个模型表示对于给定的输入 $X$，以条件概率 $P(Y|X)$ 输出 $Y$。给定训练数据集
$$
T=\{(x_1,y_1),\cdots,(x_N,y_N)\}
$$
我们可以得到 $P(X)$ 与 $P(X,Y)$ 的经验分布，即
$$
\begin{aligned}
&\tilde P(X=x,Y=y)=\frac{v(X=x,Y=y)}{N}\\
&\tilde P(X=x)=\frac{v(X=x)}{N}
\end{aligned}
$$
其中 $v(X=x,Y=y)$ 表示样本的中的频数，$N$ 表示样本容量。使用特征函数 (feature function) 可以描述输入 $x$ 与输出 $y$ 之间的某一个事实，其定义为
$$
f(x,y)=\begin{cases}1,&x与y满足某一事实\\0,&否则\end{cases}
$$
它是一个二值函数，表示特征是否存在，可以理解为特征的 one-hot 编码，当这特征存在时，特征函数的值为 1，否则为 0。

特征函数关于经验分布的期望使用 $E_{\tilde P}(f)$ 来表示
$$
E_{\tilde P}(f)=\sum_{x,y}\tilde P(x,y)f(x,y)
$$
特征函数关于模型 $P(Y|X)$ 与经验分布 $\tilde P(X)$ 的期望值使用 $E_P(f)$ 来表示
$$
E_P(f)=\sum_{x,y}\tilde P(x)P(y|x)f(x,y)
$$

如果模型可以提取到训练数据中的信息，那么模型应当满足
$$
E_{\tilde P}(f)=E_P(f)
$$
即
$$
\sum_{x,y}\tilde P(x)P(y|x)f(x,y)=\sum_{x,y}\tilde P(x,y)f(x,y)
$$

最大熵模型将上式作为约束条件，如果有 $n$ 个特征函数，那么就有 $n$ 个约束条件。假设满足所有约束条件的模型集合为
$$
\mathcal C\equiv\{P\in\mathcal P|E_P(f_i)=E_{\tilde P}(f_i)\}
$$
定义在条件概率分布 $P(Y|X)$ 上的条件熵为
$$
H(P)=-\sum_{x,y}\tilde P(x)P(y|x)\log P(y|x)
$$
我们将模型集合 $\mathcal C$ 中条件熵 $H(P)$ 最大的模型称为最大熵模型，式中的对数为自然对数。

## 最大熵模型的学习

最大熵模型的学习过程就是求解使熵最大的条件概率分布的过程，可以形式化为约束最优化问题。
$$
\begin{aligned}
\min_{P\in \mathcal C}\quad&-H(P)=\sum_{x,y}\tilde P(x)P(y|x)\log P(y|x)\\[2mm]
s.t.\quad&E_P(f_i)=E_{\tilde P}(f_i),\quad i=1,2,\cdots,n\\[2mm]
&\sum_yP(y|x)=1
\end{aligned}
$$

求解上述问题可以使用拉格朗日问题的对偶问题进行求解，即引入拉格朗日乘子，定义拉格朗日函数如下
$$
\begin{aligned}
L(P,\omega)&\equiv-H(P)+\sum_{i=1}^n\omega_i(E_P(f_i)-E_{\tilde P}(f_i))+\omega_0(\sum_yP(y|x)-1)\\
&=\sum_{x,y}\tilde P(x)P(y|x)\log P(y|x)+\omega_0(\sum_yP(y|x)-1)+\\
&\,\,\quad\sum_{i=1}^n\omega_i\left(\sum_{x,y}\tilde P(x)P(y|x)f_i(x,y)-\sum_{x,y}\tilde P(x,y)f_i(x,y)\right)
\end{aligned}
$$

原始问题等价与求解最优化问题
$$
\min_{P\in\mathcal C}\max_\omega L(P,\omega)
$$
其对偶问题为
$$
\max_\omega\min_{P\in\mathcal C}L(P,\omega)
$$
由于拉格朗日函数是关于 $P$ 的凸函数，上述最优化算式等价，我们可以通过求解对偶问题来求解原始问题。首先需要求解对偶问题中的极小化问题，具体的，令 $\varPsi(\omega)=\min L(P,\omega)$，称为对偶函数，同时将解记为 $P_\omega$，满足
$$
P_\omega=\arg\min_{P\in\mathcal C}L(P,\omega)=P_\omega(y|x)
$$
由于拉格朗日函数为凸函数，可以直接计算偏导数得到
$$
\begin{aligned}
\frac{\partial L(P,\omega)}{\partial P(y|x)}&=\sum_{x,y}\tilde P(x)(\log P(y|x)+1)-\sum_y\omega_0\\&-\sum_{x,y}\left(\tilde P(x)\sum_{i=1}^n\omega_if_i(x,y)\right)\\
&=\sum_{x,y}\tilde P(x)\left(\log P(y|x)+1-\omega_0-\sum_{i=1}^n\omega_if_i(x,y)\right)
\end{aligned}
$$
令偏导等于 0，可以计算得到
$$
P(y|x)=\exp\left(\sum_{i=1}^n\omega_if_i(x,y)+\omega_0-1\right)
$$
由于 $\sum_yP(y|x)=1$ 有
$$
P_\omega(y|x)=\frac{1}{Z_\omega}\exp\left(\sum_{i=1}^n\omega_if_i(x,y)\right)
$$
其中
$$
Z_\omega(x)=\sum_y\exp\left(\sum_{i=1}^n\omega_if_i(x,y)\right)
$$

我们将 $Z_\omega(x)$ 称为规范化因子，$f_i(x,y)$ 是特征函数，$\omega_i$ 是特征的权值，上式表示的 $P_\omega(y|x)$ 就是最大熵模型，这里的 $\omega$ 表示的是最大熵模型中的参数向量，之后求解 $\max\varPsi(\omega)$，将其解标记为 $\omega^*$，就是说，通过对偶函数的方法，可以将最大熵模型的学习转化为求解最优值向量 $\omega^*$ 的最优化问题
$$
\omega^*=\arg\max_\omega \varPsi(\omega)
$$
我们可以通过最优化算法求得上式的极值。即优化
$$
\omega^*=\arg\max_\omega\sum_{x,y}\tilde P(x,y)\sum_{i=1}^n\omega_if_i(x,y)+\sum_x\tilde P(x)\log Z_\omega(x)
$$