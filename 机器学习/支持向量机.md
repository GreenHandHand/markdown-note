---
aliases:
  - svm
  - support vector machine
tag:
  - 机器学习
  - 线性
  - 监督学习
---

# 支持向量机

支持向量机（SVM）是一种二分类机器学习算法，通过在空间中寻找一个能将所有样本点最好的分开的超平面实现二分类。支持向量机算法的思路就是找到这个超平面。

## 思想

线性可分支持向量机通过寻找一个超平面来将线性可分的目标数据分开，将距离超平面最近的几个向量作为**支持向量**，将异类支持向量之间的距离作为**间隔**。算法的目标就是找到能最大化间隔的超平面所对应的参数。

以线性可分支持向量机为基础，通过将非线性可分数据映射到高维坐标的方法将其变为线性可分的向量，在通过之前的算法进行求解。直接将向量映射到高维度过于繁杂，因此引入了核函数的方法，来简化计算。核函数是一类映射到特征空间后的内积等于他们在原始空间的内积在特征空间的映射的函数，通过这些函数我们就不必计算高维度中的内积，极大的简化了计算。

对于一些线性不可分的数据，直接向高维度映射容易造成过拟合的情况，在这种情况下，我们允许支持向量机在某种程度上出错，即可以存在向量在一定程度上不满足支持向量机，这样的方法被称为**软间隔**，这样可以得到泛化能力较好的支持向量机模型。这样允许模型在一定程度上出错的方法也被称为**正则化**，在机器学习中有着广泛的应用。

## 线性可分支持向量机

给定训练样本集 $D=\{(\vec{x}_1, y_1), (\vec x_2, y_2), \cdots, (\vec x_m, y_m)\},y_i\in\{-1, +1\}$，我们的目的就是在样本空间中找到一个超平面，该超平面能最好的将样本划分开。

设超平面的方程为：

$$
\vec w^T \vec x + b = 0
$$

易知，向量 $\vec w$ 与位移量 $b$ 唯一决定了该超平面，于是分类器表示如下：

$$
y = \mathrm {sign}(\vec w^T\vec x + b)\tag{1}
$$

对于样训练样本集有：

$$
\begin{cases}\vec w^T \vec x + b \ge 0, y = +1\\\vec w^T \vec x + b < 0, y = -1\end{cases} \Rightarrow y(\vec w^T\vec x + b) \ge 0 \tag{2}
$$

已知空间的点到超平面距离为：

$$
r = \frac{|\vec w^T\vec x + b|}{|\!|w|\!|}
$$

我们要找的超平面应当满足 $r$ 最小，即：

$$
\begin{aligned}\max_{\vec w, b}\quad&\min_{i=1,\cdots,m}\frac{1}{|\!|\vec w|\!|}|\vec w^T\vec x_i + b|\\s.t.\quad & y_i(\vec w^T\vec x_i + b)\ge 0, i=1,\cdots,m\end{aligned}\tag{3}
$$

其中 $\max\,\min\dfrac{1}{|\!|\vec w|\!|}|\vec w^T\vec x + b|=\max \dfrac{1}{|\!|\vec w|\!|}\min |\vec w^T\vec x + b|$

我们知道 $|\vec w^T\vec x + b|与|\lambda\vec w^T\vec x + \lambda b|$ 表示同一条直线，为了简化计算，我们可以取 $\min |\vec w^T\vec x + b| = 1$，满足这项的向量就称为支持向量，在可视化找中就表现为距离超平面最近的向量，上式转变为：

$$
\begin{aligned}\max_{\vec w, b}\quad &\frac{1}{|\!|\vec w|\!|}\\s.t.\quad & y_i(\vec w^T\vec x_i + b)\ge 1,i=1,\cdots,m\end{aligned}
$$

该优化式等价于：

$$
\begin{aligned}\min_{\vec w, b} \quad&\frac{1}{2}|\!|\vec w|\!|^2\\s.t.\quad& y_i(\vec w^T\vec x_i + b)\ge 1, i=1,\cdots,m\end{aligned}\tag{4}
$$

上式很明显的是一个有约束凸优化问题，使用众多的凸优化方法可以解决。但是这个问题存在一个更简单的解决方法，考虑上式的拉格朗日乘子式，即：

$$
\begin{aligned}\mathscr{L}(\vec w, b, \vec\alpha)  = \frac{1}{2}|\!|\vec w|\!|^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\vec w^T\vec x_i + b))\end{aligned}\tag{5}
$$

由于式 (4)是凸优化问题，由多元函数极值的必要条件有：

$$
\left\{\begin{aligned}&\frac{d\mathscr{L}}{d\vec w}=\vec w - \sum_{i=1}^m \alpha_iy_ix_i=0\\
&\frac{d\mathscr{L}}{d b}=\sum_{i=1}^m\alpha_iy_i = 0
\end{aligned}\right.\tag{6}
$$

于是可以得到原问题的对偶问题：

$$
\begin{aligned}\max_{\vec \alpha}\inf_{\vec w,b}\quad &\frac{1}{2}|\!|\vec w|\!|^2 + \sum_{i=1}^m\alpha_i(1 - y_i(\vec w^T\vec x_i + b))\end{aligned}
$$

带入 $(6)$ 与 KKT 条件有：

$$
\begin{aligned}\max_{\vec \alpha} \quad &\sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m \alpha_i\alpha_jy_iy_j\vec x_i^T\vec x_j \\ s.t.\quad & \sum_{i=1}^m\alpha_iy_i=0\\var.\quad &\alpha_i \ge 0\end{aligned}\tag{7}
$$

求解上式得到 $\vec \alpha^*$ 的值，于是由 $(6)$ 可以得到：

$$
w^* = \sum_{i=1}^m\alpha_i^*y_i\vec x_i\tag{8}
$$

我们知道，对于支持向量，有 $|\vec w\vec x + b | = 1$，即满足下面的式子：

$$
y(\vec w^T\vec x + b) = 1
$$

其中 $y=\pm1$，于是我们可以得到 $b$ 的值：

$$
b^* = y - \vec w^{*T}\vec x\tag{9}
$$

理论上，使用任何一个支持向量都可以求得 $b$ 的值。在实际使用中，我们常使用平均值最为最终的 b。

## 软间隔

实际应用中，存在整体上看线性可分但是不严格线性可分的数据，对于这种情况。如果直接向高维度映射，容易造成过拟合的情况。我们允许模型在一定程度上出错，即对式 $(4)$ 进行如下修改：

$$
\begin{aligned}\min_{\vec w, b} \quad&\frac{1}{2}|\!|\vec w|\!|^2 + C\sum_{i=0}^m\xi_i\\s.t.\quad& y_i(\vec w^T\vec x_i + b)\ge 1-\xi_i, i=1,\cdots,m\\var.\quad&\xi_i \ge0,i=1,\cdots,m\end{aligned}\tag{10}
$$

其中 $\xi_i$ 是松弛变量，C 是对分类错误的惩罚因子，C 越大对分类错误的惩罚力度越大，该变量允许模型在一定程度上出错。于是式 $(5)$ 变化为：

$$
\tiny
\begin{aligned}\mathscr{L}(\vec w, b, \vec\alpha,\vec \xi,\vec \mu)  = \frac{1}{2}|\!|\vec w|\!|^2 +C\sum_{i=1}^m\xi_i+ \sum_{i=1}^m\alpha_i(1 - y_i(\vec w^T\vec x_i + b))-\sum_{i=0}^m\mu_i\xi_i\end{aligned}\tag{11}
$$

对上式的偏导置零有：

$$
\sum_{i=0}^m\alpha_iy_i\vec x_i=\vec w\\[1.5mm] \sum_{i=0}^m\alpha_iy_i=0\\[1.5mm]\alpha_i+\mu_i=C
$$

于是式 $(7)$ 变换为：

$$
\begin{aligned}\max_{\vec \alpha} \quad &\sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m \alpha_i\alpha_jy_iy_j\vec x_i^T\vec x_j \\ s.t.\quad & \sum_{i=1}^m\alpha_iy_i=0\\var.\quad & 0 \le\alpha_i \le C\end{aligned}\tag{12}
$$

由 KKT 条件可以得到：

$$
\left\{\begin{aligned}&\alpha_i=0,&y_i(\vec w\vec x_i+b)\ge1\\
&0<\alpha_i< C&y_i(\vec w\vec x_i + b)= 1\\
&\alpha_i = C&y_i(\vec w\vec x_i + b)\le 1\end{aligned}\right.\tag{13}
$$

我们求出满足条件的 $\alpha^*$ 边可以得到模型的参数 $\vec w$ 和 $b$。

## 线性不可分支持向量机

对于线性不可分的数据集，我们可以先将数据向高维度映射，然后再高维度中找到一个超平面将数据划分。但是向高维度映射容易造成维度灾难的问题，并且计算量难以估计。人们使用核函数解决了这个问题。

### 核函数

核函数是一类函数，这类函数满足：

$$
\kappa(\vec x_1, \vec x_2) = \braket{\phi(\vec x_1), \phi(\vec x_2)}
$$

即样本的内积通过核函数映射后的结果等于映射后的结果的内积。通过核函数的这个性质，我们只需要将式 $(12)$ 中的内积换成核函数的内积即可将其转换到高维度空间中，而省却了大量复杂的计算。

常用的核函数有以下几种：

* 线性核函数：$\kappa(\vec x_1, \vec x_2) = \vec x_1^T\vec x_2$
* 多项式核函数：$\kappa(\vec x_1,\vec x_2)=(\vec x_1^T\vec x_2)^d$，其中 $d\ge0$ 是多项式的次数
* 高斯核函数：$\kappa(\vec x_1, \vec x_2) = \exp(-\dfrac{|\!|\vec x_1 - \vec x_2|\!|^2}{2\sigma^2})$，其中 $\sigma>0$ 是高斯核的带宽
* 拉普拉斯核函数：$\kappa(\vec x_1, \vec x_2)=\exp(-\dfrac{|\!|\vec x_1 - \vec x_2|\!|}{2\sigma})$，其中 $\sigma>0$
* Sigmoid 核函数：$\kappa(\vec x_1, \vec x_2) = \tanh(\beta \vec x_1^T\vec x_2 + \theta)$，其中 $\beta > 0,\theta < 0$

此外，还可以通过核函数的组合得到：

* 若 $\kappa_1,\kappa_2$ 是核函数，则其任意线性组合也是核函数
* 若 $\kappa_1,\kappa_2$ 是核函数，则核函数的直积也是核函数，即 $\kappa_1\otimes\kappa_2(\vec x_1,\vec x_2)=\kappa(\vec x_1,\vec x_2)\kappa_2(\vec x_1,\vec x_2)$
* 若 $\kappa$ 是核函数，则对于任意函数 $g(x)$ 有：$\kappa'(\vec x_1, \vec x_2)=g(\vec x_1)\kappa(\vec x_1, \vec x_2)g(\vec x_2)$ 是核函数

### 线性不可分支持向量机

对于这类数据，只需要将线性支持向量机中的向量内积换成核函数的内积即可，这也是我们使用对偶问题来求解的一个原因。

$$
\begin{aligned}\max_{\vec \alpha} \quad &\sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m \alpha_i\alpha_jy_iy_j\kappa(\vec x_i,\vec x_j) \\ s.t.\quad & \sum_{i=1}^m\alpha_iy_i=0\\var.\quad & 0 \le\alpha_i \le C\end{aligned}\tag{14}
$$

于是可以得到结果如下：

$$
w = \sum_{i=0}^{m}\alpha_iy_i\phi(x_i)\\[1.5mm]b = y_k - \sum_{i = 1}^{m}\alpha_iy_i\kappa(\vec x_i, \vec x_k)\tag{15}
$$

所得到的分类模型如下：

$$
f(\vec x) = sgn(\sum_{i=1}^m\alpha_iy_i\kappa(\vec x_i, \vec x)+b)\tag{16}
$$

## SMO 算法

由支持向量机的过程不难发现，最主要的难点在与如何求解式 $(7)$ 得到我们需要的 $\vec \alpha$，历史上提出了很多高效的算法可以求解 $\vec \alpha$，这里我们使用 SMO 算法。

SMO 算法通过固定两个 $\alpha$ 来进行迭代操作（因为如果只选取一个 $\alpha$ 的话一旦值改变就不满足 KKT 条件中的等式条件了），不失一般性的，我们假设固定了 $\alpha_1,\alpha_2$，这时式 $(7)$ 变为了：

$$
\small
\begin{aligned}\max_{\alpha_1,\alpha_2}\quad&\alpha_1+\alpha_2 -\alpha_1y_1\sum_{i=3}^m\alpha_iy_i\kappa(\vec x_1,\vec x_i)-\alpha_2y_2\sum_{i=3}^m\alpha_iy_i\kappa(\vec x_2, \vec x_i)\\&-\frac{1}{2}\alpha_1^2\kappa(\vec x_1, \vec x_1)-\frac{1}{2}\alpha_2^2\kappa(\vec x_2, \vec x_2)-\alpha_1\alpha_2y_1y_2\kappa(\vec x_1, \vec x_2)\\[3mm]s.t.\quad&\alpha_1 y_1 + \alpha_2y_2 = c = -\sum_{k \ne 1, 2} \alpha_ky_k\\[3mm]var.\quad& \alpha_1\ge0,\alpha_2\ge0\end{aligned}\tag{17}
$$

很明显式 $(10)$ 是一个二次规划式，这时我们不需要通过数值计算方法就可以求得上式的解，之后再重新选取 $\alpha_1$ 与 $\alpha_2$，直到满足 KKT 条件，迭代结束。下面是上式的计算过程：

利用等式关系 $\alpha_1=(c-\alpha_2y_2)y_1$ 可以将上式转换成一个变量的函数 $L(\alpha_2)$，将 $L(\alpha_2)$ 的导数置零可以得到：

$$
\tiny
\begin{aligned}&\frac{\partial L}{\partial\alpha_1}=-y_1y_2\\&\frac{\partial L}{\partial\alpha_2}=1-y_1y_2+y_2(v_1-v_2)+\alpha_1y_1y_2\kappa_{11}-\alpha_2\kappa_{22}+\alpha_2\kappa_{12}-\alpha_1y_1y_2\kappa_{12}=0\end{aligned}\tag{18}
$$

其中 $v_n=\sum\limits_{i=3}^m\alpha_iy_i\kappa(\vec x_i,\vec x_n),\kappa_{ij}=\kappa(\vec x_i, \vec x_j)$，由于 $f(\vec x_1)=\vec w^T\vec x + b = \sum\limits_{i=1}^m\alpha_iy_i\kappa(\vec x_i, \vec x_1)$，于是可以得到：

$$
v_1 = f(\vec x_1) - b - \alpha_1y_1\kappa_{11}-\alpha_2y_2\kappa_{12}
$$

同理，

$$
v_2=f(\vec x_2) - b - \alpha_1y_1\kappa_{12}-\alpha_2y_2\kappa_{22}
$$

带入 $(18)$ 中并移项有，利用 $\alpha_1^{old}y_1+\alpha_2^{old}y_2=c$ 消去 $c$ 可以得到：

$$
\tiny
\alpha_2^{new}(\kappa_{11}-2\kappa_{12}+\kappa_{22})=\alpha_2^{old}(\kappa_{11} - 2\kappa_{12}+\kappa_{22}) + 1-y_1y_2+y_2f(x_2)-y_2f(x_1)\tag{19}
$$

注意到：$E_i=y_i - f(x_i)$，因此 $(19)$ 变为：

$$
\alpha_2^{new}=\alpha_2^{old} + \frac{(E_1 - E_2)}{\eta}\tag{20}
$$

其中，$\eta=\kappa_{11} - 2\kappa_{12} + \kappa_{22}$。由 $\alpha_1^{old}y_1+\alpha_2^{old}y_2=\alpha_1^{new}y_1 + \alpha_2^{new}y_2=c$ 可以得到：

$$
\alpha_1^{new} = y_1y_2(\alpha_2^{new}-\alpha_2^{old}) + \alpha_1^{old}\tag{21}
$$

下面讨论 $\alpha_2$ 的取值范围，由于 $0\le\alpha_2\le C$，因此：

![[1680155892968.png]]
可以得到：

* $y_1\ne y_2,H=\min(C,C+\alpha_2-\alpha_1),L=\max(0,\alpha_2-\alpha_1)$
* $y_1=y_2,H=\min(C,\alpha_1+\alpha_2),L=\max(0, \alpha_1+\alpha_2-C)$

因此 $\alpha$ 的更新公式应当为：

$$
\alpha_2^{old}\leftarrow\begin{cases}H,&\alpha_2^{new}>H\\\alpha_2^{new},&L\le\alpha_2^{new}\le H\\L,&\alpha_2^{new}<L\end{cases}
$$

由于 $b$ 的值与 $\alpha$ 的值相关，所以每次更新 $\alpha$ 的值后也要同时更新 $b$ 的值，我们可以使用 $\alpha_1、\alpha_2$ 中任意一个满足 $0\le\alpha\le C$ 的数更新，一般而言，如果两个数都满足，我们使用他们计算出的 $b$ 的均值作为新的 $b$ 值。

关于 $\eta$ 的范围，一般而言 $\eta>0$，只有以下情况中有 $\eta\le0$：

* $\eta<0$，核函数不满足 Mercer 定理，即不能作为核函数，此时函数为凹函数，极值取边界值
* $\eta=0$，单调，极值取边界值

为了使迭代尽快收敛，我们有倾向地选择迭代用的 $\alpha$ 值。我们知道，$\alpha$ 必须满足式 $(13)$，即 KKT 条件，所以 $\alpha_1$ 选择最违反 KKT 条件的样本对应的参数。为了使每次迭代变化最大，选择 $\alpha_2$ 为与 $\alpha_1$ 符号相反且绝对值最大的值。

于是，SMO 算法可以描述如下：

> SMO 算法
>
> * 目标：求得最优的 $\alpha$ 序列
> * 内循环：优化给定的 $\alpha_i,\alpha_j$
>   * 给定 $\alpha_i$，判断是否满足 KKT 条件，如满足，退出。
>   * 选择变化最大的 $\alpha_j$，根据公式得到 $\alpha_i^{new},\alpha_j^{new},b^{new}$
> * 外循环：选择 $\alpha_i$ 并传递给内循环
>   * 交替遍历所有样本和满足 $0<\alpha<C$ 的样本，选择不满足 KKT 条件的样本
>   * 如没有不满足 KKT 条件的样本，退出循环，优化结束