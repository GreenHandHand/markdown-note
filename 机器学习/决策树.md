---
aliases:
  - decision tree
tags:
  - 机器学习
  - 监督学习
---

# 决策树

决策树是一种基本的分类和回归方法，这里主要介绍用于分类的决策树。决策树模型呈现树形结构，在分类问题中，表示基于特征对实例进行分类的过程，它可以被认为是 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布，根据损失函数最小化的原则建立决策树模型。决策树的学习通常包括 3 个部分：特征选择，决策树的生成，决策树的修剪。

## 决策树模型与学习

分类决策树模型是一种描述对实例进行分类的树形结构，决策树由节点 (node) 和有向边 (directed edge) 组成，结点有两种类型：内部节点 (internal node) 和叶节点 (leaf node)。内部节点表示一个特征或者属性，叶节点表示一个类。

### 决策树与 if-then 规则

可以将决策树看作一个 if-then 规则的集合，将决策树转换为 if-then 规则的过程可以描述为：
1. 由决策树的根节点到叶节点的每一条路径构建一条规则
2. 路径上内部节点的特征对应着规则的条件，而叶节点的类对应着规则的结论

决策树路径及其对应的 if-then 规则集合有一个重要的性质，即互斥且完备。也就是说，每一个实例都被一条路径覆盖，而且只被一条路径覆盖。

### 决策树与条件概率分布

决策树还表示给定特征条件下类的条件概率分布。这一个条件概率分布定义在特征空间上的一个划分，将特征空间划分为了互补相交的单元 (cell) 或者域 (region), 并在每个单元定义了一个类的概率分布，构成了条件概率分布。

### 决策树的学习

假设给定训练数据集
$$
D=\{(x_1,y_1),\cdots,(x_N,y_N)\}
$$
其中 $x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(n)})^T$ 为输入实例，$n$ 为特征个数，$y_i\in\{1,\cdots,K\}$ 为类标记，$N$ 为样本容量。决策树学习的目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。

决策树学习的实质是从训练数据集中归纳出一组分类规则，与训练数据集不相矛盾的决策树可能有多个，也可能一个都没有。我们的目标是找到一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。决策树学习使用损失函数表示这一目标，一般决策树学习的损失函数使用正则化的极大似然函数，决策树学习的策略是以损失函数为目标的最小化。当损失函数确定后，学习问题就变为了在损失意义下选择最优的局册数问题，因为从所有的决策树中选择最优的决策树是 NP 完全问题，所以现实中的算法一般采用启发式算法，近似求解这一问题。

决策树的学习算法可以描述为：
1. 开始，构建根节点，将所有的训练数据都放在根节点
2. 选择一个最优特征，按照这一特征将训练数据集分割为子集，使得各个子集有一个当前条件下最好的分类
3. 如果这些子集已经被基本正确的分类，那么构建叶节点并将这些子集分到对应的叶节点上去
4. 如果还有子集不能被正确分类，那么对这些子集递归的分类

这样就生成了一个局部最优的决策树，通过剪枝来寻找全局最优的决策树。因此决策树学习问题就可以概括为特征选择、决策树的生成、决策树的剪枝。

## 特征选择

特征选择问题在于选取对训练数据具有分类能力的特征，这样可以极大的提高决策树的学习效率。如果利用一个特征分类的结果与随机分类的结果没有很大的差别，则称这个特征是没有分类能力的，经验上扔掉这样的特征对决策树的学习精度影响不大。通常用于特征选择的准则是信息增益或者信息增益比。

### 信息增益

信息增益 (information gain)可以直观的表示分类的准则：如果一个特征具有更好的分类能力，或者说，按照这一特征将训练集分割成子集，使得各个子集在当前条件下有最好的分类，那么就应该选择这一特征。

下面先给出 [[实践知识/熵、相对熵与交叉熵|熵与条件熵]] 的定义：在信息论与概率计算中，**熵** (entropy) 是表示随机变量不确定度的度量，设 $X$ 是取值有限的离散的随机变量，其概率分布为
$$
P(X=x_i)=p_i,\quad i=1,2,\cdots,n
$$
则随机变量 $X$ 的熵定义为
$$
H(X)=-\sum_{i=1}^np_i\log p_i
$$
在上式中，若 $p_i=0$，定义 $p_i\log p_i=0$，对数通常以 2 为底或者以 $e$ 为底，这是熵的单位分别称为比特 (bit) 或者纳特 (nat)。由定义可以知道，熵只依赖于 $X$ 的分布，与 $X$ 的取值无关，所以可以将 $X$ 的熵记为 $H(p)$，即
$$
H(p)=-\sum_{i=1}^np_i\log p_i
$$
熵越大，随机变量的不确定度越大。从定义可以证明
$$
0\leqslant H(p)\leqslant \log n
$$

设随机变量 $(X,Y)$，其联合概率分布为
$$
P(X=x_i,Y=y_j)=p_{ij},\quad i=1,2,\cdots,n;\quad j=1,2,\cdots,m
$$
**条件熵**(conditional entropy) $H(Y|X)$ 表示在随机变量 $X$ 已知的情况下随机变量 $Y$ 的不确定性，定义为 $X$ 作为给定条件下 $Y$ 的概率分布的熵对 $X$ 的数学期望
$$
H(Y|X)=\sum_{i=1}^np_i H(Y|X=x_i)
$$

当熵和条件熵中的概率由数据估计得到时，所对应的熵与条件熵分别称为经验熵 (empirical entropy) 和经验条件熵 (empirical conditional entropy)。**信息增益** (information gain) 表示得知特征 $X$ 的信息而使得类 $Y$ 的信息不确定度减少的程度，即：特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D,A)$ 定义为集合 $D$ 的经验熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的经验熵 $H(D|A)$ 之差，即
$$
g(D,A)=H(D)-H(D|A)
$$
一般的，熵 $H(Y)$ 和条件熵 $H(Y|X)$ 之差称为互信息 (mutual information)，决策树学习中的信息增益等价与训练集中类与特征的互信息。在决策树中，经验熵 $H(D)$ 表示对数据集 $D$ 进行分类的分类的不确定度，经验条件熵 $H(D|A)$ 表示在特征 $A$ 给定的条件下对数据集 $D$ 进行分类的不确定度，那么他们的差，即信息增益，表示由于特征 $A$ 导致对数据集 $D$ 分类的不确定度减少的程度。因此，信息增益大的特征具有更强的分类能力。

根据信息增益准则的特征选取方法为：对训练数据集（或子集）$D$，计算其每个特征的信息增益，并比较他们的大小，选择信息增益最大的特征。算法可以描述如下：记训练数据集为 $D$，$|D|$ 表示训练数据集的样本个数，设 $C_k$ 表示为第 $k$ 个类别，$|C_k|$ 表示属于第 $k$ 个类别的样本个数，有 $\sum_{k=1}^K|C_k|=|D|$。设特征 $A$ 有 $n$ 个不同的取值，将其划分为不同的子集，记 $D_1,D_2,\cdots,D_n$，有 $\sum_{i=1}^n|D_i|=|D|$，$D_{ik}$ 即 $D_i\cap C_k$，有
1. 计算数据集 $D$ 的经验熵 $H(D)$
   $$
H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}
$$
2. 计算特征 $A$ 对数据集 $D$ 的条件经验熵 $H(D|A)$
   $$
H(D|A)=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|}
$$
3. 计算信息增益
   $$
g(D,A)=H(D)-H(D|A)
$$

### 信息增益比

以信息增益作为划分训练数据集的特征，存在偏向选择取值较多的特征的问题，使用信息增益比 (information gain ratio) 可以对这一问题进行矫正，这是特征选择的另一个准则。

**信息增益比**定义为：特征 $A$ 对训练数据集 $D$ 的信息增益比 $g_R(D,A)$ 定义为其信息增益 $g(D,A)$ 与训练数据集 $D$ 关于特征 $A$ 的值的熵 $H_A(D)$ 之比，即
$$
g_R(D,A)=\frac{g_R(D,A)}{H_A(D)}
$$
其中 $H_A(D)=-\displaystyle\sum_{i=1}^n \frac{|D_i|}{|D|}\log_2 \frac{|D_i|}{|D|}$，$n$ 是特征 $A$ 取值的个数。

## 决策树的生成

经典的决策树生成算法有 ID3 算法和 C4.5 算法。

ID3 算法的核心是在决策树的各个节点上应用信息增益准则来选取特征，递归的构建决策树。具体的来说，
1. 若 $D$ 中所有的实例属于同一类别 $C_k$，则 $T$ 为单节点树，并将类 $C_k$ 作为该节点的类标记，返回 $T$
2. 若 $A=\varnothing$，则 $T$ 为单结点树，并将 $D$ 中的实例数最大的类 $C_k$ 作为该节点的标记，返回 $T$
3. 否则，根据信息增益选取特征 $A_g$
4. 如果 $A_g$ 的信息增益小于阈值 $\varepsilon$，则置 $T$ 为单节点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类标记，返回 $T$
5. 否则，对 $A_g$ 的每一可能值 $a_i$，依 $A_g=a_i$ 将 $D$ 分割为若干非空子集 $D_i$，将 $D_i$ 中实例数最大的类作为标记，构建子节点，由节点及其子节点构成树 $T$，返回 $T$
6. 对第 $i$ 个子节点，以 $D_i$ 为训练集，以 $A-\{A_g\}$ 为特征集，递归的调用前 5 步，得到子树 $T_i$，返回 $T_i$

C4.5 算法与 ID3 算法相似，但是使用信息增益比来选择特征。

## 决策树的剪枝

决策树生成算法递归的产生决策树，直到不能再继续下去为止，这样产生的分类树往往在训练集中非常准确，但是对未知的测试数据的分类却没有那么准确，即容易出现过拟合的现象。一个解决思路是减小决策树的复杂度，即**剪枝** (pruning) 操作。

这里介绍一种简单的剪枝操作，通过极小化决策树整体的损失函数来实现，设树 $T$ 的叶节点个数为 $|T|$，$t$ 是树 $T$ 的叶节点，该叶节点有 $N_t$ 个样本，其中 $k$ 类样本点有 $N_{tk}$ 个点，$H_t(T)$ 为叶节点 $t$ 上的经验熵，$\alpha\geqslant0$ 为参数，则决策树学习的损失函数可以定义为
$$
C_\alpha(T)=\sum_{t=1}^TN_tH_t(T)+\alpha|T|
$$
其中经验熵为
$$
H_t(T)=-\sum_k\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}
$$
在损失函数中，将损失函数中的第一项记为
$$
C(T)=\sum_{t=1}^{|T|}N_tH_t(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^KN_{tk}\log \frac{N_{tk}}{N_t}
$$
这时有
$$
C_\alpha(T)=C(T)+\alpha|T|
$$
其中 $\alpha(T)$ 表示模型对训练数据的预测误差，$|T|$ 表示模型复杂度，参数 $\alpha\geqslant0$ 控制两者之间的影响，上式通过结构风险来减小模型复杂度。决策树学习的损失函数最小化等价与正则化的极大似然估计。下面给出决策树剪枝算法：
1. 计算每个节点的经验熵
2. 递归地从树的叶节点向上回缩，设一组叶节点回缩到其父节点之前和之后的整体树分别为 $T_B$ 与 $T_A$，其对应的损失函数值分别为 $C_\alpha(T_B)$ 与 $C_\alpha(T_A)$，如果
$$
C_\alpha(T_A)\leqslant C_\alpha(T_B)
$$
则进行剪枝，即将父节点变成新的叶节点。
3. 返回 2，直到不能继续为止，得到损失函数最小的子树 $T$.

## CART 算法