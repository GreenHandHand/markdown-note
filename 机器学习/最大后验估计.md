---
aliases:
  - MAP
  - maximum a posteriori
tag:
  - 机器学习
  - 基本方法
---

# 最大后验估计

最大后验（Maximum A Posteriori，MAP）估计可以利用经验数据获得对未观测量的点态估计。它与极大似然估计（Maximum Likelihood，ML）方法相近，不同的是它扩充了优化的目标函数，其中融合了预估计量的先验分布信息，所以最大后验估计可以看作是正则化（regularized）的最大似然估计。

最大后验估计的基础是贝叶斯准则和极大似然估计，它们都是机器学习中重要的基础。

## 贝叶斯准则

贝叶斯准则是概率率的基本定理，这里简单介绍贝叶斯定义的主要内容。贝叶斯定义由英国数学家贝叶斯 (Thomas Bayes) 发展而来，用于描述两个条件概率之间的关系。假设存在一定概率发生事件 A 和 B，且它们之间存在关系：
- $P(A)$ 表示事件 A 发生的概率
- $P(B)$ 表示事件 B 发生的概率
- $P(A|B)$ 表示事件 B 已经发生时，事件 A 发生的概率
- $P(B|A)$ 表示事件 A 已经发生时，事件 B 发生的概率

贝叶斯定理描述了上述四个变量之间的关系：
$$
P(A|B) = \frac{P(A)P(B|A)}{P(B)}
$$

## 极大似然估计

极大似然估计是概率论中的基本估计方法，简单来说，就是给定数据，估计数据分布的参数。极大似然估计的想法非常直观，就是将参数作为变量，寻找一个使得给定数据出现概率最大的参数。极大似然估计使用似然函数表述参数取某值的概率:
$$
L(\theta) = P(X=x\mid \theta)
$$
极大似然估计的过程就是最优化似然函数的过程：
$$
\arg\max_\theta\quad L(\theta)
$$
由于对该函数取对数并不会影响其单调性，因此我们也常最优化对数似然函数：
$$
\arg\max_\theta \quad \ln L(\theta)
$$
其他更加具体的解释可以参考 [[EM 算法#极大似然估计]]。

## 最大后验概率估计

最大后验估计是在知道一系列结果的情况下，求参数可能性最大的那一个，它优化的式子与 MLE 有一定区别：
$$
\arg\max_\theta \quad P(\theta|X=x)
$$
根据贝叶斯定义，我们可以得到
$$
P(\theta\mid X=x)=\frac{P(\theta)P(X=x\mid \theta)}{P(X=x)}
$$
其中：
- $P(X=x\mid \theta)$ 就是极大似然估计中的似然函数
- $P(\theta)$ 表示参数 $\theta$ 服从的概率分布，该值是一个先验知识，我们要在一开始假设 $\theta$ 服从的概率分布，并通过数据对其进行调整，最后计算得到 $\theta$ 的后验分布 $P(\theta\mid X=x)$。

> [!note]
> 通过贝叶斯定理，我们可以将 MAP 的最优化表达式修改为：
> $$
\arg\max_\theta P(\theta)P(X=x\mid\theta)
> $$
> 由于分母中的 $P(X=x)=\int_\theta P(\theta)P(X=x|\theta)$，相当于对参数空间整体进行了积分，因此有 $P(\theta\mid X=x)\propto P(\theta)P(X=x\mid\theta)$。

> [!example]
> 以抛硬币为例，我们一般认为抛硬币时，结果为正的概率为 0.5，因此我们可以简单的假设 $\theta\sim N (0.5, 1)$。对于显示中更加复杂的例子，可能会假设成不同的分布。

最大后验估计的过程与极大似然估计进行，只是多了对 $\theta$ 分布的修正过程。这可以看作机器学习中的结构风险最小化，或者是正则化的极大似然估计。