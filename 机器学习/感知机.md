---
aliases:
  - perceptron
tags:
  - 机器学习
  - 深度学习 
  - 二分类
  - 线性
  - 监督学习
---

# 感知机

感知机 (perceptron) 是二分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取 +1 与 -1 二值。感知机对应将输入空间中实例划分为正负两类的线性超平面，属于判别模型。感知机 1957 年由 Rosenblatt 提出，是神经网络与[[机器学习/支持向量机|支持向量机]]的基础。

## 感知机模型

假设输入空间是 $\mathcal X\subseteq R^n$，输出空间是 $\mathcal Y=\{-1, 1\}$，输入 $x\in\mathcal X$ 表示实例的特征向量，对应于输入空间的点，输出 $y\in\mathcal Y$ 表示输出的类别，由输入到输出空间的如下函数：
$$
f(x)=\mathrm{sign}(w\cdot x+b)
$$
称为感知机，其中 $w$ 与 $b$ 为感知机参数，$w\in R^n$ 称为权值 (weight)或者权值向量 (weight vector)，$b\in R$ 称为偏置 (bias)。感知机是一种线性分类模型，感知机的假设空间定义在特征空间中的所有线性分类模型 (linear classification model) 或者线性分类器 (linear classifier)，即函数 $\{f|f(x)=w\cdot x+b\}$。

感知机与如下解释：线性方程 $wx+b=0$ 对应于特征空间中的一个超平面 $S$，其中 $w$ 是超平面的法向量，$b$ 是超平面的截距。位于平面两边的点被分为了正负两类，因此超平面被称为分离超平面 (separating hyperplane)。

## 感知机学习策略

对于给定数据集
$$
T=\{(x_1, y_1),\cdots,(x_N,y_N)\}
$$
其中 $x_i\in\mathcal X=R^n,y_i\in\mathcal Y=\{-1,1\}$，如果存在一个超平面 $wx+b=0$ 能够将数据集的正实例与负实例完全分开，即对所有的 $y_i=1$ 的实例有 $w\cdot x_i+b>0$，对于所有的 $y_i=-1$ 的实例有 $w\cdot x_i+b<0$，则称数据集 $T$ 是线性可分数据集 (linearly separable dataset)，否则称数据 $T$ 线性不可分。

假设数据集是线性可分的，感知机的目标是找到一个能将正实例与负实例完全正确分开的分离超平面，即定义损失函数并将损失函极小化。定义损失为误分类点到超平面 $S$ 的总距离，于是有：
$$
\frac{1}{\lVert w\rVert}\lvert w\cdot x_0+b\rvert
$$
这里的 $\lVert w\rVert$ 是 $w$ 的 $L_2$ 范数。对于误分类的数据 $(x_i,y_i)$ 来说有：
$$
-y_i(w\cdot x_i+b)>0
$$
于是误分类点到超平面的距离是：
$$
-\frac{1}{\lVert w\rVert}y_i(w\cdot x_i+b)
$$
假设误分类点集合为 $M$，那么所有误分类点到超平面 $S$ 的总距离为：
$$
-\frac{1}{\lVert w\rVert}\sum_{(x_i,y_i)\in M}y_i(w\cdot x_i+b)
$$
于是我们将损失函数定义为：
$$
L(w,b)=-\sum_{(x_i,y_i)\in M}y_i(w\cdot x_i+b)
$$
上式即感知机学习的经验风险函数，显然损失函数是非负的，如果没有误分类点，损失函数值为 0。

## 感知机学习算法

感知机学习算法是针对以下最优化问题的算法：
$$
\begin{aligned}
\min_{w,b}\quad&L(w,b)=-\sum_{(x_i,y_i)\in M}y_i(w\cdot x_i+b)
\end{aligned}
$$
其中 $M$ 为误分类点的集合，具体使用随机梯度下降法 (stochastic gradient descent)。即先随机取一个超平面 $w_0,b_0$，然后使用梯度下降不断极小化目标函数，极小化过程不是一次使用 $M$ 中所有误分类点，而是一次随机选取一个误分类点。

### 算法收敛性证明

下面证明上述算法可以收敛，即在经过有限次迭代可以得到一个将训练数据完全正确分类的分离超平面即感知机模型。为了便于叙述，将偏置 $b$ 并入权重向量中，同时将输入向量加以扩充，即 $\hat w=(w^T, b)^T,\hat x=(x^T, 1)^T$，有 
$$\hat w\cdot \hat x=w\cdot x+b$$

现在提出定理如下：
设训练集 $T$ 线性可分，则：
1. 存在满足条件的 $\lVert w_{opt}\rVert=1$ 的超平面 $\hat w_{opt} \cdot \hat x=w_{opt}\cdot x +b_{opt}=0$ 将训练集完全分类正确，且存在 $\gamma>0$ 使得：
   $$
y_i(\hat w_{opt}\cdot\hat x)=y_i(w_{opt}\cdot x +b_{opt})\geqslant \gamma
$$
2. 令 $R=\max_{1\leqslant i\leqslant N}\lVert x_i\rVert$，则感知机在训练集上的误分类次数 $k$ 满足不等式:
   $$
k\leqslant \left(\frac{R}{\gamma}\right)^2
$$
证明如下：
由于数据线性可分，所以存在一个超平面：$w\cdot x+b=0$ 将数据完全分开，根据线性解析式的性质，取 $\lVert \hat w\rVert=1$，那么对于有限的数据点，均有：
$$
y_i(w\cdot x_i+b)>0
$$
因此存在
$$
\gamma=\min_{i}\{y_i(w\cdot x_i+b)\}
$$
使
$$
y_i(w\cdot x_i+b)\geqslant \gamma
$$
感知机算法从向量 $\hat w_0=0$ 开始，如果实例被误分类，则更新权重。令 $\hat w_{k-1}$ 是第 $k$ 个误分类实例之前的扩充权重向量，则第 $k$ 个误分类实例的条件是：
$$
y_i(\hat w_{k-1}\cdot \hat x_i)\leqslant0
$$
若 $(x_i,y_i)$ 是被误分类的数据，那么 $\hat w_{k-1}$ 的更新方式为：
$$
\hat w_k=\hat w_{k-1}+\eta y_i\hat x_i
$$
于是可以计算：
$$
\begin{aligned}
\hat w_{opt}\cdot\hat w_{k}&=\hat w_{opt}\cdot\hat w_{k-1} + \eta y_i(\hat w_{k-1} \hat x_i)\\
&\geqslant \hat w_{opt}\cdot\hat w_{k-1} + \eta\gamma\\
&\geqslant \hat w_{opt}\cdot\hat w_{k-2} + \eta\gamma
\end{aligned}
$$
继续递推可以得到：
$$
\hat w_{opt}\cdot\hat w_{k}\geqslant k\eta\gamma
$$
其中 $k$ 是迭代过程中所有数据被误分类的次数。同样的，可以计算：
$$
\begin{aligned}
\lVert\hat w_k\rVert^2&=\lVert \hat w_{k-1} + \eta y_i\hat x_i\rVert^2\\
&=\lVert\hat w_{k-1}\rVert^2+2\eta y_i (\hat w_{k-1}\hat x_i)+\eta^2\Vert x_i\Vert^2\\
&\leqslant \lVert\hat w_{k-1}\rVert^2+\eta^2\Vert x_i\Vert^2\\
&\leqslant \Vert\hat w_{k-1}\Vert^2+\eta^2R^2\\
&\leqslant \Vert\hat w_{k-2}\Vert^2 + 2\eta^2R^2
\end{aligned}
$$
继续递推下去有：
$$
\Vert\hat w_k\Vert^2\leqslant k\eta^2R^2
$$
结合上面两个不等式可以得到：
$$
\begin{aligned}
k\eta\gamma\leqslant \hat w_{k}\hat w_{opt}\leqslant \Vert\hat w_k\Vert\Vert\hat w_{opt}\Vert\leqslant\sqrt k\eta R
\end{aligned}
$$
于是有：
$$
k\leqslant\left(\frac{R}{\gamma}\right)^2
$$
即误分类次数 $k$ 存在上界，可以通过有限次数搜索获得将训练数据集完全正确分开的分类超平面。也就是说，当训练数据线性可分时，感知机学习算法原始形式迭代是可以收敛的，且这个结果是不唯一的。当训练数据线性不可分时，感知机学习算法不收敛。

### 感知机学习算法的对偶形式

对偶形式的基本想法是将 $w$ 和 $b$ 看作为实例 $x_i$ 和标记 $y_i$ 的线性组合的形式，从而通过求解其系数而求得 $w$ 和 $b$。不失一般性的，将 $w_0$ 与 $b_0$ 的初始值均设为 0，则误分类点通过：
$$
\begin{aligned}
w&\leftarrow w+\eta y_ix_i\\
b&\leftarrow b+\eta y_i
\end{aligned}
$$
来更新 $w$ 和 $b$，因此 $w$ 与 $b$ 关于点 $(x_i,y_i)$ 的更新总量分别为 $n_i\eta x_iy_i$ 与 $n_i\eta y_i$。设 $\alpha_i=n_i\eta$，其中 $n_i$ 是误分类的次数，于是最后学习到的 $w$ 和 $b$ 可以表示为：
$$
\begin{aligned}
w&=\sum_{i=1}^N\alpha_iy_ix_i\\b&=\sum_{i=1}^N\alpha_iy_i
\end{aligned}
$$
于是我们的学习目标就变为了：
$$
f(x)=\mathrm{sign}(\sum_{i=1}^N\alpha_iy_ix_i\cdot x+\sum_{i=1}^N\alpha_iy_i)
$$
使用随机梯度下降的方法学习。使用对偶形式的好处是我们可以事先将内积计算出来，并以矩阵的形式储存，以节省计算的消耗，这个矩阵被称为 Gram 矩阵：
$$
\mathbf G=[x_i\cdot y_i]_{N\times N}
$$ 
## 结构化感知机

### 结构化预测

感知机是 [[线性模型]]，要将其应用与结构化预测任务中，需要进行一些处理。结构化预测任务是预测对象结构的一类监督任务，相应的模型训练过程称为结构化学习。分类任务的预测结果是一个决策边界，回归问题的预测结果是一个实数标量，而结构化预测任务的结果是一个完整的结构，可见结构化预测的难度更大。

结构化预测的过程就是给定一个模型 $\lambda$ 与一个打分函数 $\text{score}_\lambda(\cdot)$，利用打分函数为一些备选结构打分，选择分数最高的结构作为预测输出，即
$$
\hat y=\arg\max_{y\in Y}\text{score}_\lambda(x,y)
$$
其中 $Y$ 是备选结构的集合，备选结构可以是解空间的全集，也可以是一个子集。

### 结构化感知机

在感知机中，输入与输出遵循函数
$$
f(x)=\hat\omega \cdot \hat x
$$
结构化感知机需要同时将 $y$ 也考虑进去。具体的，定义一个新的特征函数 $\phi(x,y)$，将结构 $y$ 也作为一种特征，输出一个新的结构化特征向量 $\phi(x,y)\in \mathbb R^{D\times 1}$。将该特征向量作为感知机的输入，即
$$
\text{score}(x,y)=\hat\omega\cdot \phi(\hat x,y)
$$
于是结构化预测函数为
$$
\hat y=f(x) = \arg\max_{y\in Y}(\hat\omega \cdot \phi(\hat x,y))
$$
与感知机类似的，结构化感知的学习就是最优化问题
$$
\min_\hat\omega\quad \frac{1}{N}\sum_{i=1}^{N}\left(\hat\omega\cdot\phi(\hat x_i,y_i)-\hat\omega\cdot\phi(\hat x_i, \hat y_i)\right)^2
$$
其中 $N$ 是训练数据数量，$(\hat x, \hat y)$ 是目标结构，$(\hat x,y)$ 预测结构。于是结构化感知机算法可以描述为
1. 读入样本 $(x_i,y_i)$，计算 $f(x_i)=\arg\max_{y\in Y}(\omega\cdot \phi(\hat x_i,y))$。
2. 与监督样本对比，如果 $f(x_i)\ne y_i$，则使用下面的式子更新参数：
	- $\hat\omega\leftarrow\hat\omega + \alpha\left(\phi(\hat x_i,y_i)-\phi(x_i,f(x_i))\right)$

### 结构化感知机与序列标注

从结构化感知机的思路来看，特征函数对于结构化感知机的实现效果起到了决定性的作用。设计一个好的特征函数可以大幅度的提高结构化感知机的最终效果。这里以序列标注为例子，对特征函数再进行描述。

序列标注最大的特点就是序列中不同标签之间的依赖性。在 [[隐马尔可夫模型]] 模型中，这种依赖性被建模为了转移概率。在结构化感知机中，这种依赖性就需要通过特征的方式进行描述。对于序列中的连续标签，使用下面的**转移特征**进行描述：
$$
\small\phi_k(y_{t-1},y_t)=\begin{cases}1,&\text{$y_{t-1}=s_i$ 且 $y_t=s_j$}\\0,&\text{other.}\end{cases}\quad i=0,\cdots,N;j=1,\cdots,N
$$
其中 $y_{t}$ 是序列中第 $t$ 个标签，$s_i$ 为标注集中第 $i$ 种标签，$N$ 为标注集大小。定义 $s_0=\text{<bos>}$，表示序列第一个标签之前的标签，$k=i\times N+j$ 表示转移特征的编号，共有 $(N+1)\times N$ 种转移特征。

此外，对于一些特殊的状态，例如文本分词中的前一个字符是否是数字、当前字符是否等于前一个字符等，需要根据当前的任务进行特殊的设计。这样的特征下面的式子进行描述。
$$
\phi_l(x_t,y_t)=\begin{cases}1&\text{some case}\\0&\text{other case}\end{cases}
$$

最后，对于序列标注任务，我们设计的结构化感知机的特征函数就是转移特征与状态特征的集合
$$
\phi=[\phi_k,\phi_l],\quad k=1,\cdots,(N+1)N;\,l=1,\cdots,(N+1)N
$$
统一的，使用 $\phi(y_{t-1},y_t,x_t)$ 表示特征函数。最后，一个序列的分数就是结构化感知机的输出，即
$$
\text{score}(x,y)=\sum_{t=1}^T\omega\cdot \phi(y_{t-1},y_t,x_t)
$$

最后，通过训练可以得到一个结构化感知机模型，使用该模型进行预测，即搜索状态空间中，使得分数达到最大的一组序列。我们一般使用 [[隐马尔可夫模型#维特比算法|维特比算法]] 求解。