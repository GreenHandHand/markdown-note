---
aliases:
  - naive Bayes
tags:
  - 机器学习
  - 监督学习
---

# 朴素贝叶斯

朴素贝叶斯 (naive Bayes) 方法是基于贝叶斯估计与特征条件独立假设的分类方法。对于给定数据集，首先基于特征条件独立假设学习输入输出的[[概率论与数理统计/多维随机变量及其分布|联合概率分布]]，然后基于此模型，对给定的输入 $x$ 利用贝叶斯定理求出后验概率最大的输出 $y$。朴素贝叶斯方法实现简单，学习与预测的效率都很高，是一种常用的方法。

## 朴素贝叶斯法的学习和分类

设输入空间 $\mathcal X\subseteq R^n$ 为 $n$ 维向量的集合，输出空间为类标记集合 $\mathcal Y=\{c_1,c_2,\cdots,c_K\}$，输入为特征向量 $x\in\mathcal X$，输出为类标记 (class label) $y\in\mathcal Y$。$X$ 是定义在输入空间 $\mathcal X$ 上的[[概率论与数理统计/一维随机变量及其分布|随机变量]]，$Y$ 是定义在输出空间 $\mathcal Y$ 上的随机变量，$P(X,Y)$ 是 $X$ 与 $Y$ 的联合概率分布，训练数据集
$$
T=\{(x_1,y_1),\cdots,(x_N,y_N)\}
$$
由 $P(X,Y)$ 独立同分布产生。朴素贝叶斯法通过训练数据集学习联合概率分布 $P(X,Y)$，具体的，学习以下先验概率分布即条件概率分布，先验概率分布为
$$
P(Y=c_k),\quad k=1,2,\cdots,N
$$
条件概率分布为
$$
P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)}|Y=c_k)
$$
于是学到了联合概率分布 $P(X,Y)$。条件概率分布 $P(X=x|Y=c_k)$ 有指数量级的参数，其估计实际是不可行的，朴素贝叶斯法对条件概率分布作了条件独立性假设，由于这是一个较强的假设，朴素贝叶斯法也由此得名。具体的，条件独立性假设是
$$
\begin{aligned}
P(X=x|Y=c_k)&=P(X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)}|Y=c_k)\\&=\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)
\end{aligned}
$$
朴素贝叶斯方法实际上学到的是生成数据的机制，属于生成模型。条件独立假设等于是说用于分类的特征在类确定的情况下都是条件独立的，这一假设使得朴素贝叶斯方法变得简单，但是会牺牲一定的分类准确性。

朴素贝叶斯法进行分类时，对于给定的输入 $x$，通过学习到的模型计算后验概率分布 $P(Y=c_k|X=x)$，将后验概率最大的类作为 $x$ 的类输出，后验概率计算根据贝叶斯定理进行
$$
P(Y=c_k|X=x)=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_kP(X=x|Y=c_k)P(Y=c_k)}
$$
将朴素贝叶斯带入上式可以得到
$$
P(Y=c_k|X=x)=\frac{P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_kP(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}
$$
这就是朴素贝叶斯的基本公式，朴素贝叶斯分类器可以表示为
$$
y=\arg\max_{c_k}\frac{P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_kP(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}
$$
将与 $c_k$ 无关的常数项消去后得到
$$
y=\arg\max_{c_k}P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)
$$

### 后验概率极大化的含义

朴素贝叶斯法将实例分到后验概率最大的类中，这等价于期望风险最小化，假设选择 $0-1$ 损失函数，则
$$
L(Y,f(X))=\begin{cases}1,&Y\ne f(X)\\0,&Y=f(X)\end{cases}
$$
式中 $f(X)$ 为分类决策函数，这是期望风险函数为
$$
R_{\exp}(f)=E[L(Y,f(X))]
$$
期望是对联合分布取的，由此取条件期望得到
$$
R_{\exp}(f)=E_X\sum_{k=1}^K[L(c_k,f(X))]P(c_k|X)
$$
为了使期望风险最小化，只需要对 $X=x$ 逐个极小化，由此得到
$$
\begin{aligned}
f(x)&=\arg\min_{y\in\mathcal Y}\sum_{k=1}^KL(c_k,y)P(c_k|X=x)\\
&=\arg\min_{y\in\mathcal Y}\sum_{k=1}^KP(y\ne c_k|X=x)\\
&=\arg\min_{y\in\mathcal Y}(1-P(y=c_k|X=x))\\
&=\arg\min_{y\in\mathcal Y}P(y=c_k|X=x)
\end{aligned}
$$
于是根据期望风险最小化准则得到了后验概率最大化准则
$$
f(x)=\arg\max_{c_k}P(c_k|X=x)
$$
即朴素贝叶斯采用的原理。

## 朴素贝叶斯的参数估计

在朴素贝叶斯方法中，学习意味着估计 $P(Y=c_k)$ 和 $P(X^{(j)}=x^{(j)}|Y=c_k)$，可以应用极大似然估计获得相应的概率，先验概率 $P(Y=c_k)$ 的极大似然估计是
$$
P(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)}{N}
$$
设第 $j$ 个特征 $x^{(j)}$ 可能的取值的集合为 $\{a_{j1},a_{j2},\cdots,a_{jS_j}\}$，条件概率 $P(X^{(j)}=a_{jl}|Y=c_k)$ 的极大似然估计是
$$
P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}
$$
式中，$x_i^{(j)}$ 是第 $i$ 个样本的第 $j$ 个特征，$a_{jl}$ 是第 $j$ 个特征可能取的第 $l$ 个值，$I$ 是指示函数。

## 学习分类算法

下面给出朴素贝叶斯法的学习与分类算法。对于输入的训练数据集，有
1. 计算先验概率及条件概率
   $$
   \begin{aligned}
P(Y=c_k)&=\frac{\sum_{i=1}^NI(y_i=c_k)}{N}\\
P(X^{(j)}=a_{jl}|Y=c_k)&=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}
\end{aligned}
$$
2. 对于给定实例 $x$，计算
$$
P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)
$$
3. 确定类别
$$
y=\arg\max_{c_k}P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)
$$
## 贝叶斯估计

用极大似然估计可能会出现所要估计的概率值为 0 的情况，这是会影响到后面的概率计算结构，使分类产生偏差，解决这一问题的方法是采用贝叶斯估计，具体的条件概率的贝叶斯估计是
$$
P_\lambda(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum_{i=1}^N I(y_i=c_k)+S_j\lambda}
$$
式中 $\lambda\leqslant0$。等价于在随机变量各个取值的频数上赋予一个正数 $\lambda>0$，当 $\lambda=0$ 时是极大似然估计，常取 $\lambda=1$，这是称为拉普拉斯平滑 (Laplacian smoothing)，显然对任何 $l=1,2,\cdots,S_j,k=1,2,\cdots,K$ 有
$$
\begin{aligned}
P_\lambda(X^{(j)}=a_{jl}|Y=c_k)>0\\
\sum_{l=1}^{S_j}P_\lambda(X^{(j)}=a_{jl}|Y=c_k)=1
\end{aligned}
$$
表明上式的确是一种概率分布，同样，先验概率分布的贝叶斯估计是
$$
P_\lambda(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)+\lambda}{N+K\lambda}
$$
