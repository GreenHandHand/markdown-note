---
tags:
  - 强化学习
---
# 动态规划算法

由于 [[马尔科夫决策过程#贝尔曼期望方程|贝尔曼期望方程]] 的特性，我们可以使用动态规划算法对状态价值函数进行求解。

## 策略迭代算法

策略迭代是策略评估和策略提升不断循环交替，直至最后得到最优策略的过程。

### 策略评估

策略评估这一过程用来计算一个策略的状态价值函数。根据贝尔曼期望方程中的价值状态函数：
$$
V^{\pi}(s)=\sum_{a\in \mathcal{A}}\pi(a|s)\left(r(s,a)+\gamma\sum_{s'\in\mathcal{S}}P(s'|s,a)V^{\pi}(s')\right)
$$
将其改写为迭代方程式：
$$
V^{k+1}(s)=\sum_{a\in \mathcal{A}}\pi(a|s)\left(r(s,a)+\gamma\sum_{s'\in\mathcal{S}}P(s'|s,a)V^{k}(s')\right)
$$
可以证明，当 $k\to \infty$ 时，$V^{k}\to V^{\pi}$。因此上式为一个动态规划式，如果在一轮循环中，有 $\max_{s\in\mathcal{S}}|V^{k+1}(s)-V^{k}(s)|$ 的值小于一个很小的常数时，则可以认为收敛。这样做可以提高效率，并且得到的值非常接近真实值。

### 策略提升

使用策略评估可以计算一个策略的状态价值函数。在计算得到状态价值函数后，我们可以根据此对策略进行改进。假设智能体在状态 $s$ 进行动作 $a$ ，其余时候仍然遵循策略 $\pi$。如果有 $Q^{\pi}{(s,a)}>V^{\pi}(s)$，则说明在状态 $s$ 下采取动作 $a$ 可以得到更高的期望回报，于是可以修改我们的策略。

现在假设存在一个确定性策略 $\pi'$，在任意一个状态 $s$ 下都满足：
$$
Q^{\pi}(s,\pi'(s))\geqslant V^{\pi}(s)
$$
于是在任意状态下有：
$$
V^{\pi'}(s)\geqslant V^{\pi}(s)
$$
上式即**策略提升定理**。我们可以贪心的在每一个状态选择动作价值最大的动作，也就是：
$$
\pi'(s)=\arg\max_{a}Q^\pi(s,a)=\arg\max_a\{r(s,a)+\gamma\sum_{s'\in \mathcal S}P(s'|s,a)V^\pi(s')\}
$$
当策略 $\pi'$ 与之前的策略 $\pi$ 相同时，说明策略迭代达到了收敛，此时 $\pi'$ 和 $\pi$ 就是最优策略。

### 策略迭代算法

总的来说，策略迭代算法分为下面几个步骤：
1. 评估当前策略，得到当前策略的状态价值函数
2. 策略提升，得到一个更好的策略
3. 重复上面的两个步骤直到策略收敛

伪代码如下：
```python
# randomly set the pi(s) and V(s)
pi, V = randomSet()
pi_old = pi
while pi_old != pi:
	while delta > theat:
		delta = 0
		for s in S:
			v = V[s]
			V[s] = r(s,pi[s]) + gamma * sum([P(ss|s,pi[s])*V[ss] for ss in S])
			delta = max(delta, abs(v-V[s]))
	pi_old = pi
	# update pi
	for s in S:
		pi[s] = argmax(r(s,pi(s)) + gamma * sum([P(ss|s,pi[s]) * V[ss] for ss in S]))
return pi
```

## 价值迭代算法

从策略迭代算法的步骤中我们可以发现在每次迭代中都需要重新计算一次状态价值函数，这需要耗费大量的时间。价值迭代算法的思想就是每一次策略评估中只进行一轮，然后使用这个结果更新策略。可以使用这种方法的原因可以可以通过 [[马尔科夫决策过程#贝尔曼最优方程|贝尔曼最优方程]] 得到：
$$
V^{k+1}(s) = \max_{a\in\mathcal A}\{r(s,a) + \gamma\sum_{s'\in\mathcal S}P(s'|s,a)V^{k}(s')\}
$$
价值迭代算法可以看作根据上式进行的动态规划过程，当 $V^{k+1}(s)$ 与 $V^{k}(s)$ 相同时，就对应这最优状态价值函数 $V^*(s)$，我们可以利用 $\pi(s)=\arg\max_a\{r(s,a)+\gamma\sum_{s'}P(s'|s,a)V^{*}(s')\}$ 恢复出最优策略。

价值迭代算法的伪代码如下：
```python
# randomly set the V(s)
V = randomSet()
while delta > theta:
	delta = 0
	for s in S:
		v = V[s]
		V[s] = max([r(s,a) + gamma * sum([P(ss|s, a)*V[ss] for ss in S]) for a in pi(s)])
		delta = max(delta, abs(v-V[s]))
for s in S:
	pi[s] = argmax(r(s,pi[s]) + gamma * sum([P(ss|s,pi[s]) * V(ss) for ss in S]))
return pi
```