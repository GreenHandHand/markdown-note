---
tags:
  - 并行编程
  - 概述
---

# 并行编程概述

## 并行编程

- 问题能否（适不适合）被并行化？
- 问题如何进行划分？任务划分、数据划分
- 并行处理单元之间数据如何通信？
- 数据之间的依赖如何？
- 任务之间需不需要同步？
- 负载是否均衡？

## 并行程序的性能评估

下面介绍一些评估并行程序的性能的指标。

### 加速比

加速比 (Speedup) 的计算方式为
$$
S_p=\frac{T_1}{T_p}
$$
其中 $p$ 指 CUP 数量，$T_1$ 为算法串行执行的时间，$T_p$ 指当有 $p$ 个处理器时执行算法的时间。

### 并行效率

并行效率 (parallel efficiency) 的计算方式为
$$
E_p=\frac{S_p}{p}=\frac{T_1}{pT_p}
$$
其中 $p$ 指 CUP 数量，$S_p$ 指加速比。并行效率 $E_p$ 的大小一般介于 0 到 1 之间，用于表示在解决问题时，相较于通信与同步上的开销，参与计算的处理器得到了多大程度的充分利用。根据这个定义，一个充分运行的并行程序的并行效率为 1。

### 可拓展性

可拓展性 (Scalability) 指一个并行程序对于一个数据规模的可拓展性。如果一个技术（算法/程序）可以处理规模不断增长的问题，那么它就是可以拓展的。

对于并行程序，如果输入规模增大，同时增加进程/线程个数 (PE 个数)，能够使得并行效率保持不变，那么该程序就有很好的可拓展性。
- 强可拓展性：问题规模固定，增加进程/线程个数时，效率不变。
- 弱可拓展性：增加线程/进程个数时，只有以相同倍率增大问题规模才能使得效率值不变。

### Amdahl 定律

Amdahl 定律给出了加速比与程序部分执行时间提升 $k$ 倍的关系。设系统执行某应用程序需要的时间为 $T_{old}$，该应用某部分执行时间与总时间比例为 $\alpha$，若将该部分性能提升 $k$ 倍，总的执行时间为
$$
\begin{aligned}
T_{new}&=\frac{\alpha T_{old}}{k}+(T_{old}-\alpha T_{old})\\&=T_{old}[(1-\alpha)+\alpha/k]
\end{aligned}
$$
于是加速比 $S=\frac{T_{old}}{T_{new}}$ 为
$$
S=\frac{1}{(1-\alpha)+\alpha/k}
$$

Amdahl 定律说明了对系统某部分加速时，其对系统整体的影响取决于该部分重要性和加速程度。想要显著加速整个系统，必须提升全系统中相当大的部分的速度。

### Gustafson 定律

Gustafson 定律内容如下：定义 $a$ 为系统串行执行时间，$b$ 为系统并行执行时间，$n$ 为处理器个数，那么系统执行时间 (串行时间+并行时间) 可以表示为 $a+b$，系统总执行时间（串行时间）可以表示为 $a+nb$，串行比例 $F=a/(a+b)$，于是加速比为
$$
\begin{aligned}
S_n&=\frac{a+nb}{a+b}=\frac{a}{a+b}+\frac{nb}{a+b}\\[3mm]
&=F+n(1-F)=n-F(n-1)
\end{aligned}
$$

该定律说明了处理器数量、串行比例和加速比之间的关系。只要有足够的并行化任务，那么加速比和处理器个数成正比。

### 程序计时

进程的三种状态为阻塞、就绪、运行。其中
- 时钟时间=阻塞时间+就绪时间+运行时间
- 用户 CPU 时间=运行状态下用户空间的时间
- 系统 CPU 时间=运行状态下系统空间的时间
- 用户 CPU 时间+系统 CPU 时间=运行时间

一般而言，使用时钟时间来衡量性能，使用 API 提供的基石接口可以获取时间，但是需要注意时间的单位。

## 并行编程方式

目前常用的并行编程框架或者方法主要包括：
- [[pThread多线程编程|pThread 多线程库]]
- OpenMP 多线程框架
- MPI 多进程编程
- CUDA 编程