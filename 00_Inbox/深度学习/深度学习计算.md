---
tags:
  - 深度学习
---
# 深度学习计算

本节介绍深度学习框架的关键组件，包括模型的构建、参数访问与模型初始化、设计自定义层和块、将模型读写到磁盘上以及利用 GPU 加速计算。与 [[00_Inbox/深度学习/预备知识]] 不同，虽然这些知识与深度学习中的理论关联不大，但在设计神经网络时却是必备的知识。

## 层和块

在研究较为复杂的模型时，神经网络设计人员就不在考虑单个神经元的行为，而是考虑更加粗粒度的块 (block)，从编程的角度来看，块由类 (class)表示，类的任何子类都必须定义一个将输入转换为输出的前向传播函数，并且必须储存任意的参数。为了计算梯度，块必须拥有反向传播函数，但是在深度学习框架中，我们使用自动微分可以得到参数的梯度，因此在设计块时，我们只需要考虑前向传播函数与必须的参数。

### 自定义块

任何自定义块都继承自 `nn.Module` 模块，这个自定义块必须提供以下功能：
1. 将输入数据作为其前项传播函数的参数。
2. 通过前向传播函数来生成输出。
3. 计算其输出关于输入的梯度。这通常是自动完成的。
4. 存储和访问前向传播计算所需的参数。
5. 根据需要初始化模型参数。

下面作为一个模板：
```python
class Block(nn.Module):
	def __init__(self, ...):
		# 初始化块，可以有参数
		super().__init__()

		# 定义前向传播中必要的参数，例如
		self.fc = nn.Linear(...)
		...

	def forward(self, x):
		# 定义前向传播函数，返回神经网络的输出
		x = F.relu(self.fc(x))
		...
		return x
```

上面就是一个块的构成，我们可以将其作为其他块的组件，或者直接作为一个神经网络。在前向传播函数中，我们并不一定要使用块对参数进行处理，实际上，在前向传播函数中可以使用任意 python 控制流，但是必须将涉及到的所有需要更新梯度的参数保存在模型中。

### 顺序块

现在再来回顾 `nn.Sequential` 类，该类定义了一个顺序块，可以将输入按顺序进行处理，可以作为一种自定义类的简单实现，可以在某种程度上简化代码。

## 参数管理

在选择了架构并设置了超参数后，就进入了训练阶段，我们希望找到使损失函数最小的模型参数值。在完成训练后，我们可能希望获得模型的参数，在其他环境中复用它们或者将模型保存下来。接下来介绍：
1. 访问参数的方法
2. 参数初始化
3. 在不同模型组件间共享参数

### 参数访问

当使用 `nn.Sequential` 来定义一个块时，可以通过索引来访问模型的任意层。对于自定义块，可以通过属性来访问模型的任意块。
- 对于模型的每一层，使用 `state_dict()` 方法可以获得该层的参数字典。
- 对于每一个块或层，使用 `parameters()` 方法可以获得该块的所有参数的迭代器，该迭代器每个元素都是 `(name, parameters)` 元组。
- 每一个块都有相应的参数属性，例如 `net.weight` 将会得到一个 `parameter` 类，使用 `data` 属性可以获得该类的值。
- 对于嵌套块，我们只需要向嵌套列表索引那样访问即可。

### 参数初始化

在 pytorch 的 `nn.init` 模块中内置了多种初始化方法, 使用这些方法可以方便的完成每一层的初始化工作。当需要使用自定义的初始化方法时，我们可以通过直接设置参数来进行初始化。

### 参数绑定

有时我们希望定义一个稠密层，使用该层的参数来设置另一层的参数，我们可以通过在不同的层使用同一个块来实现，具体的：
```python
shared = nn.Linear(8, 8)
net = nn.Sequential(
	nn.Linear(4, 8), nn.ReLU(),
	shared, nn.ReLU(),
	shared, nn.ReLU(),
	nn.Linear(8, 1)
)
```
这样 net 网络的第 2 层与第 4 层都是 shared 对象，因此他们共享参数，在进行梯度反向传播时，shared 的梯度将是这两层梯度的和。

## 读写文件

在训练完成后，如果下次还需要使用，或者在其余环境下使用，我们需要将模型保存下来。在 pytorch 中可以使用 `torch.save` 方法与 `torch.load` 方法来实现。

`torch.save` 方法可以将一个张量保存在指定的位置，之后我们可以使用 `torch.load` 方法来读取它。另外，我们可以使用 `torch.save` 方法储存一个张量列表或者字典，并使用 `torch.load` 方法读取这个列表或者字典。

如果我们想要保存整个模型，单独保存每个向量是十分麻烦的，其次，模型本身很难序列化，因此容易出现错误。我们通常使用 `torch.save` 方法保存模型的参数，然后使用 `torch.load` 来读取模型的参数字典，pytorch 的模型中内置了方法 `load_state_dict` 方法来使用参数初始化模型。

## GPU

在 pytorch 中每个张量都有一个设备属性，默认情况下，该属性为'cpu'，如果我们想要使用 GPU 的高性能来计算张量，就需要将其复制到 GPU 上。在 pytorch 中，可以使用 `torch.device(f'cuda:{i}')` 与 `torch.device('cpu')` 来表示设备，pytorch 提供了多种方法来使用不同的设备。
- 使用张量的 `to(device)` 方法可以将其复制到对应的设备中。
- 在创建张量时传入参数 device 来指定其所在设备。
- 使用张量的 `cuda()` 方法将其复制到 GPU 中，该方法传入一个参数表示 GPU 设备序号（从 0 开始）
- 使用张量的 `cpu()` 方法将其复制到 CPU 中。

需要注意的是，如果我们将其复制到原本的设备中，那么将返回它本身，而不会创建一个新的变量。另外，在不同的设备上的张量不能进行计算，因为系统找不到对应的张量，且不知道应当储存在哪。

对于神经网络，我们也可以使用相同的方法来将其复制到 GPU 上进行计算。