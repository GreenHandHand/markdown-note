---
tags:
  - 深度学习
---

# 神经网络的思想

受神经元的工作方式的启发，我们使用计算机模拟神经元，得到了神经网络。

从生物学中，我们可以得到以下构成大脑的神经元的知识：
- 神经元形成网络
- 对于从其他多个神经元中传递过来的信号，如果他们的和不超过一个固定大小的值（阈值），则神经元不做出反应
- 对于从其他多个神经元传递过来的信号，如果他们的和超过一个固定大小的值（阈值），则神经元做出反应（称为点火），向另外的神经元传递固定强度的信号
- 从多个神经元传递过来的信号之和中，每个信号对应的权重不一样

## 神经元工作的数学表示

我们使用变量 x 表示输入信号，如下所示：
$$
\begin{cases}无输入信号：x=0\\有输入信号：x=1\end{cases}
$$
使用变量 y 输出信号，如下所示：
$$
\begin{cases}无输出信号：y=0\\有输出信号：y=1\end{cases}
$$
最后，我们使用数学方式来表示点火的判定条件：

神经元的输入信号之和可以表示为：
$$
w_1x_1 + w_2x_2 + w_3x_3
$$
式中的 $w_1$、$w_2$、$w_3$ 是输入信号 $x_1$、$x_2$、$x_3$ 的权重（**weight）。**

根据神经元超过阈值时点火可以将点火条件表示如下：
$$
\left.\begin{matrix}有输出信号(y=0)：& w_1x_1 + w_2x_2 + w_3x_3 < \theta \\无输出信号(y=1)：& w_1x_1 + w_2x_2 + w_3x_3 \geqslant\theta\end{matrix}\right\}
$$
这里是神经元固有的阈值。

> [!note] 利用阶跃函数
> $$
> u(z)=\begin{cases}0 & (z<0)\\1&(z \geqslant 0)\end{cases}
> $$
> 可以将点火条件用一个式中表述：
> $$
> y = u(w_1x_1 + w_2x_2 + w_3x_3 - \theta)
> $$
> 此外，阶跃函数中的 $z$ 称为该神经元的**加权输入**。

## 激活函数：将神经元的工作一般化

> [!note] 在实际使用中，我们通常需要绘制大量的神经元，所以在实际中我们通常使用下面的简易图表：
<<<<<<< Updated upstream:00_Inbox/深度学习/神经网络的思想.md
> ![[Assets/神经网络的思想-1.png]]
> 为了与生物学的神经元区分开来，我们把经过这样简化、抽象化的神经元称为神经单元(unit)。
=======
> ![[image/神经网络的思想-1.png]]
>
> 为了与生物学的神经元区分开来，我们把经过这样简化、抽象化的神经元称为神经单元 (unit)。
>>>>>>> Stashed changes:深度学习/神经网络的思想.md

### 激活函数

将点火条件中的阶跃函数一般化，我们可以得到一般的点火函数：
$$
y = a(w_1x_1 + w_2x_2 + w_3x_3 - \theta)
$$
这里的函数 $a$ 是建模者定义的函数，称为**激活函数（activation function）**。这个点火函数就是神经网络的出发点。虽然上式只考虑了 3 个输出，但是上面的式子可以很容易的推广到任意多个输入。

### Sigmoid 函数

Sigmoid 函数是激活函数的一个代表例子，定义如下：
$$
\sigma(z) = \dfrac{1}{1+e^{-z}}
$$
该函数的图形如下：
<<<<<<< Updated upstream:00_Inbox/深度学习/神经网络的思想.md
![[Assets/神经网络的思想-2.png]]
=======

![[image/神经网络的思想-2.png]]
>>>>>>> Stashed changes:深度学习/神经网络的思想.md

### 偏置

式 $y = a(w_1x_1 + w_2x_2 + w_3x_3 - \theta)$ 中 $\theta$ 被称为阈值，数学上为了美观将上式改为了：
$$
y = a(w_1x_1 + w_2x_2 + w_3x_3 + b)
$$
这样一来式子变得更加美观，计算也不容易出错。这个 $b$ 称为**偏置 (bias)。**

## 认识神经网络

这里介绍的神经网络分为基础的阶层型神经网络以及由其发展来的卷积型神经网络。

### 神经网络各层的职责

阶层型神经网络如下如所示，按照层划分神经网络，通过这些神经单元处理信号，并从输出层得到结果。
<<<<<<< Updated upstream:00_Inbox/深度学习/神经网络的思想.md
![[Assets/神经网络的思想-3.png]]
* 输入层负责读取给予神经网络的信息。属于这个层的神经单元没有输入箭头，他们是简单的神经单元，只是将数据得到的值原样输出。
* 隐藏层的神经单元执行前面所复习过的处理操作，在神经网络中，这是实际处理信息的部分。
* 输出层与隐藏层一样执行信息的处理操作，并显示神经网络计算出的结果，也就是整个神经网络的输出。

### 深度学习

深度学习是叠加了很多层的神经网络。叠加层有各种各样的方法，其中著名的是[[00_Inbox/深度学习/卷积神经网络|卷积神经网络]]。
=======
![[image/神经网络的思想-3.png]]
- 输入层负责读取给予神经网络的信息。属于这个层的神经单元没有输入箭头，他们是简单的神经单元，只是将数据得到的值原样输出。
- 隐藏层的神经单元执行前面所复习过的处理操作，在神经网络中，这是实际处理信息的部分。
- 输出层与隐藏层一样执行信息的处理操作，并显示神经网络计算出的结果，也就是整个神经网络的输出。

### 深度学习

深度学习是叠加了很多层的神经网络。叠加层有各种各样的方法，其中著名的是 [[深度学习/卷积神经网络|卷积神经网络]]。
>>>>>>> Stashed changes:深度学习/神经网络的思想.md

## 网络自学习的神经网络

网络自学习是神经网络中确定权重大小的重要方法。

参数确定方法分为有监督学习与无监督学习，这里只介绍有监督学习。有监督学习只为例确定神经网络的权重和偏置，事先给予数据，这些数据被称为学习数据。根据给定的学习数据确定权重和偏置的过程，称为学习。

神经网络学习的方法通常是通过计算神经网络预测值与正解的误差，确定使得误差综合达到最小的权重和偏置。这在数学上被称为模型的最优化。

关于预测值与正解的误差总和，有各种各样的定义，这里使用误差的平方（称为平方误差），然后再相加。这个误差总和称为代价函数，用符号 $C_T$ 表示。这个方法在数学中称为最小二乘法。

# 神经网络的数学基础

以下数学概念在神经网络中较为常用：
- 一次函数
- 二次函数
- 单位阶跃函数
- 指数函数与 Sigmoid 函数
- 正态分布的概率密度函数
- 数列与递推关系式
- 联立递推关系式
- 求和
- 有向线段与向量
- 柯西不等式
- 矩阵计算
- 微积分基础
- 求导的链式法则
- 单变量、多变量函数的近似公式及其向量表示
- 梯度

## 梯度下降法

在数学中我们将一个函数导数变化最快的方向叫做函数在该点的梯度。通过梯度我们可以利用计算机寻找最小值。
$$
(\Delta x_1, \Delta x_2, \cdots, \Delta x_n) = - \eta \left\{\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n}\right\}(\eta为微小常数)
$$
在实际神经网络中，主要处理由成千上万个变量构成函数的最小值，在这种情况下，上式往往显得过于冗长，因此我们使用哈密顿算子 $\nabla$：
$$
\nabla = \left\{\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n}\right\}
$$
利用该符号可以将式子简化为：
$$
(\Delta x_1, \Delta x_2, \cdots, \Delta x_n) = -\eta\nabla f
$$
更简洁地，上式表述为：
$$
\Delta x = -\eta \nabla f
$$
在数学中，$\eta$ 被看作每次点移动的步长，在神经网络的世界中，$\eta$ 被称为学习率，它的确定方法没有明确的标准，只能通过反复试验来寻找恰当的值。

## 最优化问题和回归分析

最优化问题就是如何确定模型的参数的问题。在数学上来说，确定的神经网络的参数使一个最优化问题，具体的就是对神经网络的参数进行拟合，使得神经网络的输出与实际数据相吻合。

# 神经网络的最优化

## 神经网络的参数和变量

由于神经网络中用到的参数和变量的数量庞大，因此参数和变量的表示需要统一标准。

首先，我们对层进行编号，将输入层记为层 1，隐藏层为层 2，层 3……最右边的输出层为层 l（这里的 l 为 last 的首字母，表示层的总数）。使用下表来表示变量与参数：

| 符号         | 含义                                                                                                                                                                 |
| ------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| $x_i$      | 表示输入层（层 1）的第 i 个神经单元的输入的变量，由于输入层的神经单元的输入和输出为同一值，<br />所以也是表示输出的变量。此外，这个变量名也可以作为神经单元的名称使用。 |
| $w_{ij}^l$ | 从层 l-1 的第 i 个神经单元指向层 l 的第 j 个神经单元的箭头和权重。请注意 i 和 j 的顺序，这是神经网络的参数                                                                       |
| $z_j^l$    | 表示层 l 的第 j 个神经元的加权输入的变量                                                                                                                                 |
| $b_j^l$    | 层 l 的第 j 个神经单元的偏置。这是神经网络的参数                                                                                                                         |
| $a_j^l$    | 层 l 的第 j 个神经单元的输出变量。此外，这个变量名也作为神经单元的名称使用。                                                                                             |

## 神经网络中变量的关系式

这里通过 12 个输入神经单元、3 个输出神经单元的层来作为演示，以 $a(z)$ 为激活函数，可以得到：
$$
\left.\begin{aligned}z^2_1=w^2_{11}x_1+w^2_{12}x_2+w^{2}_{13}x_3+\cdots+w^2_{1\,12}x_{12} + b^2_1
\\z^2_2=w^2_{21}x_1+w^2_{22}x_2+w^{2}_{23}x_3+\cdots+w^2_{2\,12}x_{12} + b^2_2\\
z^2_3=w^2_{31}x_1+w^2_{32}x_2+w^{2}_{33}x_3+\cdots+w^2_{3\,12}x_{12} + b^2_3\\
a_1^2=a(z_1^2),a_2^2 = a(z_2^2),a_3^2=a(z_3^2)\end{aligned}\right\}
$$
使用矩阵的形式可以更容易看清式子的整体关系。
$$
\def\arraystretch{1.5}\begin{pmatrix}z_1^2 \\ z_2^2 \\ z_3^2\end{pmatrix} = \begin{pmatrix}w^2_{11}&w^2_{12}&w^{2}_{13}&\cdots & w^2_{1\,12} \\ w^2_{21}&w^2_{22}&w^{2}_{23}&\cdots & w^2_{2\,12} \\ w^2_{31}&w^2_{32}&w^{2}_{33}&\cdots & w^2_{3\,12}\end{pmatrix}\begin{pmatrix}x_1^2 \\ x_2^2 \\ x_3^2 \\ \vdots \\ x_{12}\end{pmatrix} + \begin{pmatrix}b_1^2 \\ b_2^2 \\ b_3^2\end{pmatrix}
$$

## 学习数据与正解

我们可以将数据的正解定义为离散的变量，并设置对应数量的输出层，通过逻辑运算的方式定义正解，可以得到平方误差和为 $\dfrac{1}{2}\{(t_1 - a_1^3)^2 + (t_2-a_2^3)^2\}$。其中 $\dfrac{1}{2}$ 是为了方便后面的计算，t 为正解，a 为预测值。

上式作为学习使用较为容易理解，但是存在计算收敛时间长的情况。为了克服这个缺陷，人们提出了各种各样的误差指标，其中特别有名的一个指标就是交叉熵。交叉熵将误差函数替换为了
$$
-\dfrac{1}{n}\left[\left\{t_1\log a_1 + (1-t_1)\log(1-a_1)\right\} + \left\{t_2\log a_2 + (1-t_2)\log(1-a_2)\right\}\right]
$$
推广可以得到：
$$
-\dfrac{1}{n}\sum_{i=1}^n\left[t_i\log a_i+(1-t_i)\log(1-a_i)\right]
$$
上式中，n 为数据规模。利用这个交叉熵和 Sigmoid 函数，可以消除 Sigmoid 函数的冗长性，提高梯度下降法的计算速度。

## 神经网络的代价函数

在数学中，用模型参数表示的总体误差的函数称为代价函数，此外也可以称为损失函数、目的函数、误差函数等。

通过最优化的思想，我们可以通过确定参数使得神经网络得出的代价函数最小来得到合适的神经网络模型。

例如，可以记第 k 个训练数据的代价函数为 $C_k$。有：
$$
C_T = \sum C_i
$$
然后使用梯度下降法等方法求出代价函数的最小值对应的参数，建立神经网络模型。

> [!note] 神经网络与回归分析的差异
>
> 虽然神经网络与回归分析确定模型的原理相同，但是它们存在以下差异：
<<<<<<< Updated upstream:00_Inbox/深度学习/神经网络的思想.md
> * 相比回归分析中使用的参数模型，神经网络中使用的参数的数目十分巨大。
> * [[00_Inbox/机器学习/线性模型|线性回归]]分析中使用的函数为一次式，而神经网络中使用的函数（激活函数）不是一次式。
=======
> - 相比回归分析中使用的参数模型，神经网络中使用的参数的数目十分巨大。
> - [[机器学习/线性模型|线性回归]] 分析中使用的函数为一次式，而神经网络中使用的函数（激活函数）不是一次式。
>>>>>>> Stashed changes:深度学习/神经网络的思想.md
>
> 鉴于存在以上差异，相比回归分析，神经网络需要更加强大的数学武器，其中代表性的一种方法就是误差反向传播法。

# 神经网络和误差反向传播法

在实际运用中，梯度下降法存在计算过于复杂，容易进入所谓的导数地狱的问题。为了解决这个问题，人们研究除了误差反向传播法。

## 神经单元误差 $\delta_j^l$

误差反向传播法的特点是将繁杂的导数计算替换为数列的递推关系式，而提供这些递推关系式的就是名为神经单元误差的变量 $\delta_j^l$。利用平方误差 C，其定义如下：
$$
\delta_j^l = \frac{\partial C}{\partial z_j^l}\,\,\,(l=2,3,\cdots)
$$
使用神经单元误差，我们可以将参数表述为下面的形式：
$$
\frac{\partial C}{\partial w_{ji}^{l}} = \delta_j^la_i^{l-1},\frac{\partial C}{\partial b_j^l} = \delta_j^l \quad(l=2,3,\cdots)
$$

## 误差反向传播法

误差反向传播法以梯度下降法为基础，但是将繁杂的导数计算替换为数列的递推关系式。
$$
\delta_i^l=\left\{\delta_1^{l+1}w_{1i}^{l+1} + \delta_2^{l+1}w_{2i}^{l+1}+\cdots+\delta_m^{l+1}w_{mi}^{l+1}\right\}a'(z_i^l)
$$
从上式我们可以发现，利用该式不需要进行复杂的导数计算也可以求出第二层的 $\delta_i^2$ 的值，这就是误差反向传播法。只需要求出输出层的神经单元误差，其他的神经单元误差就不需要进行偏导数的计算。

使用矩阵的形式可以使式子变得更加的简洁，例如：
$$
\def\arraystretch{1.5}\begin{pmatrix}\delta_1^2 \\ \delta_2^2  \\ \delta_3^2\end{pmatrix} = \begin{bmatrix}\begin{pmatrix}w_{11}^3&w_{12}^3&w_{13}^3\\w_{21}^3&w_{22}^3&w_{23}^3\end{pmatrix}^T\begin{pmatrix}\delta_1^3\\\delta_2^3\end{pmatrix}\end{bmatrix}\odot\begin{pmatrix}a'(z_1^2)\\a'(z_2^2)\\a'(z_3^2)\end{pmatrix}
$$
到现在为止，已经可以运用已有的知识实现一个神经网络了，误差反向传播算法可以总结如下：
$$
\def\arraystretch{2.5}\begin{matrix}\bold{A}^l = a(\bold{W}^l\cdot\bold{A}^{l-1}+\bold{B}^l)\\
\bold{\delta}^l=\dfrac{\partial C}{\partial A}\odot a'(\bold{Z}^l)\\
\delta^l = \left[(\bold{W}^{l+1})^T\cdot\delta^{l+1}\right]\odot a'(\bold{Z}^l)\\
\dfrac{\partial C}{\partial w_{ji}^{l}} = \delta_j^la_i^{l-1},\dfrac{\partial C}{\partial b_j^l} = \delta_j^l\end{matrix}
$$

# 卷积神经网络

深度学习是叠加了很多层隐藏层的神经网络，这样的神经网络中隐藏层具有一定的结构，从而更加有效的学习。卷积神经网络是一种深度学习结构。

## 卷积神经网络的组成

卷积神经网络由四个部分组成，分别是：

- 输入层：输入数据
- 卷积层：通过过滤器对图像进行卷积得到的卷积矩阵
- 池化层：将卷积层中分为几个部分，对每个部分使用池化函数进行综合得到池化层，不使用激活函数
- 输出层：将池化层的输出作为输出得到结果

## 卷积神经网络的数学表达

下面是卷积神经网络中的符号含义

| 位置   | 符号              | 含义                                                         |
| ------ | ----------------- | ------------------------------------------------------------ |
| 输入层 | $x_{ij}$        | 图像输入值                                                   |
| 过滤器 | $w_{ij}^{Fk}$   | 第 k 个过滤器第 (i, j) 位置的权重                                |
| 卷积层 | $z_{ij}^{Fk}$   | 卷积层 k 第 (i, j) 位置的加权输入                                |
|        | $b^{Fk}$        | 卷积层 k 的偏置，值得注意的是同一个卷积层中的偏置是相同的      |
|        | $a_{ij}^{Fk}$   | 卷积层 k 第 (i, j) 位置的输出                                    |
| 池化层 | $z_{ij}^{Pk}$   | 池化层 k 第 (i, j) 位置的加权输入                                |
|        | $a_{ij}^{Pk}$   | 池化层 k 第 (i, j) 位置的输出，该值应当与加权输入的值相同        |
| 输出层 | $w_{k-ij}^{On}$ | 从池化层 k 第 (i, j) 位置的神经单元指向输出层第 n 个神经单元的权重 |
|        | $z_{n}^O$       | 输出层第 n 个元素的加权输入                                    |
|        | $b_{n}^O$       | 输出层第 n 个元素的偏置                                        |
|        | $a_{n}^{O}$     | 输出层第 n 个元素的输出                                        |

### 输入层

输入层的输入与输出相同：
$$
a_{ij}^I = x_{ij}
$$

### 过滤器与卷积层

我们将过滤器中的每个元素看作权重，则可以得到：
$$
z_{ij}^{Fk}=w_{11}^{Fk}x_{i,j}^{Fk} + w_{12}^{Fk}x_{i,j+1}^{Fk} + \cdots + w_{nn}^{Fk}x_{i+n,j+n}+ b^{Fk}
$$
最后可以得到输出：
$$
a_{ij}^{Fk} = a(z_{ij}^{Fk})
$$

### 池化层

压缩方法使用最大池化法，即取最大值
$$
\begin{cases}z_{ij}^{Pk}=\max(a_{2i-1,2j-1}^{PK}, a_{2i-1, 2j}^{PK}, a_{2i, 2j-1}^{PK}, a_{2i, 2j}^{PK})\\a_{ij}^{Pk}=z_{ij}^{Pk}\end{cases}
$$

### 输出层

输出层就是将所有池化层的加权输出求和。
$$
z_{n}^O=w_{1-11}^{On}a_{11}^{P1}+w_{1-12}^{On}a_{12}^{P1}+\cdots+w_{2-11}^{On}a_{11}^{P2} + \cdots + b_n^O
$$
最后得到输出：
$$
a_{n}^O = a(z_{n}^O)
$$

## 卷积神经网络与误差反向传播法

与之前相同，误差反向传播法基于梯度下降法，定义了神经单元误差：
$$
\delta_{ij}^{Fk} = \frac{\partial C}{\partial z_{ij}^{Fk}} \quad\quad \delta_{n}^O=\frac{\partial C}{\partial z_n^O}
$$
与神经网络相同，我们可以使用神经单元误差来表示输出层的偏置与权重：
$$
\frac{\partial C}{\partial w_{k-ij}^{On}}=\delta_{n}^Oa_{ij}^{Pk} \quad\quad \frac{\partial C}{\partial b_n^O}=\delta_n^O
$$

卷积层的偏置与权重可以表示如下：
$$
\begin{cases}\dfrac{\partial C}{\partial w_{ij}^{Fk}}=\delta_{11}^{Fk}x_{i,j} + \delta_{12}^{Fk}x_{i,j+1}+\cdots+\delta_{44}^{Fk}x_{i+3,j+3}\\
\dfrac{\partial C}{\partial b^{Fk}}=\delta_{11}^{Fk} + \delta_{12}^{Fk} + \cdots + \delta_{44}^{Fk}\end{cases}
$$

类比与简单神经网络的反向误差传播法，我们需要递推关系使用输出层的神经单元误差反向求出卷积层的神经单元误差，首先计算输出层的神经单元误差：
$$
\delta_n^O=(a_n^O-t_n)a'(z_n^O)
$$

可以得到神经单元误差的递推式为：
$$
\begin{split}\delta_{ij}^{Fk}=\{\delta_1^Ow_{k-i'j'}^{O1} + \delta_2^Ow_{k-ij}^{O2}+\delta_3^Ow_{k-i'j'}^{O3} + \cdots\}\\\times(当a_{ij}^{Fk}在区块中最大时为1，否则为0)&\times a'(z_{ij}^{Fk})\end{split}
$$

其中 k,i,j 与前面含义相同，i', j' 表示卷积层 i 行 j 列的神经单元连接的池化层神经单元的位置。
