---
tags:
  - 深度学习
---
# 计算机视觉

计算机视觉是深度学习中的一个主要领域，自动驾驶、医学诊断等应用都离不开计算机视觉。这里简单介绍一些计算机视觉中的基本知识、数据处理方式，并结合 [[00_Inbox/深度学习/现代卷积神经网络]] 描述各种各样的计算机视觉任务，如目标检测 (object detection)、语义分割 (semantic segmentation) 和样式迁移 (style transfer)。

## 图像增广

大型数据集是成功运用深度学习模型的先决条件，而图像增广就是对图像经过一系列的处理后，生成相似但是不同的训练样本，从而扩大数据集的规模。同时，应用图像增广技术，可以随机地减少模型对数据中某些属性的依赖，从而提高模型的泛化能力。该技术在 AlexNet 中首次被使用，并且是 AlexNet  成功的一个重要原因。

在 pytorch 中，常用的图像处理函数被封装在了 `torchvision` 包中，使用 `import torchvision` 来导入该库。

### 常用的图像增广技术

常用的图像增广技术包括翻转、裁剪、调整颜色。
- 翻转：翻转一般不会改变图像的类别，是使用最早、最广的一种增广技术。左右翻转是常用的，而上下翻转不如左右翻转常用。
- 裁剪：通过对图像进行随机裁剪，使物体以不同的比例出现在图像的不同位置，这样可以降低模型对目标位置的敏感性。
- 调整颜色：通过调整图像颜色的四个方面：亮度、对比度、饱和度和色调，来得到不同的数据。
- 组合使用上述方法：将上述的方法组合，可以得到一个更加庞大的数据集。

以下面的图为例，列举常用的图像增广技术与 pytorch 的 API 实现。

|            图像增广技术            |            pytorch 代码             |                 结果                 |
|:----------------------------------:|:-----------------------------------:|:------------------------------------:|
|                原图                |                                     |          ![[Assets/output 1.svg]]           |
|              左右翻转              | `transforms.RandomHorizontalFlip()` |          ![[Assets/output 2.svg]]           |
|              上下翻转              |  `transforms.RandomVerticalFlip()`  |           ![[Assets/output.png]]            |
|              随机裁剪              |  `transforms.RandomResizedCrop()`   | ![[Assets/Pasted image 20230730235927.png]] |
| 随机调整亮度、饱和度、对比度、色相 |     `transforms.ColorJitter()`      | ![[Assets/Pasted image 20230731000140.png]] |
|              组合使用              |     `transforms.Compose([...])`     | ![[Assets/Pasted image 20230731000300.png]] |
|                                    |                                     |                                      |

需要注意的是，`torchvision.transforms` 模块中的增广函数都是基于 `PIL.Image` 对象的，因此在对 `numpy` 数组或者 `torch` 张量进行变换之前，需要在前面添加 `transforms.ToImage` 将其转变为 `Image` 对象。

>Q：为什么经过 transform 后得到的图片数量没有变化，却说数据被增广了？
>A：由于在每个 epoch 中，都会经过 transform 的变化，而这个变化是按照概率进行的，也就是说每次进行处理得到的图像都不相同，于是在数据量没有变化的同时增广了数据。

## 迁移学习简介

在实践中，我们常用的数据集少的有 6 万张图像的 Fashion-MNIST 图像数据集, 多的有超过 1000 万张图像和 1000 类目标的 ImageNet 数据集。大的数据集通常需要大的模型来进行学习，但是这样大的模型在简单的数据集中就容易过拟合。一种显而易见的方式是收集更大的数据集，但是收集和标注数据通常需要耗费大量的时间和金钱。另一种解决方法是**迁移学习** (transfer learning)，将从源数据集学习得到的知识迁移到目标数据集。尽管 ImageNet 中可能大多数的图像都与目标无关，但是模型可能会提取通用特征，这有助于识别边缘、纹理、形状和目标组合。

### 微调

**微调** (fine-tuning) 是迁移学习中的一种常见技巧，其包含下面的 4 个步骤：
1. 在源数据集 (如 ImageNet) 上训练神经网络模型，即源模型。
2. 创建一个新的网络模型，即目标模型。这将复制源模型上的所有的模型设计及参数 (除了输出层)。我们假设模型参数包含从源数据集中学习的知识，这些知识将使用于目标数据集，同时我们假设源模型的输出层与源数据集中的标签密切相关，因此不在目标模型中使用该层。
3. 向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。
4. 在目标数据集上训练目标模型。输出层将从头开始进行训练，而其他所有层的参数将根据源模型的参数进行微调。

当目标数据集比源数据集小得多时，微调有助于提高模型的泛化性能。

## 目标检测

之前我们提到的都是图像分类模型，在这些任务中，我们假设图像只有一个主要目标，只关注如何识别其类别。然而，很多时候可以在图像中找到多个我们感兴趣的目标，我们不仅想知道它的类别，也想得到它们的位置。在计算机视觉领域中，这个任务被称为**目标检测** (object detection) 或者**目标识别** (object recognition)。

### 边界框

在目标检测中，我们常使用边界框 (bounding box) 来描述对象的空间位置。边界框是矩形，由矩形左上角和右下角的 x 与 y 坐标确定。另一种描述方式是边界框的中心和长宽表示。

在 matplotlib 中，矩形使用 (左上坐标，宽度，高度) 进行表示，需要进行转换才能可视化。这几种表示方式的数据量均相同，因此都可以作为边界框的编码形式。

![[Assets/Pasted image 20230801114028.png]]

### 锚框

目标检测算法通常会在输入图像中抽取大量的区域，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边界，从而更加准确地预测目标的真实边界框 (ground-truth bounding box)。不同模型使用的区域的抽样方法可能不同，这里介绍其中的一种：以每个像素为中心，生成多个缩放比和宽高比不同的边界框，这些边界框被称为锚框 (anchor box)。

#### 生成多个锚框

假设输入图像的高度为 $h$，宽度为 $w$，我们以图像的每个像素为中心生成形状不同的锚框，使用缩放比 (scale) $s\in(0,1]$，宽高比 (aspect ratio) $r>0$，那么锚框的宽度和高度分别为 $hs\sqrt{r}$ 和 $hs/\sqrt{r}$。当中心给定，那么已知宽和高锚框就固定了。

为了形成形状不同的锚框，我们需要从一组 $s_i$ 与一组 $r_i$ 中取不同的值进行组合，为了减小资源消耗的同时更好的覆盖所有形状，我们仅使用 $s_1$ 和 $r_1$ 的不同组合，即
$$
(s_1,r_1),(s_1,r_2),\cdots,(s_1,r_n)与(s_2,r_1),(s_3,r_1),\cdots,(s_n,r_1)
$$
这样得到的每个像素上的锚框数量为 $(m+n-1)$，对于整个图像，一共将生成 $wh(m+n-1)$ 个锚框。

通过函数 `multibox_prior` 可以得到 (批量大小, 锚框总数量, 4) 的输出，其中批量大小为输入图片张量的批量大小，锚框总数量为一张图片中所有锚框的数量，4 即锚框的表示，这里使用四角坐标进行表示。可以通过 `reshape` 方法将其转变为 (图像高度, 图像宽度, 以同一像素为中心的锚框数量, 4) 便于可视化。

![[Assets/Pasted image 20230801114059.png]]

### 交并比 (IoU)

有了锚框，我们需要一个指标来衡量某个锚框较好的覆盖了图像中的狗。如果已知真实的边界框，那么可以直接使用相似度来进行度量。杰卡德指数 (Jaccard index)，也称杰卡德相似系数 (Jaccard similarity coefficient) 可以度量两组集合的相似性。给定集合 $A$ 和集合 $B$，杰卡德指数指他们的交并比：
$$
J(A,B)=\frac{|A\cap B|}{|A\cup B|}
$$
在这里，我们将任何边界框的像素区域视为一个集合，于是我们可以使用杰卡德指数来衡量他们之间的相似度，称为交并比 (intersection over union, IoU)，即两个边界框相交面积与相并面积之比。

![[Assets/Pasted image 20230801115153.png]]

### 在训练数据中标注锚框

在训练集中，我们将每个锚框视为一个训练样本。为了训练检测目标的模型，我们需要每个锚框的类别和偏移量标签，其中前者是与每个锚框相关的目标的类别，后者是真实边界框对于锚框的偏移量。在预测时，我们为每张图像生成多个锚框，预测所有的锚框类别和偏移量，根据预测的偏移量来调整他们的位置已获得预测的边界框，最后指输出符合特定条件的预测边界框。

#### 将真实边界框分配给锚框

这里介绍一个用于标注锚框的算法。该算法将最接近真实边界框分配给锚框，以此来对锚框进行标注。给定图像，假设锚框是 $A_1,A_2,\cdots,A_{n_a}$，真实边界框是 $B_1,B_2,\cdots,B_{n_b}$，其中 $n_a\geqslant n_b$，我们定义一个矩阵 $X\in\mathbb R^{n_a\times n_b}$，其中第 $i$ 行 $j$ 列元素 $x_{ij}$ 是锚框 $A_i$ 和真实边界框 $B_j$ 的 IoU，该算法描述如下：
1. 在矩阵 $X$ 中找到最大的元素，并将它的行索引和列索引分别表示为 $i_1$ 和 $j_1$。然后将真实边界框 $B_{j_1}$ 分配给锚框 $A_{i_1}$。在第一个分配完成后，丢弃矩阵中第 $i_1$ 行和第 $j_1$ 列中的所有元素。
2. 在矩阵 $X$ 中找到剩余元素中的最大元素，并将它的行索引和列索引分别表示为 $i_2$ 和 $j_2$，然后将真实边界框 $B_{j_2}$ 分配给锚框 $A_{i_2}$ 分配完成后，丢弃矩阵中第 $i_2$ 行和第 $j_2$ 行的所有元素。
3. 重复直到为 $n_b$ 个锚框分配了一个真实边界框。
4. 遍历剩余的 $n_a-n_b$ 个锚框，根据阈值判断是否为他们分配真实边界框。

#### 标记类别和偏移量

现在为每个锚框标记类别和偏移量。假设一个锚框 $A$ 被分配给了真实边界框 $B$，一方面，锚框 $A$ 的类别将被标注为与 $B$ 相同，另一个方面，锚框 $A$ 的偏移量将更加锚框 $A$ 与真实边界框 $B$ 的中心坐标的相对位置和这两个框的相对大小进行标注。

由于数据集中不同的框的位置和大小不相同，我们可以对那些相对位置和大小应用变换，使偏移量的分布更加均匀且易于拟合。下面介绍一种常见的转换，给定框 $A$ 和 $B$，中心坐标为 $(x_a,y_a)$ 和 $(x_b,y_b)$，宽度分别为 $w_a$ 和 $w_b$，高度分别为 $h_a$ 和 $h_b$，可以将 $A$ 的偏移量标注为
$$
\left(\frac{\dfrac{x_b-x_a}{w_a}-\mu_x}{\sigma_x},\frac{\dfrac{y_b-y_a}{h_a}-\mu_y}{\sigma_y},\frac{\log\dfrac{w_b}{w_a}-\mu_w}{\sigma_w},\frac{\log\dfrac{h_b}{h_a}-\mu_h}{\sigma_h}\right)
$$
其中，常量的默认值为 $\mu_x=\mu_y=\mu_w=\mu_h=0$，$\sigma_x=\sigma_y=0.1$，$\sigma_w=\sigma_h=0.2$。

如果一个锚框没有被分配真实边界框，我们只需要将锚框的类别标注为背景 (background)。背景类别的锚框通常被视为负类锚框，其余的锚框被称为真类锚框。

### 使用非极大值抑制预测边界框

在预测时，我们先为图像生成多个锚框，再为这些锚框一一预测类别和偏移量。一个预测好的边界框则根据其中某个带有偏移量的锚框生成。

当有许多的锚框时，可能会输出许多相似的具有明显重叠的预测边界框，他们都围绕着同一目标。为了简化输出，我们可以使用非极大值抑制 (non-maximum suppression, NMS) 合并属于同一目标的相似的边界框。下面给出非极大值抑制的原理。

对于一个预测边界框 $B$，目标检测模型会计算每个类别的预测概率。假设最大的预测概率为 $p$，则该概率所对应的类别 $B$ 即为预测的类别。具体来说，我们将 $p$ 称为预测边界框 $B$ 的置信度。在同一张图像中，所有预测的非背景边界框都按照置信度降序排序，以生成列表 $L$。然后我们根据下面的步骤操作排序列表 $L$：
1. 从 $L$ 中选取置信度最高的预测边界框 $B_1$ 作为基准，然后将所有与 $B_1$ 的 IoU 超过预订阈值 $\epsilon$ 的非基准预测边界框从 $L$ 中移除。
2. 重复上面的步骤，直到 $L$ 中所有的边界框都被作为基准过。
3. 输出列表 $L$ 中的边界框。

在实践中，我们甚至可以在执行非极大值抑制前先将置信度较低的边界框移除，从而减小算法的计算量。

## 多目标尺度检测

在之前，我们以每个图像为中心，生成了多个锚框，基本而言，这些锚框代表了图像中的不同区域。然而，如果为每个像素都生成一个锚框，这样得到的需要计算的锚框数量太多了。多尺度锚框是一种减少计算量的方式。

### 多尺度锚框

我们可以在不同的尺度下，生成不同数量不同大小的锚框。直观的说，在较大的尺度上，我们生成较大的锚框，抽样较少的区域；在较小的尺度上，我们使用较小的锚框来检查较小的目标，抽样较多的区域。
<div align="center">
   <img src=""  height=160><img src="" height=160>
</div>

### 多尺度目标检测

这里介绍一种多尺度目标检测模型，单发多框检测 (SSD)。该模型简单、快速且被广泛使用，尽管知识目标检测模型之一，但其设计原则和实现细节也适用于其他模型。

#### 模型

SSD 模型结构简单可以概括为下面的图：
![[Assets/Pasted image 20230805174522.png]]

##### 类别预测网络

设目标类别数量为 $q$，这样需要预测的类别数量为 $q+1$，其中 $0$ 类为背景。在某个尺度下，设特征图的长宽为 $w$ 与 $h$，如果每个中心生成 $a$ 个锚框，则需要进行 $hwa$ 个锚框分类，如果使用全连接层，那么将会面临参数过多的情况。这里使用卷积层，在进行分类的同时保证了锚框之间的位置关系。

具体来说，就是使用一个输出通道数为 $a(q+1)$ ，输出图像大小不变的卷积层，其中索引为 $i(q+1)+j$ 的代表了索引为 $i$ 的锚框有关索引为 $j$ 的类别的预测。

##### 边界框预测网络

边界框预测方式的设计与类别预测层相同，唯一的不同是需要为每个锚框预测 4 个偏移量，而不是 $q+1$ 个类别。

##### 连结多尺度的预测

单发多框检测使用不同尺度的锚框来预测类别和偏移量，在不同的尺度下，特征图的形状和以同一像素为中心的锚框的个数可能会不同，因此在不同尺度下预测出的形状也不相同。为了提高效率，我们将不同尺度的锚框连接起来，进行计算。

具体来说，对于两个张量，形状分别为 `(batch_size, channels1, h1, w1)` 和 `(batch_size, channels2, h2, w2)`，我们通过将其转换为 `(batch_size, h*w*channels)` 后再将其相连进行运算。

经过这样的处理，不同尺度的向量仍然可以批量的形式进行计算。

##### 高和宽减半块

为了在多个尺度下检测目标，我们定义了下面的高度和宽度都减半的块，实际上这就是 [[00_Inbox/深度学习/现代卷积神经网络#VGG]] 中定义的 VGG 网络。

```python
def down_sample_block(in_channels, out_channels):
	block = []
	for _ in range(2):
		block.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))
		block.append(nn.BatchNorm2d(out_channels))
		block.append(nn.ReLU())
		in_channels = out_channels
	block.append(nn.MaxPool2d(2))
	return nn.Sequential(*block)
```

上面的网络块将会将改变输入的通道数，并将输入的图像尺寸减半输出。

##### 基本网络块

基本网络块用于从图像中抽取特征，这里使用一个简单的基本网络块。

```python
def base_net():
	 block = []
	 num_filter = [3, 16, 32, 64]
	 for i in range(len(num_filter) - 1):
		 blk.append(down_sample_block(num_filter[i], num_filter[i+1]))
	return nn.Sequential(*blk)
```

该基本模块使用了 4 个减半模块，每次将通道数翻倍。

#### 完整的网络结构

完整的 SSD 模型由 5 个模块组成：
1. 基本网络块
2. 高和宽减半块
3. 高和宽减半块
4. 高和宽减半块
5. 高和宽减半块，并使用全局最大池化将高度和宽度都降为 1

其中第一个基本网络块用于图像特征的提取，之后每个块称为多尺度特征块，输出的特征图既用于生成锚框，又用于预测这些锚框的类别和偏移量。于是，我们可以得到每个多尺度特征块的向前传播函数中需要的要素为
```python
def forward_blk(X, blk, scale, ratio, class_pred, bbox_pred):
	Y = blk(X)
	anchors = mutlibox_prior(Y, scale=.., ratio=..)
	class_pred = class_pred_block(Y)
	bbox_pred = bbox_pred_block(Y)
	return (Y, anchors, class_pred, bbox_pred)
```
于是 TinySSD 网络的向前传播函数为
```python
def forward(X):
	anchors, class_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5
	for i in range(5):
		X, anchor, class_pred, bbox_pred = forward_blk(
			X, getattr(self, f'blk_{i}'), 
			sizes[i], ratios[i],
			getattr(self, f'cls_{i}'), 
			getattr(self, f'bbox_{i}')
		)
	anchors = torch.cat(anchors, dim=1)
	class_pred = concat_preds(class_pred)
	class_preds = class_preds.reshape(class_preds.shape[0], -1, self.num_classes + 1)
	bbox_preds = concat_preds(bbox_preds)
	return anchors, class_preds, bbox_preds
```

#### 模型的训练

接下来介绍模型的训练。

##### 损失函数

目标检测需要两种类型的损失函数。
- 锚框类别的损失函数：可以简单的使用交叉熵来处理锚框类别的损失。
- 锚框偏移量的损失函数：锚框偏移量的预测是一个回归问题，但是这里不推荐使用均方误差来作为损失函数，推荐使用 $L1$ 范数，即预测值与真实值之差的绝对值，使用掩码来排除负类锚框与填充锚框的损失计算。

最后我们将上面两个损失相加得到目标检测问题的损失函数。在评价模型时，我们可以使用准确率来评价锚框类别，使用平均绝对误差来评价锚框偏移度。