---
tags:
  - 概率论
  - 概率论与数理统计
---

# 大数定律与中心极限定理

## 依概率收敛

设[[00-笔记/概率论与数理统计/一维随机变量及其分布|随机变量]] $X$ 与随机变量序列 $\{ X_{n} \}(n=1,2,\cdots)$，如果对任意的 $\varepsilon>0$，有
$$
\lim\limits_{ n \to \infty } P\{ |X_{n}-X|\geqslant \varepsilon \}=0\ \text{或者}\ P\{ |X_{n}-X|<\varepsilon \}=1
$$
则称随机变量序列 $\{ X_{n} \}$ **依概率收敛于随机变量 $X$**，记为
$$
\lim\limits_{ n \to \infty } X_{n}=X(P)\ \text{或者}\ X_{n} \xrightarrow{P}X(n\to \infty)
$$

> [!note]
> 1. 上面的定义将 $X$ 写成常数 $a$ 也成立。
> 2. 如果 $X_{n}\xrightarrow{P}X,Y_{n}\xrightarrow{P}Y$，$g(x,y)$ 是二元连续函数，则 $g(X_{n},Y_{n})\xrightarrow{P}g(X,Y)$，一般地，对于 $m$ 元连续函数，上式也成立。

> [!tip] 类比数列的极限
> 1. 在极限理论中，设 $\{ x_{n} \}$ 是一个数列，当 $n\to \infty$ 时，对于任意的 $\varepsilon>0$，有 $|x_{n}-x|<\varepsilon$，称数列收敛于 $x$, 即 $x_{n}\to x$。
> 2. 在概率论中，设 $\{ X_{n} \}$ 是一个随机变量序列，当 $n\xrightarrow{p}\infty$ 时，即 $n$ 以概率 $p$ 趋于无穷时，对于任意的 $\varepsilon>0$，有 $P\{ |X_{n}-X|<\varepsilon \}\to 1$，称随机变量序列 $\{ X_{n} \}$ 以概率 $p$ 收敛于 $X$，即 $X_{n}\xrightarrow{p}X$。

## 大数定律

### 切比雪夫大数定律

假设 $\{ X_{n} \}$ 是相互独立的随机变量序列，如果方差 $DX_{i}$ 存在且一致有上界，即存在常数 $C$，使得 $DX_{i}\leqslant C$ 对一切 $i\geqslant 1$ 均成立，则 $\{ X_{n} \}$ 服从大数定律：
$$
\frac{1}{n}\sum\limits_{i=1}^{n}X_{i}\xrightarrow{P}\frac{1}{n}\sum\limits_{i=1}^{n}EX_{i}
$$

> [!note] 当样本容量足够大时，随机变量序列的均值收敛于随机变量序列的期望。

> [!warning] 条件
> 1. 独立同分布
> 2. 方差存在且有一致上界

### 伯努利大数定律

假设 $\mu_{n}$ 是 $n$ 重伯努利试验中事件 $A$ 发生的次数，在每次试验中事件 $A$ 发生的概率为 $p$，则
$$
\dfrac{\mu_{n}}{n}\xrightarrow{P}p
$$
即对任意的 $\varepsilon> 0$，有
$$
\lim\limits_{ n \to \infty } P\left\{ \left|\frac{\mu_{n}}{n}-p \right|<\varepsilon \right\} =1
$$

> [!note] 当样本容量足够大时，伯努利试验的频率收敛于概率。

> [!note] 经验分布函数
> 在数理统计中，我们可以用频率模拟分布函数，即对于样本，求
> $$
F_{n}(x)=\dfrac{I\left\{ x_{1},x_{2},\cdots,x_{n}\text{中小于等于}x\text{的样本值个数} \right\} }{n}
> $$
> 根据伯努利大数定律可知，当 $n$ 足够大时，$F_{n}(x)$ 可以作为未知分布函数 $F(x)$ 的一个近似。

### 辛钦大数定律

假设 $\{ X_{n} \}$ 是独立同分布的随机变量序列，如果数学期望 $EX_{i}=\mu$ 存在，则
$$
\frac{1}{n}\sum\limits_{i=1}^{n}X_{i}\xrightarrow{P}\mu
$$
即对任意 $\varepsilon>0$，有
$$
\lim\limits_{ n \to \infty } P\left\{ \left| \frac{1}{n}\sum\limits_{i=1}^{n}X_{i}-\mu \right| <\varepsilon \right\} =1
$$

> [!note] 当样本容量足够大时，则样本的均值收敛于总体的均值 (数学期望)。

> [!warning] 条件
> 1. 独立同分布
> 2. 数学期望存在

## 中心极限定理

> [!note] 中心极限定理
> 中心极限定理基本都在描述这样一个事实：  
> 不论 $X_{i}$ 服从什么样的分布，只要将它们求和，在 $n$ 足够大时，它们的和近似服从[[00-笔记/概率论与数理统计/一维随机变量及其分布#正态分布|正态分布]]，在 $n\to \infty$ 时，$\sum\limits_{i=1}^{n}X_{i}\sim N(n\mu,n\sigma^{2})$。


### 列维-林德伯格定理

假设 $\{ X_{n} \}$ 是独立同分布的随机变量序列，如果
$$
EX_{i}=\mu,DX_{i}=\sigma^{2}>0
$$
存在，则对任意的实数 $x$，有
$$
\lim\limits_{ n \to \infty } P\left\{ \frac{\sum\limits_{i=1}^{n}X_{i}-n\mu}{\sqrt{ n }\sigma}\leqslant x \right\} =\frac{1}{\sqrt{ 2\pi }}\int_{-\infty}^{x}e^{ -\frac{t^{2}}{2} }\text{d}t=\varPhi(x)
$$

> [!note]
> 1. 定律必须满足三个条件：
> 	- ==独立==
> 	- ==同分布==
> 	- ==期望、方差存在==
> 2. 只要 $X_{n}$ 满足定理条件，那么当 $n$ 很大时，独立同分布的随机变量的和 $\sum\limits_{i=1}^{n}X_{i}$ 近似服从正态分布 $N(n\mu,n\sigma^{2})$，由此可知，当 $n\to \infty$ 时，有
> $$
P\left\{ a<\sum\limits_{i=1}^{n}X_{i}<b \right\} \approx \varPhi\left( \frac{b-n\mu}{\sqrt{ n }\sigma} \right)-\varPhi\left( \frac{a-n\mu}{\sqrt{ n }\sigma} \right)
> $$
> 只要题目涉及独立同分布随机变量的和，我们就应该考虑中心极限定理。

### 棣莫弗-拉普拉斯定理

假设随机变量 $Y_{n}\sim B(n,p)$，则对任意实数 $x$，有
$$
\lim\limits_{ n \to \infty } P\left\{ \frac{Y_{n}-np}{\sqrt{ np(1-p) }}\leqslant x \right\} =\frac{1}{\sqrt{ 2\pi }}\int_{-\infty}^{x}e^{ -\frac{t^{2}}{2} }\text{d}t
$$

> [!note] 计算二项分布概率的方法
> 1. $n$ 不太大时，直接计算
> 2. $n$ 比较大，$p$ 比较小，而 $\lambda=np$ 适中时，利用泊松分布计算。
> 3. $n$ 较大，而 $p$ 不大时 ($p<0.1, np\geqslant 10$)，使用正态分布计算。

---
< [[00-笔记/概率论与数理统计/随机变量的数字特征|随机变量的数字特征]] | [[00-笔记/概率论与数理统计/数理统计|数理统计]] >