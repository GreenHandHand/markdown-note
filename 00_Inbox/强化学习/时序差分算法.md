---
tags:
  - 强化学习
---
# 时序差分算法

[[00_Inbox/强化学习/动态规划算法]] 要求马尔科夫决策过程是已知的，即要求与智能体交互的环境是完全已知的。在这样的条件下，智能体不需要与环境交互来采样数据，直接通过动态规划算法就可以解出最优价值或策略。但是这在大部分场景下并不现实，对于大部分的强化学习任务，其马尔科夫决策过程的状态转移概率是无法写出的，智能体只能和环境进行交互，通过采样得到的数据来学习，这类学习方法统称为**无模型的强化学习**。

本部分介绍无模型的强化学习中的两大经典算法：Sarsa 算法与 Q-learning 算法，它们都是基于时序差分的强化学习算法。

## 时序差分

时序差分是一种用来估计策略的价值函数的方法，它结合了蒙特卡洛和动态规划的思想。在 [[00_Inbox/强化学习/马尔科夫决策过程#蒙特卡洛方法|蒙特卡洛方法]] 中，我们使用下面的式子来更新状态价值函数：
$$
V(s_t) \leftarrow V(s_t) + \alpha[\,G_t - V(s_t)\,]
$$
这里我们将 $1/N(s)$ 替换为了 $\alpha$，表示对价值的计算步长。可以将 $\alpha$ 取一个常数，但是此时更新的值不再严格的等于期望。蒙特卡洛方法必须等整个序列采样完成后才可以计算这一次的回报 $G_t$，但是时序差分算法在当前步结束时就可以计算。具体来说，时序差分使用下面的式子来更新状态价值函数：
$$
V(s_t)\leftarrow V(s_t)+\alpha[\,r_t + \gamma V(s_{t+1}) - V(s_t)\,]
$$
其中 $r_t +\gamma V(s_{t+1}) - V(s_t)$ 称为时序差分（temporal difference, TD）误差（error），时序差分算法将其与步长的乘积作为状态价值函数的更新量。这个算式来自：
$$
\begin{align}V^{\pi}(s)&=E_{\pi}[\,G_t|S_t=s\,]\\&=E_{\pi}[\,\sum_{k=0}^{\infty}\gamma^kR_{t+k}|S_t=s\,]\\&=E_{\pi}[\,R_t+\gamma\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}|S_t = s\,]\\&=E_{\pi}[\,R_t+\gamma V^{\pi}(S_{t+1})|S_t=s\,]\end{align}
$$
于是在用策略与环境交互时，每采样一步，我们就可以用时序差分算法来更新状态价值估计。可以证明，$V(s)$ 最终可以收敛到 $V^{\pi}(s)$。

## Sarsa 算法

使用时序差分算法可以估计状态价值函数，那么是否可以使用 [[00_Inbox/强化学习/动态规划算法#策略迭代算法|策略提升]] 类似的方法来进行强化学习？

Sarsa 算法就是直接使用时序差分进行强化学习的算法。使用时序差分可以估计动作价值函数：
$$
Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[\,r_t+\gamma Q(s_{t+1}, a_{t+1})-Q(s_t,a_t)\,]
$$
然后我们再使用贪婪算法来选取每个状态下动作价值函数最大的那个动作，即 $\arg\max_aQ(s,a)$。

上面的要素已经可以构成一个完整的强化学习算法，下面对算法进行讨论：
1. 如果要准确的估计策略的状态价值函数，我们就需要使用极大量的样本来进行更新。而实际上，我们可以忽略这一点，仅使用一些样本对策略进行评估。这样做的原因是策略提升可以在策略评估未完全进行的情况下进行，这也是价值迭代算法可以实现的原因。
2. 如果在策略提升中一直使用贪婪的方法，那么在算法收敛时可能还存在一些状态动作从来没有在序列中出现，以至于无法对其动作价值进行评估，进而无法保证策略提升后的策略比之前的好。可以采用 $\epsilon$ -贪婪策略，以 $1-\epsilon$ 的概率采用动作价值最大的动作，另外有 $\epsilon$ 的概率从动作空间中随机采取一个动作，从而更加充分的进行探索：
$$
\pi(a|s)=\begin{cases}\epsilon\,/\,|A| + 1 - \epsilon&if\quad a=\arg\max_{a'}Q(s,a')\\\epsilon\,/\,|A|&其他动作\end{cases}
$$

现在，我们可以得到一个基于时序差分算法的强化学习算法，该算法的动作价值更新用到了当前状态 $s$，当前动作 $a$，获得的奖励 $r$，下一个状态 $s'$ 和下一个动作 $a'$，因此该算法称为 Sarsa 算法。其过程如下：
```python
# initialize the Q table
Q = np.zeros(len(S), len(A))
for i in range(sequence_num):
	s = start_state
	a = epsilon_choic_action(Q, epsilon)
	done = False
	while not done:
		reward, s_new, done = next_step(s, a)
		a_new = epsilon_choic_action(Q, epsilon)
		Q[s][a] += alpha * (reward + gamma * Q[s_new][a_new] - Q[s][a])
		s = s_new
		a = a_new
```

## 多步 Sarsa 算法

时序差分算法只顾及到了下一个状态的价值估计而不是真实价值，因此时序差分算法得到的估计值是有偏估计。而蒙特卡洛方法得到的估计是无偏估计，为了修正时序差分算法中估计值的有偏性，提出了多步时序差分算法。使用公式表示，即将
$$
G_{t}=r_t+\gamma Q(s_{t+1},a_{t+1})
$$
替换成
$$
G_{t}=\sum_{k = 0}^{n-1}\gamma^kr_{t+k} + \gamma^nQ(s_{t+n},a_{t+n})
$$
相应的，将
$$
Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[\,r_t+\gamma Q(s_{t+1}, a_{t+1})-Q(s_t,a_t)\,]
$$
替换成
$$
Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[\,\sum_{k=0}^{n-1}\gamma^kr_{t+k}+\gamma^{n} Q(s_{t+n}, a_{t+n})-Q(s_t,a_t)\,]
$$

## Q-learning 算法

Q-learning 算法也是一种使用时序差分方法的强化学习算法，它与 Sarsa 算法的主要差别在与 Q-learning 算法的时序差分更新方法为：
$$
Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[\,r_t+\gamma\max_a Q(s_{t+1}, a)-Q(s_t,a_t)\,]
$$

算法的流程如下：
```python
# initialize the Q table
Q = np.zeros(len(S), len(A))
for i in range(sequence_num):
	s = start_state[i]
	done = False
	while not done:
		a = epsilon_choic_action(Q, epsilon)
		reward, s_new = next_step(s, a)
		Q[s][a] += alpha * (reward + gamma * max([Q[s_new][a_new] for a_new in A]) - Q[s][a])
		s = s_new
```

我们可以使用价值迭代的思路来理解 Q-learning 算法，即使用
$$
Q^*(s,a)=r(s,a)+\gamma\sum_{s'\in\mathcal S}P(s'|s,a)\max_{a'\in\mathcal A}Q^*(s',a')
$$
来进行迭代，并根据 $\epsilon$ -贪心策略进行探索。

由于 Sarsa 算法必须使用本轮 $\epsilon$ -贪心策略得到的数据进行迭代，所以我们称这类算法为在线策略（on-policy）。相对的，Q-learning 算法不一定使用当前步 $\epsilon$ -策略得到的数据，因此称为离线策略（off-policy）。这在强化学习中是两个非常重要的概念。一般而言，离线策略能够重复使用过往的训练样本，往往具有更小的样本复杂度，因此更受欢迎。