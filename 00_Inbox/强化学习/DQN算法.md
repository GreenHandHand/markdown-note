---
tags:
  - 强化学习
---
# DQN 算法

在 Q-learning 算法中，我们通过动作价值函数 $Q[s][a]$ 建立的矩阵进行计算，然而，使用矩阵作为储存单位只在环境的状态、动作都是离散的，并且空间都非常小的情况下才适用。对于现实中的任务，环境的状态、动作是连续的，或者状态动作数量非常大的时候，这种做法就不适用了。

对于这种情况，我们需要使用函数拟合的方法来估计 $Q$ 的值，即将之前的 $Q$ 表视为数据，使用一个参数化的 $Q_{\theta}$ 来拟合这些数据，称为近似方法。

DQN 算法可以用于解决连续状态下离散动作的问题。

## 车杆环境

车杆环境 (CartPole-v1) 的状态值是连续的，动作是离散的。该环境无法使用 [[00_Inbox/强化学习/时序差分算法#Q-learning 算法|Q-learning算法]] 等传统方式进行求解，本部分介绍的 DQN 算法可以用于解决这类问题。

车杆环境中的，有一辆小车，智能体的任务是通过左右移动保持车上的杆竖直，若杆的倾斜角度过大，或者车子距离初始位置的左右偏离程度过大，或者坚持达到 200 帧时游戏结束。智能体每坚持一分钟，则获得分数为 1 的奖励，坚持时间越长，则最后获得的分数越高，坚持 200 帧就可以获得最高的分数。

车杆环境的状态空间：

|维度|状态|最小值|最大值|
|:---:|:---:|:---:|:---:|
|0|车的位置|-2.4|2.4|
|1|车的速度|-Inf|Inf|
|2|杆的角度|-41.8°|41.8°|
|3|杆尖端的速度|-Inf|Inf|

车杆环境的动作空间：

|表号|动作|
|:---:|:---:|
|0|向左移动车|
|1|向右移动车|

## DQN 

在车杆环境中得到的动作价值函数 $Q(s,a)$ 中，由于每一维度都是连续的，因此无法使用表格记录，因此一个常见的解决方式是函数拟合。由于神经网络具有强大的表达能力，因此可以使用一个神经网络来表示函数 $Q$，然后输出一个标量，表示在状态 $s$ 下进行动作 $a$ 能获得的价值。如果动作是有限的，我们还可以只将状态 $s$ 输入神经网络，使其同时输出每一个动作的 $Q$ 值。

通常 DQN 算法（以及 Q-learning）算法只能处理动作离散的情况，因为在更新过程中有 $max_a$ 这一个操作。假设神经网络用来拟合函数的参数为 $\omega$，即每个状态下所有可能动作 $a$ 的 $Q$ 值都能表示为 $Q_{\omega}(s,a)$，我们将用于拟合函数 $Q$ 的神经网络称为 $Q$ 网络。

### 更新 Q 值

在 [[00_Inbox/强化学习/时序差分算法#Q-learning 算法|Q-learning算法]] 中，我们使用时序差分来学习目标 $r_t +\gamma\max_a Q(s_t,a)$ ：
$$
Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha[\,r_t+\gamma\max_a Q(s_{t+1}, a)-Q(s_t,a_t)\,]
$$
因此可以自然的将 $Q$ 网络的损失函数构造称均方误差的形式：
$$
\omega^*=\arg\min_\omega\frac{1}{2N}\sum_{i=1}^N\left[Q_\omega(s_i,a_i)-\left(r_i +\gamma\max_{a\in \mathcal A}Q(s'_i,a)\right)\right]^2
$$
于是我们将 Q-learning 算法拓展到了神经网络的形式——深度 Q 网络 (deep Q network, DQN)算法。由于 DQN 是离线算法，因此我们在收集数据时可以使用 $\epsilon$ 贪婪策略来平衡探索与利用，将收集到的数据保存起来，在后续训练中使用。

### 经验回放

在一般的有监督学习中，假设数据时独立同分布的，我们每次在数据中随机采用一个或者若干个用于梯度下降（随机梯度下降方法）。在 Q-learning 算法中，每个数据只用一次来更新 $Q$ 值。为了更好的将 Q-learning 算法与神经网络结合，DQN 采用经验回放（expercience replay）方法，维护一个回放缓冲区，将每次从环境中采样得到的数据储存在回放缓冲区中，在训练时再从回放缓冲区中随机采样若干数据进行梯度下降。这样做有下面两个好处：
1. 使样本满足独立性假设。在 MDP 中交互采样得到的数据本身不满足独立假设，因为这一时刻的状态与上一时刻的状态有关。非独立同分布的数据对训练神经网络的影响很大，会使网络拟合到最近训练的数据上。采用经验回放可以打破样本之间的关联性，使其满足独立性假设。
2. 提高样本效率。每个样本都可以使用多次，时候深度神经网络的梯度学习。

### 目标网络

DQN 算法的最终更新目标是让 $Q_{\omega}(s,a)$ 逼近 $r+\gamma \max_{a'} Q_\omega(s',a')$，由于 TD 误差目标本身就包含了神经网络的输出，因此在更新网络参数的同时目标也在不断改变，这非常容易造成神经网络训练的不稳定性。为了解决这个问题，DQN 使用了目标网络 (target network)的思想。

在 DQN 中，我们使用两套 Q 网络：
1. 原来的训练网络 $Q_{\omega}(s,a)$ 用于计算原来的损失函数 $\displaystyle\frac{1}{2}\left[Q_{\omega}(s,a)-\left(r+\gamma \max_{a'}Q_{\omega^-}(s',a')\right)\right]$ 中的 $Q_{\omega}(s,a)$ 项，使用正常的梯度下降方法。
2. 目标网络中的 $Q_{\omega^-}(s',a')$ 使用训练网络中较为旧的一套参数，并每隔 $C$ 步与训练网络同步一次

如果训练网络与目标网络中的参数时刻保持一致，则仍然为原来的不稳定的方法。每隔几步再进行同步可以使目标网络相对于训练网络更加稳定。

### DQN 伪代码

```python
# 初始化神经网络net
net = Network()
target_net = Network()
# 经验缓冲池
replay_buffer = ReplayBuffer()
for x in X:
	s = start_state
	done = False
	while not done:
		a = choice_action(s)
		s_next, reward, done = env.step(s, a)
		replay_buffer.add(s, a, reward, s_next, done)
		s = s_next
		if replay_buffer.size > minizial_size:
			s_array, a_array, r_array, s_next_array, done_array = replay_buffer.sample(batch_size)

			# 使用均方误差优化训练网络
			q_value = target_net(s_array).gather(a_array, 1)
			q_max = max(net(s_next_array), 1)
			q_target = r + gamma * q_max
			loss = mse_loss(q_value, q_target)

			optim.zero_gard()
			loss.backward()
			optim.step()
			count++
			
			if count % N == 0:
				target_net.w = net.w
```

## 算法评估

DQN 的主要思想是使用一个神经网络来表示最优策略的函数 Q，然后使用 Q-learning 的思想进行参数更新。为了保证训练的稳定性与高效性，算法引入了经验回放与目标网络两个模块，是算法在应用中取得了较好的效果。通过修改网络结构，DQN 算法甚至能够通过深度学习中的不同网络来学习不同的环境。

DQN 是深度强化学习的基础，它成功将神经网络引入了强化学习中。作为先驱，DQN 算法在许多方面还有不足，例如对 Q 值的预估总是偏高，因此在动作空间复杂的环境中难以应用。在 [[00_Inbox/强化学习/DQN改进算法]] 中我们将着手解决这个问题。