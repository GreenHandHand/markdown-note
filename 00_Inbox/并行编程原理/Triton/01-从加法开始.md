---
tags:
  - Triton
  - 并行编程
---

# 从加法开始

Triton 是一种用于并行编程的语言（DSL）和编译器。它的核心目标是**降低 GPU 高性能编程的门槛**。

- **定位**：介于 Python (PyTorch) 和 CUDA C++ 之间。
- **优势**：它允许我们用类似 Python 的语法编写 GPU Kernel，同时由编译器自动处理复杂的内存合并（Coalescing）和共享内存（Shared Memory）管理，从而在保持高性能的同时大幅提升开发效率。

> [!note] Triton 的设计目标
> Triton 提供了一种 Pythonic 的方式来编写 GPU kernel，通过自动处理内存加载/存储、线程调度等底层细节，让开发者聚焦于算法逻辑本身，而非硬件细节。

Triton 是一个 python 库，通过下面的命令安装。

```shell
pip install triton
```

Triton 官方文档中通过一系列算子的例子来介绍 Triton 的编写方式。

## 向量加法

在并行编程中，向量加法就是 `Hello World`。这个例子用向量加法来介绍了 Triton 的基本编程模型。先看代码：

```python
import torch

import triton
import triton.language as tl

DEVICE = triton.runtime.driver.active.get_active_torch_device()

@triton.jit
def add_kernel(x_ptr,  # *Pointer* to first input vector.
               y_ptr,  # *Pointer* to second input vector.
               output_ptr,  # *Pointer* to output vector.
               n_elements,  # Size of the vector.
               BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.
               # NOTE: `constexpr` so it can be used as a shape value.
               ):
    # There are multiple 'programs' processing different data. We identify which program
    # we are here:
    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.
    # This program will process inputs that are offset from the initial data.
    # For instance, if you had a vector of length 256 and block_size of 64, the programs
    # would each access the elements [0:64, 64:128, 128:192, 192:256].
    # Note that offsets is a list of pointers:
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    # Create a mask to guard memory operations against out-of-bounds accesses.
    mask = offsets < n_elements
    # Load x and y from DRAM, masking out any extra elements in case the input is not a
    # multiple of the block size.
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    # Write x + y back to DRAM.
    tl.store(output_ptr + offsets, output, mask=mask)
```

我们可以通过代码中的注释来学习 Triton 的编程模型。Triton 的编程模型是在 CUDA 基础上的简化，因此了解过 [[00_Inbox/并行编程原理/CUDA/CUDA基础|CUDA基础]] 的话很容易理解这里的操作。**Triton 的核心理念是基于分块的编程范式可以促进升级网络高性能计算核心的构建。**

| **特性**   | **CUDA (SIMT)**           | **Triton (Block-based)**              |
| -------- | ------------------------- | ------------------------------------- |
| **基本单元** | **Thread (标量)**           | **Block (向量/块)**                      |
| **索引计算** | 计算每个线程处理的**单点**索引 (`idx`) | 计算每个 Program 处理的**索引向量** (`offsets`)  |
| **控制流**  | 需要手动编写分支 (`if idx < n`)   | 使用 Mask 掩码处理边界 (`mask = offsets < n`) |
| **并行维度** | Grid -> Block -> Thread   | Grid -> Program (相当于 CUDA 的 Block)    |

- 在 CUDA 中，我们编写的代码是在**线程粒度**上的。简单来说，在 CUDA 上计算向量加法，我们需要找到每个线程中需要计算的元素，将他们对应相加后放回内存。
- 在 Triton 中，我们编写的代码是在**分块粒度**上的。简单来说，在 Triton 中，每个 Kernel 是对一块元素进行操作的（对一个微型向量进行加法），并在计算完成后把这个块整个放回内存。

> [!note] **Triton 的逻辑流：**
> 1. 定义一个 Grid（由多个 Program 组成）。
> 2. 每个 Program 也就是代码里的 `pid`，负责处理一大块数据（例如 1024 个元素）。
> 3. 在 Program 内部，我们不再写循环或管理单个线程，而是**直接操作这就代表 1024 个元素的张量**。

上面的代码计算与 CUDA 的向量加法例子如出一辙。同样的，计算 `idx = start + offset`，不同的是 Triton 中的 offset 是一整个块的元素。Triton 中也提供了一系列的算子，对这些小块进行操作。这些算法在 `triton.lanuage` 中被定义，在 Nvidia 机器上引用的是 CUDA 的 `libdevice`。

下面是上面的代码中一些其他要素的解释：

- `@triton.jit` 装饰器用于声明 `kernel function`,这些函数将会在 GPU 上运行。 
- 我们可以使用 `tl.load` 和 `tl.store` 两个方法在 GPU 与内存之间通信。

> [!tip] `triton.language.load`
> `tl.load` 函数的定义如下：
> ```python
> triton.language.load(
> 	pointer, # 可以是标量指针或指针张量（即 `pointer + offsets` 形式）。
> 	mask=None, # 控制哪些位置实际读取；未命中位置返回 `other`（默认为 0）。
> 	other=None, # 逻辑如下：return pointer[idx] if mask[idx] else other[idx]
> 	boundary_check=(),
> 	padding_option='',
> 	cache_modifier='',
> 	eviction_policy='',
> 	volatile=False,
> )
> ```
> 支持三种模式：
>   1. **标量加载**：`pointer` 为单个地址，`mask` 为标量。
>   2. **向量加载**（最常用）：`pointer` 为地址张量（如 `base + offsets`），自动加载多个元素。
>   3. **块指针加载**：通过 `tl.make_block_ptr` 构造，用于高级内存布局（如矩阵分块）。

> [!tip] `triton.language.store`
> `tl.store` 函数的定义如下：
> ```python
> triton.language.store(
> 	pointer,
> 	value, # 要存储的元素张量
> 	mask=None,
> 	boundary_check=(),
> 	cache_modifier='',
> 	eviction_policy='',
> )
> ```
> - 行为与 `load` 对称，但 **不支持 `other` 参数**。
> - 同样支持标量、向量和块指针三种模式。

为了像使用 PyTorch 原生操作一样调用 Triton kernel，我们通常封装一个辅助函数：

```python
def add(x: torch.Tensor, y: torch.Tensor):
    # We need to preallocate the output.
    output = torch.empty_like(x)
    assert x.device == DEVICE and y.device == DEVICE and output.device == DEVICE
    n_elements = output.numel()
    # The SPMD launch grid denotes the number of kernel instances that run in parallel.
    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].
    # In this case, we use a 1D grid where the size is the number of blocks:
    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )
    # NOTE:
    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.
    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.
    #  - Don't forget to pass meta-parameters as keywords arguments.
    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)
    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still
    # running asynchronously at this point.
    return output
```

这样我们就可以像 pytorch 内置的函数一样的方式计算。

> [!Note] 为什么 `BLOCK_SIZE` 是 `meta-parameter`? 
> 在 Kernel 定义中，`BLOCK_SIZE` 被标记为 `tl.constexpr`。这意味着它不是普通的变量，而是一个编译时常量。Triton 编译器会针对这个特定的块大小对代码进行极致优化。

> [!note] 执行模型对比
> 
> | 概念         | CUDA               | Triton                     |
> |--------------|--------------------|----------------------------|
> | 并行单元     | Thread             | Program（隐含多个线程）    |
> | 编程粒度     | 线程级             | 块级（block-level）        |
> | 内存访问     | `__global__` 指针  | `tl.load` / `tl.store`     |
> | 启动配置     | `<<<grid, block>>>`| `kernel[grid](...)`        |
> 
> Triton 并未改变 GPU 的底层执行模型（仍是 grid → block → warp → thread），而是**将线程内的操作向量化并自动化**。

## 性能测试

Triton 专为高性能计算而设计，尤其适用于构建 AI 框架中的高效自定义算子。为了方便评估和比较 kernel 的性能，Triton 内置了完善的基准测试（benchmarking）工具。

```python
@triton.testing.perf_report(
    triton.testing.Benchmark(
        x_names=['size'],  # Argument names to use as an x-axis for the plot.
        x_vals=[2**i for i in range(12, 28, 1)],  # Different possible values for `x_name`.
        x_log=True,  # x axis is logarithmic.
        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.
        line_vals=['triton', 'torch'],  # Possible values for `line_arg`.
        line_names=['Triton', 'Torch'],  # Label name for the lines.
        styles=[('blue', '-'), ('green', '-')],  # Line styles.
        ylabel='GB/s',  # Label name for the y-axis.
        plot_name='vector-add-performance',  # Name for the plot. Used also as a file name for saving the plot.
        args={},  # Values for function arguments not in `x_names` and `y_name`.
    ))
def benchmark(size, provider):
    x = torch.rand(size, device=DEVICE, dtype=torch.float32)
    y = torch.rand(size, device=DEVICE, dtype=torch.float32)
    quantiles = [0.5, 0.2, 0.8]
    if provider == 'torch':
        ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y, quantiles=quantiles)
    if provider == 'triton':
        ms, min_ms, max_ms = triton.testing.do_bench(lambda: add(x, y), quantiles=quantiles)
    gbps = lambda ms: 3 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)
    return gbps(ms), gbps(max_ms), gbps(min_ms)
```

上述代码展示了一个典型的 Triton benchmark 模板，只需稍作修改即可复用于其他算子。它通过 @triton.testing.perf_report 装饰器自动运行不同输入规模下的性能测试，并生成带可视化图表的报告。

从实际结果来看，在向量加法这类内存带宽受限的操作中，Triton 实现的性能**几乎与高度优化的 PyTorch 原生操作持平**。这表明 Triton 不仅大幅简化了 GPU 编程，还能在保持代码简洁的同时**达到接近理论最优的硬件利用率**——在某些场景下，甚至优于手写的低效 CUDA 代码。

---

| [[00_Inbox/并行编程原理/Triton/02-FusedSoftmax|02-FusedSoftmax]] >