---
tags:
  - 机器学习
  - 监督学习
  - 无监督学习
---

# 隐马尔可夫模型

隐马尔可夫模型是用于标注问题的统计学习模型，描述由隐马尔可夫链随机生成观测序列的过程，属于生成模型。隐马尔可夫模型在语音识别，自然语言处理，生物信息，模式识别等领域有着广泛的应用。

## 隐马尔可夫模型的基本概念

隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测从而产生随机观测序列的过程。

在隐马尔可夫模型中，隐藏的马尔可夫链随机生成的状态的序列称为状态序列 (state sequence)，每个状态生成一个观测，由此产生的观测的随机序列称为观测序列 (observation sequence)。序列的每个位置可以看作一个时刻。下面给出隐马尔可夫模型的形式定义：

首先声明一些符号：
- 设 $Q$ 是所有可能的状态集合，$V$ 是所有可能的观测集合，即
$$
Q=\{q_1,\cdots,q_N\}\quad V=\{v_1,\dots,v_M\}
$$
其中 $N$ 是可能的状态数，$M$ 是可能的观测数。
- 设 $I$ 是长度为 $T$ 的状态序列，$O$ 是对应的观测序列，即
$$
I=(i_1,\cdots,i_T),\quad O=(o_1,\cdots,o_N)
$$
-  $A$ 是状态转移概率矩阵，即
$$
A=[a_{ij}]_{N\times N}
$$
其中,
$$
a_{ij}=P(i_{t+1}=q_j|i_{t}=q_i)
$$
是在时刻 $t$ 处于状态 $q_i$ 的条件下在时刻 $t+1$ 转移到状态 $q_j$ 的概率。
-  $B$ 是观测概率矩阵，即
$$
B=[b_j(k)]_{N\times M}
$$
其中，
$$
b_j(k)=P(o_t=v_k|i_t=q_j)
$$
是时刻 $t$ 处于状态 $q_j$ 的条件下生成观测值 $v_k$ 的概率。
-  $\pi$ 是初始状态概率向量，即
$$
\pi=(\pi_i)
$$
其中,
$$
\pi_i=P(i_1=q_i)
$$
是时刻 $t=1$ 时处于状态 $q_i$ 的概率。

一个隐马尔可夫模型由初始状态概率向量 $\pi$，状态转移概率矩阵 $A$ 和观测概率矩阵 $B$ 决定，其中 $\pi$ 和 $A$ 决定状态序列，$B$ 决定观测序列。因此隐马尔可夫模型 $\lambda$ 可以使用三元组表示为
$$
\lambda=(A,B,\pi)
$$
上面的三个称为隐马尔可夫模型的三要素。综合起来，也可以将隐马尔科夫模型记为
$$
\lambda=(N,M,A,B,\pi)
$$
其中：
- $N$ 是可能的状态数
- $M$ 是可能的观测数
- $A$ 是状态转移概率矩阵
- $B$ 是观测概率矩阵
- $\pi$ 是初始状态概率向量

从定义上看，隐马尔可夫模型做了以下两个基本假设：
1. 其次马尔可夫假设：假设隐藏的马尔可夫链在任意时刻 $t$ 的状态都只依赖与其前一时刻的状态，与其他时刻的状态或观测无关，与时刻 $t$ 也无关，即
   $$
P(i_t|i_{t-1},o_{t-1},\cdots,i_1,o_1)=P(i_t|i_{t-1})
$$
2. 观测独立假设：假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测和状态无关
$$
P(o_t|i_T,o_T,\cdots,i_{t},o_{t},\cdots,i_1,o_1)=P(o_t|i_t)
$$

## 观测序列的生成

根据隐马尔可夫模型的定义，可以将一个长度为 $T$ 的观测序列的生成过程描述如下：
1. 按照初始状态分布 $\pi$ 产生状态 $i_1$
2. 令 $t=1$
3. 按照状态 $i_t$ 的观测概率分布 $b_{i_t}(k)$ 生成 $o_t$
4. 按照状态 $i_t$ 的状态转移分布 $\{a_{i_t,i_{t+1}}\}$ 产生装填 $t_{i+1}$
5. $t=t+1$，如果 $t<T$，回到 3，否则终止

## 隐马尔可夫模型的学习

使用隐马尔可夫模型有三个基本问题：
1. 概率计算问题：给定模型 $\lambda=(A,B,\pi)$ 和观测序列 $O$，计算在模型 $\lambda$ 下观测序列 $O$ 出现的概率 $P(O|\lambda)$
2. 预测问题，也称为解码 (decoding)，寻找状态序列：已知模型 $\lambda=(A,B,\pi)$ 和观测序列 $O$，求对给定观测序列条件概率 $P(I|O)$ 最大的状态序列 $I=(i_1,\cdots,i_T)$。即给定观测序列，求最有可能的对应的状态序列。
3. 学习问题：已知观测序列 $O$，估计模型 $\lambda=(A,B,\pi)$ 的参数，使得在该模型下观测序列概率 $P(O|\lambda)$ 最大，即使用极大似然估计的方法估计参数。

### 概率计算问题

这里介绍两个用于计算观测序列概率 $P(O|\lambda)$ 的前向 (forward) 与后向 (backward) 算法。

#### 直接计算

这里先介绍理论上可行的直接计算法。给定模型 $\lambda=(A,B,\pi)$ 和观测序列 $O=(o_1,\cdots,o_T)$，计算观测序列 $O$ 出现的概率 $P(O|\lambda)$。最直接的方式是按概率公式进行计算，通过枚举所有可能的长度为 $T$ 的状态序列 $I$，求各个状态序列 $I$ 与观测序列 $O$ 的联合概率 $P(O,I|\lambda)$，然后对所有可能的状态序列求和，得到 $P(O|\lambda)$。

具体的，状态序列 $I$ 的概率为
$$
P(I|\lambda)=\pi_{i_1}a_{i_1,i_2}a_{i_2,i_3}\cdots a_{i_r,i_{r-1}}
$$
对固定的状态序列 $I$，观测序列 $O$ 的概率为
$$
P(O|I,\lambda)=b_{i_1}(o_1)b_{i_2}(o_2)\cdots b_{iT}(o_T)
$$
$O$ 和 $I$ 同时出现联合概率为
$$
\begin{aligned}
P(O,I|\lambda)&=P(O|I,\lambda)P(I|\lambda)\\
&=\pi_{i1}b_{i1}(o_1)a_{i_1,i_2}b_{i_2}(o_2)\cdots a_{i_{T-1},i_T}b_{iT}(o_T)
\end{aligned}
$$
然后对所有可能的状态序列进行求和可以得到观测序列 $O$ 的概率为
$$
\begin{aligned}
P(O|\lambda)&=\sum_{I}P(O|I,\lambda)P(I|\lambda)
\\&=\sum_{i_1,i_2,\cdots,i_T}\pi_{i_1}b_{i_1}(o_1)a_{i_1,i_2}b_{i_2}(o_2)\cdots a_{i_{T-1}i_T}b_{iT}(o_T)
\end{aligned}
$$
使用上式即可计算 $P(O|\lambda)$，但是该计算方式的时间复杂度为 $O(TN^T)$ 阶，这种算法是不可行的。

#### 前向算法

首先定义前向概率：给定隐马尔可夫模型 $\lambda$，定义到时刻 $t$ 部分观测序列为 $o_1,o_2,\cdots,o_t$ 且状态为 $q_i$ 的概率为前向概率，记为
$$
\alpha_{t}(i)=P(o_1,o_2,\cdots,o_t,i_t=q_t|\lambda)
$$
可以递推的求得前向概率及观测序列概率 $P(O|\lambda)$。前向算法描述如下：
1. 初始值：
$$
\alpha_1(i)=\pi_ib_i(o_1)
$$
2. 递推，对于 $t=1,2,\cdots,T-1$
$$
\alpha_{t+1}(i)=\left[\sum_{j=1}^N\alpha_t(j)\alpha_{ji}\right]b_t(o_{t+1})\quad i=1,\cdots,N
$$
3. 终止
$$
P(O|\lambda)=\sum_{i=1}^N\alpha_T(i)
$$

前向算法实质上式通过状态序列的路径结构递推计算 $P(O|\lambda)$ 的算法，是一种动态规划算法，前向算法的计算量是 $O(N^2T)$。

#### 后向算法

首先定义后向概率：给定隐马尔可夫模型，定义在时刻 $t$ 状态为 $q_t$ 的条件下，从 $t+1$ 到 $T$ 的部分观测序列为 $o_{t+1},o_{t+2},\cdots,o_T$ 的概率为后向概率，即
$$
\beta_t(i)=P(o_{t+1},o_{t+2},\cdots,o_T|i_t=q_i,\lambda)
$$
可以递推的求得后向概率及观测序列 $P(O|\lambda)$。后向算法如下：
1. 初始值：
$$
\beta_T(i)=1\quad i=1,2,\cdots,N
$$
2. 对 $t=T-1,T-2,\cdots,1$
$$
\beta_t(i)=\sum_{j=1}^Na_{ij}b_j(o_{t+1})\beta_{t+1}(j),\quad i=1,2,\cdots,N
$$
3. 终止
$$
P(O|\lambda)=\sum_{i=1}^N\pi_ib_i(o_1)\beta_1(i)
$$

后向算法的思路与前向算法相同，区别在与后向算法通过反向对后向概率进行递推，利用前向概率与后向概率的定义可以将观测序列概率 $P(O|\lambda)$ 统一写成
$$
P(O|\lambda)=\sum_{i=1}^N\sum_{j=1}^N\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)
$$

#### 一些概率与期望的计算

利用前向概率与后向概率，我们可以计算单个状态和两个两个状态的概率
1. 给定模型 $\lambda$ 和观测 $O$，在时刻 $t$ 处于状态 $q_i$ 的概率，即
$$
\gamma_t(i)=P(i_t=q_i|O,\lambda)
$$
可以通过前向后向概率进行计算，即
$$
\begin{aligned}
\gamma_t(i)&=\frac{P(i_t=q_i,O|\lambda)}{P(O|\lambda)}\\
&=\frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N\alpha_t(j)\beta_t(j)}
\end{aligned}
$$
2. 给定模型 $\lambda$ 和观测 $O$，在时刻 $t$ 处于状态 $q_i$ 且在时刻 $t+1$ 处于状态 $q_j$ 的概率可以记为
$$
\begin{aligned}
\xi_t(i,j)&=P(i_t=q_i,i_{t+1}=q_j|O,\lambda)\\
&=\frac{P(i_t=q_i,i_{t+1}=q_j,O|\lambda)}{P(O|\lambda)}\\
&=\frac{\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^N\sum_{j=1}^N\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}
\end{aligned}
$$

使用这两个概率可以在学习算法时使计算更加清晰。


### 学习计算

隐马尔可夫模型的学习，根据训练数据时包括观测数据和对应的状态序列还是只有观测序列，可以分布由监督学习与无监督学习实现，这里首先介绍监督学习算法，然后介绍无监督学习算Baum-Welch 算法 (EM 算法)。

#### 监督学习算法

假设已给训练数据集包含 $S$ 个长度相同的观测序列和对应的状态序列 $\{(O_1,I_1),\cdots,(O_S,I_S)\}$，那么可以使用极大似然估计法来估计隐马尔可夫模型的参数，具体方法如下：
1. 估计状态转移概率 $a_{ij}$
设样本中时刻 $t$ 状态为 $i$ 时刻 $t+1$ 状态为 $j$ 的频数为 $A_{ij}$ 可以得到状态转移概率的估计为
$$
\hat a_{ij}=\frac{A_{ij}}{\sum_{j=1}^NA_{ij}}\quad i=1,2,\cdots,N,j=1,2,\cdots,N
$$
2. 估计观测样本概率 $b_j(k)$
设样本中状态为 $j$ 并观测为 $k$ 的频数为 $B_{jk}$，那么状态为 $j$ 观测为 $k$ 的概率的估计为
$$
\hat b_j(k)=\frac{B_{jk}}{\sum_{k=1}^M B_{jk}},j=1,2,\cdots,N,k=1,2,\cdots,M
$$
3. 估计初始状态概率
初始状态概率 $\pi_i$ 的估计为 $S$ 个样本中初始状态为 $q_i$ 的频率
$$
\hat \pi_i=\frac{n(q_i)}{S}
$$

由于监督学习使用标注的数据，而人工标注数据往往代价很高，有时就会利用无监督学习的方法。

#### Baum-Welch 算法

假定设定训练数据只包括 $S$ 个长度为 $T$ 的观测序列 $\{O_1,\cdots,O_S\}$ 而没有对应的状态序列，目标是学习隐马尔可夫模型的参数，可以将状态序列看作不可观测的隐变量 $I$，那么实际上隐马尔可夫模型的学习实际上是一个含有隐变量的概率模型的学习，因此可以使用 [[00-笔记/机器学习/EM 算法]]实现。

下面对模型的 EM 算法进行简单的推导：
1. 确定隐变量
   将状态序列数据视为隐变量，即 $Z=I$
2. 初始化参数 $\lambda^{(0)}=(A^{(0)},B^{(0)},\pi^{(0)})$
3. EM 算法中的 E 步：
$$
\begin{aligned}
Q(I)&=P(I|O,\lambda)\\
&=\frac{P(O,I|\lambda)}{P(O|\lambda)}
\end{aligned}
$$
其中 $P(O|\lambda)$ 可以通过前向后向算法得到，$P(O,I|\lambda)$ 可以使用下式
$$
P(O,I|\lambda)=\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)\cdots a_{i_{r-1}i_r}b_{ir}(o_r)
$$
4. EM 算法中的 M 步
$$
\begin{aligned}
\lambda^{(n+1)}&=\arg\max_\lambda \sum_i\sum_I Q(I_i)\log\frac{P(O_i,I_i|\lambda^{(n)})}{Q(I_i)}
\end{aligned}
$$
对上式进行偏导置零可以计算对应的参数，下面提出 Baum-Welch 算法，即一种更加简洁的 EM 算法形式。

在 EM 算法的 M 步中，我们需要极大化 $Q$ 函数来求参数，可以写出完全数据的对数似然函数：
$$
\begin{aligned}
\log P(O,I|\lambda)&=\log \pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)\cdots a_{i_{T-1}i_T}b_{iT}(o_T)\\
&=\sum_{t=1}^{T-1}\log a_{i_ti_{t+1}} + \sum_{t=1}^{T}\log b_{i_t}(o_t)+\log\pi_{i_1}
\end{aligned}
$$
于是可以计算 $Q$ 函数为
$$
\begin{aligned}
Q(\lambda,\hat\lambda)&=\sum_IP(I|O,\hat\lambda)P(O|\hat\lambda)\log P(O,I|\lambda)\\
&=\sum_IP(O,I|\hat\lambda)\log P(O,I|\lambda)\\
&=\sum_I\sum_{t=1}^{T-1}\log a_{i_ti_{t+1}}P(O,I|\hat\lambda) + \sum_I\sum_{t=1}^{T}\log b_{i_t}(o_t)P(O,I|\hat\lambda)+\\&\,\,\quad\sum_I\log\pi_{i_1}P(O,I|\hat\lambda)
\end{aligned}
$$
因为
$$
\sum_I\log\pi_{i_1}P(O,I|\hat\lambda)=\sum_{i=1}^N\log\pi_iP(O,i_1=i|\hat\lambda)
$$
注意到 $\sum\pi_i=1$，所以利用拉格朗日乘子法：
$$
\frac{\partial}{\partial \pi_i}\left[\sum_{i=1}^N\log\pi_iP(O,i_1=i|\hat\lambda)+\gamma\left(\sum_{i=1}^N\pi_i-1\right)\right]=0
$$
于是解得
$$
\gamma=-P(O|\hat\lambda)
$$
带入上式得到
$$
\pi_i=\frac{P(O,i_t=i|\hat\lambda)}{P(O|\lambda)}
$$
同理，因为
$$
\sum_I\sum_{t=1}^{T-1}\log a_{i_ti_{t+1}}P(O,I|\hat\lambda)=\sum_{i=1}^N\sum_{j=1}^N\sum_{t=1}^{T-1}\log a_{ij}P(O,i_t=i,i_{t+1}=j|\hat\lambda)
$$
注意到 $\sum_{j=1}^N a_{ij}=1$，使用拉格朗日乘子法有
$$
a_{ij}=\frac{\sum_{t=1}^{T-1}P(O,i_t=i,i_{t+1}=j|\hat\lambda)}{\sum_{t=1}^{T-1}P(O,i_t=i|\hat\lambda)}
$$
同理可以求得
$$
\sum_I\sum_{t=1}^{T}\log b_{i_t}(o_t)P(O,I|\hat\lambda)=\sum_{j=1}^N\sum_{t=1}^T\log b_j(o_t)P(O,i_t=j|\hat\lambda)
$$
注意到 $\sum_{k=1}^Mb_j(k)=1$，特别的，只有在 $o_t=v_k$ 时 $b_j(o_t)$ 对 $b_j(k)$ 的偏导才不为 0，因此使用 $I(o_t=v_k)$ 表示
$$
b_j(k)=\frac{\sum_{i=1}^TP(O,i_t=j|\hat\lambda)I(o_t=v_k)}{\sum_{t=1}^TP(O,i_t=j|\hat\lambda)}
$$

综上，我们可以写出参数估计公式为
$$
\begin{aligned}
a_{ij}&=\frac{\sum_{t=1}^{T-1}\xi_t(i,j)}{\sum_{t=1}^{T-1}\gamma_t(i)}\\
b_j&=\frac{\sum_{t=1,o_t=v_k}^T\gamma_t(j)}{\sum_{t=1}^T\gamma_t(j)}\\
\pi_i&=\gamma_1(i)
\end{aligned}
$$

Baum-Welch 算法总结如下：
1. 初始化模型参数 $\lambda^{(0)}=(A^{(0)},B^{(0)},\pi^{(0)})$
2. 递推：使用上次的值递推计算下一次的值
$$
\begin{aligned}
a_{ij}^{(n+1)}&=\frac{\sum_{t=1}^{T-1}\xi_t(i,j)}{\sum_{t=1}^{T-1}\gamma_t(i)}\\
b_j^{(n+1)}&=\frac{\sum_{t=1,o_t=v_k}^T\gamma_t(j)}{\sum_{t=1}^T\gamma_t(j)}\\
\pi_i^{(n+1)}&=\gamma_1(i)
\end{aligned}
$$
3. 终止，得到算法参数

### 预测算法

下面介绍两种隐马尔可夫模型预测的算法，分别是近似算法与维特比算法 (Viterbi alogrithm)。

#### 近似算法

近似算法的想法是在每个时刻 $t$ 选择在该时刻最有可能出现的状态 $i^*_t$，从而得到一个状态序列 $I^*$，将它作为预测的结果。

给定隐马尔可夫模型 $\lambda$ 和观测序列 $O$，在时刻 $t$ 处于状态 $q_i$ 的概率 $\gamma_t(i)$ 是
$$
\gamma_t(i)=\frac{\alpha_t(i)\beta_t(i)}{P(O|\lambda)}=\frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N\alpha_t(j)\beta_t(j)}
$$
在每一时刻 $t$ 最有可能的状态为
$$
i^*_t=\arg\max_{1\leqslant i\leqslant N}\gamma_t(i)
$$
从而得到状态序列 $I^*$。近似算法的优点是计算简单，但是不能保证预测状态序列整体是最有可能的状态序列，因为预测状态序列可能有实际不发生的部分。事实上，上述方法得到的状态序列可能会存在转移概率为 0 的相邻状态，尽管如此，近似算法仍然是由用的。

#### 维特比算法

维特比算法实际上是动态规划解隐马尔可夫模型预测问题，即使用动态规划求概率最大的路径。这时一条路径对应这一个状态序列。下面直接给出维特比算法：
1. 初始化
   $$
   \begin{aligned}
\delta_1(i)&=\pi_ib_i(o_1),\quad &i=1,2,\cdots,N\\
\varPsi_1(i)&=0,\quad &i=1,2,\cdots,N
\end{aligned}
$$
2. 递推。对 $t=2,3,\cdots,T$ 有
   $$
   \begin{aligned}
\delta_t(i)&=\max_{1\leqslant j\leqslant N}[\delta_{t-1}(j)a_{ji}]b_i(o_t), &i=1,2,\cdots,N\\
\varPsi_t(i)&=\arg\max_{1\leqslant j\leqslant N}[\delta_{t-1}(j)a_{ji}],&i=1,2,\cdots,N
\end{aligned}
$$
3. 终止
   $$
   \begin{aligned}
P^*&=\max_{1\leqslant i\leqslant N} \delta_T(i)\\
i^*_T&=\arg\max_{1\leqslant i\leqslant N}[\delta_T(i)]
\end{aligned}
$$
4. 最优路径回溯。对 $t=T-1,T-2,\cdots,1$ 有
   $$
i^*_t=\varPsi_{t+1}(i^*_{t+1})
$$
