---
aliases:
  - spectral clustering
tags:
  - 机器学习
  - 聚类
  - 无监督学习
---

# 谱聚类

谱[[00_Inbox/机器学习/聚类|聚类]]是一种无监督学习算法，主要用于将数据集中的样本分配到不同的聚类中心中，以便于对数据进行分析和处理。谱聚类的主要思想是通过将数据表示成一个图，并对图进行谱分解，得到特征向量，进而对数据进行聚类。与 k-means 方法不同，谱聚类可以处理非凸的类别。

## 问题抽象

由于谱聚类通过图的思想对数据进行划分，然后根据图划分的理论进行聚类。因此首先要做的是将数据抽象为图结构。设数据集为 $X=\{x_1,x_2,\cdots,x_N\}$，图定义为 $G=(V,E)$，我们可以将每个数据作为图的节点，即 $V=\{x_1,x_2,\cdots,x_N\}$ 为数据集，下面对边的构建进行讨论。

### 全连接图

一种简单的方式是将节点构建为全连接图，即使用距离度量或者相似度 (核函数) 作为权重，将每个节点都连接起来，这样的好处是不会省略数据的信息，但是处理数据的时间复杂度会大大提高。可以使用邻接矩阵表示这样的全连接图：
$$
E=\begin{pmatrix}d_{11}&\cdots&d_{1N}\\\vdots&&\vdots\\d_{N1}&\cdots &d_{NN}\end{pmatrix}
$$

常使用高斯核函数作为度量，即
$$
d_{ij}=\exp(-\frac{\Vert x_i-x_j\Vert_2^2}{2\sigma^2})
$$
根据对称性我们可以得到邻接矩阵为一个对称矩阵，即我们得到的图为无向图。

### k 近邻法

在全连接图上进行计算往往有着非常可怕的时间复杂度，因此我们需要一些方法来减少图的复杂性，一种简单的方式是 [[00_Inbox/机器学习/k 近邻|k 近邻]]法，即只将每个数据最近的 k 个数据与其相连。以高斯核函数为例，我们可以得到 k 近邻方法构建的邻接矩阵如下：
$$
d_{ij}=\begin{cases}\exp(-\dfrac{\Vert x_i-x_j\Vert_2^2}{2\sigma^2})&x_j\in\text{KNN}(x_i)\\0&x_j\notin \text{KNN}(x_i)\end{cases}
$$
这样做有一个缺陷，即可能得到的邻接矩阵不是对称矩阵，即得到一个有向图。有向图的性质相比于无向图要复杂很多，因此我们更加希望得到的是一个无向图，下面有几种处理方式可以将其转边为无向图：
1. 只要其中一个节点是另一个节点的邻居，就连接他们
2. 只有两个节点互相是对方的邻居时才连接他们
3. 求平均，即
   $$
W'=\frac{W+W^T}{2}
$$

### ε-近邻法

给定一个阈值 $\varepsilon$，只有当距离小于阈值 $\varepsilon$ 或者相似度大于阈值 $\varepsilon$ 时才将其相连，即
$$
d_{ij}=\begin{cases}\varepsilon&d(x_i,x_j)\geqslant \varepsilon\\0&d(x_1,x_j)<\varepsilon\end{cases}
$$
这样可以得到一个无向无权图，结构简单但是精度不如其他方法。

## 目标函数

现在考虑我们的目标函数，在聚类中，我们总是希望相同类中的数据距离最小，不同类间数据的距离最大，可以表示为
$$
\min\frac{\text{\small 簇间相似度}}{\text{\small 簇内相似度}}
$$

当数据表示为图时，定义每个聚类结果为数据集的一个划分，即
$$
\begin{aligned}
\text{cut}(V)&=\{A_1,A_2,\cdots,A_k\}\\\bigcup_{i=1}^kA_i&=V,\quad\bigcap_{i=1}^kA_i=\varnothing
\end{aligned}
$$
定义每个簇间的距离为簇 $A_i$ 与簇 $A_j$ 间的边的权重和，即
$$
W(A_i,A_j)=\sum_{i\in A_i,j\in A_j}w_{ij}
$$
于是簇间相似度可以描述为每个簇同其他所有簇的距离之和，即：
$$
\small\text{\small簇间相似度}=\frac{1}{2}\sum_{k=1}^K W(A_k,\overline A_k)=\frac{1}{2}\sum_{k=1}^KW(A_k,V)-W(A_k,A_k)
$$
上式可以作为损失函数使用，但是在实际运用当中，上式对于数据较多的类存在偏向性。有两种方式可以修正，一种是除以簇的势 $|A_i|$，或者除以簇的体积 $\text{vol}(A_k)=\sum_{i\in A_k}d_i$，其中 $d_i$ 定义为：
$$
d_i=\sum_{j=1}^Nw_{ij}
$$
为节点的度数。簇的体积即该簇内所有的节点的度之和。我们可以发现簇的体积优于簇的势，因为簇的体积中包含了所有权重的信息。于是我们的优化目标可以表示为
$$
\begin{aligned}
\{A_1,\cdots,A_k\}&=\arg\min_{\{A_k\}_{k=1}^K}\sum_{k=1}^K\frac{W(A_k,\overline A_k)}{\sum_{i\in A_k}d_i}
\\&=\arg\min\sum_{k=1}^K\frac{W(A_k,V)-W(A_k,A_k)}{\sum_{i\in A_k}\sum_{j=1}^Nw_{ij}}
\end{aligned}
$$

其中分子为簇间相似度，分母为簇内相似度。下面将上式转换为矩阵形式，将目标分类转换为 one-hot 编码，即
$$
Y=\begin{pmatrix}y_1^T\\y_2^T\\\vdots\\y_N^T\end{pmatrix}
$$
其中 $y_n=(0,\cdots,1,\cdots,0)^T_{1\times k}$ 表示该数据分类为第 n 类。于是可以得到
$$
\small\begin{aligned}
Y^*&=\arg\min_Y\sum_{k=1}^K\frac{W(A_k,\overline A_k)}{\sum_{i\in A_k}d_i}\\
&=\arg\min_Y\text{tr}\begin{pmatrix}W(A_1,\overline A_1)\\&W(A_2,\overline A_2)\\&&\ddots\\&&&W(A_k,\overline A_k)\end{pmatrix}\cdot\\
&\begin{pmatrix}\sum_{i\in A_1}d_i\\&\sum_{i\in A_2 }d_i\\&&\ddots\\&&&\sum_{i\in A_k}d_i\end{pmatrix}^{-1}=\arg\min_YOP^{-1}
\end{aligned}
$$
由于
$$
\begin{aligned}
Y^{T}Y=\sum_{i=1}^Ny_iy_i^T&=\begin{pmatrix}N_1&\cdots&&0\\\vdots&N_2&&\vdots\\&&\ddots\\0&&&N_k\end{pmatrix}\\&=\begin{pmatrix}\sum_{i\in A_1}1&\cdots&&0\\\vdots&\sum_{i\in A_2}1&&\vdots\\&&\ddots\\0&&&\sum_{i\in A_k}1\end{pmatrix}
\end{aligned}
$$
所以有
$$
P=Y^{T}DY
$$
其中
$$
\small D=\begin{pmatrix}d_1\\&d_2\\&&\ddots\\&&&d_N\end{pmatrix}=\text{diag}(W\cdot\begin{pmatrix}1\\\vdots\\1\end{pmatrix})=\text{diag}(W\cdot1_N)
$$
而根据 $W(A_k,\overline A_k)=W(A_k,V)-W(A_k,A_k)$ 使用同样的推导方法可以得到
$$
\text{tr} O=\text{tr}(Y^TDY-Y^TWY)=\text{tr}(Y^TLY)
$$
其中 $L$ 称为标准化的拉普拉斯矩阵 (Laplacian matrix)，定义为 $L=D-W$，其中 $D$ 是对角线为每个数据的度的对角矩阵，$W$ 为图的邻接矩阵。综上，优化目标转变为了
$$
\begin{aligned}
Y^*&=\arg\min_Y\text{tr}OP^{-1}\\
&=\arg\min_Y\text{tr}(Y^TLY)\cdot(Y^TDY)^{-1}
\end{aligned}
$$
其相当于求解