---
aliases:
  - 主成分分析
  - PCA
  - principal component analysis
tags:
  - 机器学习
  - 无监督学习
  - 降维
---

# 主成分分析 (PCA)

主成分分析 (principal component analysis, PCA) 是一种常用的无监督学习方法，这一方法利用正交变换把由线性相关变量表示的观测数据转换为少数几个由线性无关变量表示的数据，线性无关的变量称为主成分。由于主成分的个数一般小于原始变量的个数，因此主成分分析属于降维方法。

主成分分析利用正交变换把由线性相关变量表示的观测数据转换为少数几个由线性无关变量表示的数据，线性无关的变量称为主成分。由于主成分的个数一般小于原始变量的个数，因此主成分分析属于降维方法。

主成分分析主要用于发现数据中的基本结构，即数据中变量间的关系，是数据分析有力的工具，也用于其他机器学习方法的前处理。主成分分析是多元统计分析的经典方法。

## 总体主成分分析

统计分析中，数据变量间可能存在一些相关性，以至于增加了分析的难度，因此，考虑由少数不相关的变量代替相关的变量，用来表示数据，并要求保留数据中的绝大部分信息。

在主成分分析中，首先对给定数据进行规范化，使得数据每一变量的均值为 0，方差为 1。之后对数据进行正交变换，原来由线性相关变量表示的数据通过正交变换分解为了若干个线性无关的新变量表示的数据。新变量可能是正交变换中变量的方差最大（信息保存最多）的。将新变量依次称为第一主成分，第二主成分等。这就是主成分分析的基本思想。

通过主成分分析，可以利用主成分近似的表示原始数据，可以理解为发现数据的基本结构，也可以把数据用少数主成分表示，理解为将数据降维。

### 直观理解

数据集合中的样本由实数空间（正交坐标系）中的点表示，空间的一个坐标轴表示一个变量，规范化处理后得到的数据分布在原点附近。对原坐标中的点进行坐标变化，将数据投影到新的坐标轴上，新坐标系的第一坐标轴、第二坐标轴就分别表示第一主成分、第二主成分。数据在每一轴上的坐标值的平方表示相应变量的方差，并且这个方差在所有的可能的坐标系中是最大的。
![[Assets/Pasted image 20230817214638.png]]

如果只去第一主成分，就相当于将坐标压缩到了一个轴上，且这个轴表示数据保留了最多的信息。下面是方差最大的解释。对于下面的图
![[Assets/Pasted image 20230817214901.png]]

### 定义

假设 $x=(x_1,x_2,\cdots,x_m)^T$ 是 $m$ 维 [[02_Areas/概率论与数理统计/一维随机变量及其分布|随机变量]]，其均值向量为 $\vec\mu$，于是有
$$
\vec\mu=E(x)=(\mu_1,\cdots,\mu_m)^T
$$
协方差矩阵 $\varSigma$ 为
$$
\varSigma=\mathrm{cov}(x,x)=E[(x-\vec\mu)(x-\vec\mu)^T]
$$
考虑到由 $m$ 维随机变量 $x$ 到 $m$ 维随机变量 $y=(y_1,y_2,\cdots,y_m)^T$ 的线性变换
$$
y_i=\vec\alpha_i^Tx=\alpha_{1i}x_1+\alpha_{2i}x_2+\cdots+\alpha_{mi}x_m
$$
其中 $\alpha^T_i=(\alpha_{1i},\alpha_{2i},\cdots,\alpha_{mi}),i=1,2,\cdots,m$。

由随机变量的性质可以得到
$$
\begin{aligned}
&E(y_i)=\vec\alpha_i^T\mu\quad i=1,2,\cdots,m\\[2mm]
&\mathrm{var}(y_i)=\vec\alpha_i^T\varSigma\vec\alpha_i\quad i=1,2,\cdots,m\\[2mm]
&\mathrm{cov}(y_i,y_j)=\vec\alpha_i^T\varSigma\vec\alpha_j\quad i=1,2,\cdots,m;\,j=1,2,\cdots,m
\end{aligned}
$$

下面给出总体主成分分析的定义：给定一个线性变换，如果它满足下面的条件：
1. 系数向量 $\vec\alpha_i^T$ 是单位向量，即 $\vec\alpha_i^T\vec\alpha_i=1,i=1,2,\cdots,m$
2. 变量 $y_i$ 与变量 $y_j$ 互不相关，即 $\mathrm{cov}(y_i,y_j)=0(i\ne j)$
3. 变量 $y_i$ 是 $x$ 的所有线性变换中方差最大的；$y_2$ 是与 $y_1$ 不相关的 $x$ 的所有线性变换中方差最大的；一般的，$y_i$ 是与 $y_{i-1},\cdots,y_1$ 不相关的 $x$ 的所有线性变换中方差最大的。

这时称 $y_1,y_2,\cdots,y_m$ 为 $x$ 的第一主成分，第二主成分，... 第 $m$ 主成分。

### 推导

首先叙述一个有关总体主成分分析的定理。这一定理阐述了总体主成分与协方差矩阵的特征值和特征向量的关系，同时给出了一个求主成分的方法。

**定理**：设 $x$ 是 $m$ 维随机变量，$\varSigma$ 是 $x$ 的协方差矩阵，$\varSigma$ 的特征值分别是 $\lambda_1\geqslant\lambda_2\geqslant\cdots\geqslant\lambda_m\geqslant 0$，特征值对应的单位特征向量分别是 $\alpha_1,\alpha_2,\cdots,\alpha_m$，则 $x$ 的第 $k$ 主成分是
$$
y_k=\vec\alpha_k^Tx=\vec\alpha_{1k}x_1+\vec\alpha_{2k}x_2+\cdots+\vec\alpha_{mk}x_m,\quad k=1,2,\cdots,m
$$
$x$ 的第 $k$ 主成分的方差为
$$
\mathrm{var}(y_k)=\vec\alpha_k^T\varSigma\vec\alpha_k=\lambda_k,\quad k=1,2,\cdots,m
$$
即协方差矩阵 $\varSigma$ 的第 $k$ 个特征值。

下面给出该定理的证明：采用拉格朗日乘子法求出主成分，首先求第一主成分 $y_1=\vec\alpha_1^Tx$，即求出系数向量 $\alpha_1$。根据定义可知，第一主成分的 $\vec\alpha_1$ 是在 $\vec\alpha_1^T\vec\alpha_1$ 条件下，$x$ 的所有线性变化中使得方差
$$
\mathrm{var}(\alpha_1^Tx)=\vec\alpha_1^T\varSigma\vec\alpha_1
$$
达到最大的 $\vec\alpha_1$，于是求第一主成分就变成了求解有约束最优化问题
$$
\begin{aligned}
\max_{\vec\alpha_1}\quad&\vec\alpha_1^T\varSigma\vec\alpha_1\\
\mathrm{s.t.}\quad&\vec\alpha_1^T\vec\alpha_1=1
\end{aligned}
$$
使用拉格朗日乘子法求解，定义拉格朗日函数为
$$
L(\vec\alpha_1)=\vec\alpha_1^T\varSigma\vec\alpha_1-\lambda(\vec\alpha_1^T\vec\alpha_1-1)
$$
其中 $\lambda$ 为拉格朗日乘子，对函数中的 $\vec\alpha_1$ 求导并使导数为 0 得到
$$
\frac{\partial L}{\partial \vec\alpha_1}=2\varSigma\vec\alpha_1-2\lambda\vec\alpha_1=0
$$
上式可以转换为
$$
\lambda\vec\alpha_1=\varSigma\vec\alpha_1
$$
因此 $\lambda$ 为 $\varSigma$ 的特征值，而 $\vec\alpha_1$ 是对应的单位特征向量。于是目标函数可以转换为
$$
\vec\alpha_1^T\varSigma\vec\alpha_1=\lambda\vec\alpha_1^T\vec\alpha_1=\lambda
$$
假设 $\vec\alpha_1$ 是 $\varSigma$ 的最大特征值 $\lambda$ 对应的单位特征向量，显然 $\vec\alpha_1$ 是最优化问题的解。所以 $\vec\alpha_1^Tx$ 构成第一主成分，其方差为协方差矩阵中最大的特征值。

同理，计算 $x$ 的第二主成分 $y_2=\vec\alpha_2^Tx$，第二主成分的 $\vec\alpha_2$ 是在 $\vec\alpha_2^T\vec\alpha_2=1$，且 $\vec\alpha_2^Tx$ 与 $\vec\alpha_2^Tx$ 不相关的条件下，$x$ 的所有线性变换中使方差
$$
\mathrm{var}(\vec\alpha_2^Tx)=\vec\alpha_2^T\varSigma\vec\alpha_2
$$
达到最大，于是求解 $\vec\alpha_2$ 变成了求解有约束最优化问题
$$
\begin{aligned}
\max_{\vec\alpha_2}\quad& \vec\alpha_2^T\varSigma\vec\alpha_2\\
\mathrm{s.t.}\quad&\vec\alpha_1^T\varSigma\vec\alpha_2=0,\quad \vec\alpha_2^T\varSigma\vec\alpha_1=0\\
&\vec\alpha_2^T\vec\alpha_2=1
\end{aligned}
$$
由于 $\lambda_1\vec\alpha_1=\varSigma\vec\alpha_1$，所以
$$
\vec\alpha_1^T\varSigma\vec\alpha_2=\vec\alpha_2^T\varSigma\vec\alpha_1=\vec\alpha_2^T\lambda_1\vec\alpha_1=\lambda_1\vec\alpha_1^T\vec\alpha_2=\lambda_1\vec\alpha_2^T\vec\alpha_1
$$
于是约束条件变为
$$
\vec\alpha_1^T\vec\alpha_2=0,\quad\vec\alpha_2^T\vec\alpha_1=0
$$
定义拉格朗日函数
$$
L(\vec\alpha_2)=\vec\alpha_2^T\varSigma\vec\alpha_2-\lambda(\vec\alpha_2^T\vec\alpha_2-1)-\phi\vec\alpha_2^T\vec\alpha_1
$$
求导并使导数为 0 得到
$$
\frac{\partial L}{\partial \vec\alpha_2}=2\varSigma\vec\alpha_2-2\lambda\vec\alpha_2-\phi\vec\alpha_1=0
$$
上式左乘 $\vec\alpha_1^T$ 得到
$$
2\vec\alpha_1^T\varSigma\vec\alpha_2-2\lambda \vec\alpha_1^T\vec\alpha_2-\phi=0
$$
此式的前两项为 0，于是有 $\phi=0$，因此上式变为
$$
\varSigma\vec\alpha_2-\lambda\vec\alpha_2=0
$$
所以 $\lambda$ 是 $\varSigma$ 的特征值，而 $\vec\alpha_2$ 是对应的特征向量。于是 $\vec\alpha_2$ 非常明显应当为第二大的特征值对应的特征向量，并且有
$$
\mathrm{var}(\vec\alpha_2^Tx)=\vec\alpha_2^T\varSigma\vec\alpha_2=\lambda_2
$$
按照上述方法，可以推导得到所有的 $m$ 个主成分的情况，最终得到第 $k$ 个主成分为第 $k$ 个特征值对应的特征向量，并且第 $k$ 个主成分对应的方差为 $\lambda_k$。

---

根据上述定理，我们可以得到推论：$m$ 维随机变量 $y=(y_1,y_2,\cdots,y_m)^T$ 的分量依次是 $x$ 的第一主成分到第 $m$ 主成分的充要条件是：
1. $y=A^Tx$，其中 $A$ 是正交矩阵
2. $y$ 的协方差矩阵为对角矩阵，即 $\mathrm{cov}(y)=\mathrm{diag}(\lambda_1,\lambda_2,\cdots,\lambda_m)$，且 $\lambda_1\geqslant\lambda_2\geqslant\cdot\geqslant\lambda_m$

在之前的证明中，$\lambda_k$ 是 $\varSigma$ 的第 $k$ 个特征值，$\alpha_k$ 是对应的单位特征向量，即
$$
\varSigma\vec\alpha_k=\lambda_k\vec\alpha_k,\quad k=1,2,\cdots,m
$$
使用矩阵表示为
$$
\varSigma A=A\varLambda
$$
这里的 $A=[\alpha_{ij}]_{m\times m}$，$\varLambda$ 是对角矩阵，其第 $k$ 个对角元素是 $\lambda_k$，因为 $A$ 是正交矩阵，即 $A^TA=AA^T=I$，由上式得到两个公式
$$
A^T\varSigma A=\varLambda
$$
和
$$
\varSigma=A\varLambda A^T
$$

#### 性质

下面叙述几个总体主成分的性质：
1. 总体主成分 $y$ 的协方差矩阵是对角矩阵
   $$
\mathrm{cov}(y)=\varLambda=\mathrm{diag}(\lambda_1,\lambda_2,\cdots,\lambda_m)
$$
2. 总体主成分 $y$ 的方差之和等于随机变量 $x$ 的方差之和
   $$
\sum_{i=1}^m\lambda_i=\sum_{i=1}^m\sigma_{ii}
$$
其中 $\sigma_{ii}$ 是随机变量 $x_i$ 的方差，即协方差矩阵 $\varSigma$ 的对角元素。事实上，利用矩阵的迹可以得到
$$
\begin{aligned}
\sum_{i=1}^m\mathrm{var}(x_i)&=\mathrm{tr}(\varSigma^T)=\mathrm{tr}(A\varLambda A^T)=\mathrm{tr}(A^T\varLambda A)\\&=\mathrm{tr}(\varLambda)=\sum_{i=1}^m\lambda_i=\sum_{i=1}^m\mathrm{var}(y_i)
\end{aligned}
$$
3. 第 $k$ 个主成分 $y_k$ 与变量 $x_i$ 的相关系数 $\rho(y_k,x_i)$ 称为因子负荷量 (factoring loading)，它表示第 $k$ 个主成分 $y_k$ 与变量 $x_i$ 的相关关系。计算公式为
   $$
\rho(y_k,x_i)=\frac{\sqrt{\lambda_k}\alpha_{ik}}{\sqrt{\sigma_{ii}}},\quad k,i=1,2,\cdots,m
$$
因为
$$
\rho(y_k,x_i)=\frac{\mathrm{cov}(y_k,x_i)}{\sqrt{\mathrm{var}(y_k)\mathrm{var}(x_i)}}=\frac{\mathrm{cov}(\vec\alpha_k^Tx,\vec e_i^Tx)}{\sqrt{\lambda_k}\sqrt{\sigma_{ii}}}
$$
其中 $\vec e_i$ 为基本单位向量，其第 $i$ 个分量为 1，其余为 0。再由协方差的性质有
$$
\mathrm{cov}(\vec\alpha_k^Tx,\vec e_i^Tx)=\vec\alpha_k^T\varSigma \vec e_i=\vec e_i^T\varSigma\vec\alpha_k^T=\lambda_k\vec e_i^T\vec\alpha_k=\lambda\vec\alpha_{ik}
$$
4. 第 $k$ 个主成分 $y_k$ 与 $m$ 个变量的因子负荷量满足
   $$
\sum_{i=1}^m\sigma_{ii}\rho^2(y_k,x_i)=\lambda_k
$$
因为
$$
\sum_{i=1}^m\sigma_{ii}\rho^2(y_k,x_i)=\sum_{i=1}^m\lambda_k\vec\alpha_{ik}^2=\lambda_k\vec\alpha_k^T\vec\alpha_k=\lambda_k
$$
5. $m$ 个主成分与第 $i$ 个变量 $x_i$ 的因子负荷量满足
   $$
\sum_{k=1}^m\rho^2(y_k,x_i)=1
$$
因为 $y_1,y_2,\cdots,y_m$ 互不相关，因此
$$
\rho^2(x_i,(y_1,y_2,\cdots,y_m))=\sum_{k=1}^m\rho^2(y_k,x_i)
$$
又因为 $x_i$ 可以表示为 $y_1,y_2,\cdots,y_m$ 的线性组合，所以 $x_i$ 与 $y_1,y_2,\cdots,y_m$ 的相关系数的平方为 1。

### 主成分的个数

主成分分析的主要作用是降维，所以一般选择 $k(k \ll m)$ 个主成分来替代 $m$ 个原有变量，使得问题得以简化，并能保留原有变量的大部分信息。这里所说的原有变量的信息就是指原有变量的方差。下面给出定理，说明选择前 $k$ 个主成分是最优选择。

**定理**：对任意正整数 $q$，$1\leqslant q\leqslant m$，考虑正交线性变换
$$
y=B^Tx
$$
其中 $y$ 是 $q$ 维向量，$B^T$ 是 $q\times m$ 矩阵，令 $y$ 的协方差矩阵为
$$
\varSigma_y=B^T\varSigma B
$$
则 $\varSigma_y$ 的迹 $\mathrm{tr}(\varSigma_y)$ 在 $B=A_q$ 时取最大值，其中矩阵 $A_q$ 由正交矩阵 $A$ 的前 $q$ 列组成。

下面给出证明：令 $\vec\beta_k$ 是 $B$ 的第 $k$ 列, 由于正交矩阵 $A$ 的列构成 $m$ 维空间的基，所以 $\vec\beta_k$ 可以由 $A$ 的列表示，即
$$
\vec\beta_k=\sum_{j=1}^mc_{jk}\vec\alpha_j,\quad k=1,2,\cdots,q
$$
等价的，
$$
B=AC
$$
其中 $C$ 是 $m\times q$ 矩阵，其第 $j$ 行第 $k$ 列元素为 $c_{jk}$。

首先，由
$$
B^T\varSigma B=C^TA^T\varSigma AC=C^TAC=\sum_{j=1}^m\lambda_j\vec c_j\vec c_j^T
$$
其中 $\vec c_j$ 是 $C$ 的第 $j$ 行。因此
$$
\begin{aligned}
\mathrm{tr}(B^T\varSigma B)&=\sum_{j=1}^m\lambda_j\mathrm{tr}(\vec c_j\vec c_j^T)\\&=\sum_{j=1}^m\lambda_j\vec c_j^T\vec c_j\\&=\sum_{j=1}^m\sum_{k=1}^q\lambda_jc_{jk}^2
\end{aligned}\tag{1}
$$

其次，根据 $A$ 的正交性有
$$
C=A^{-1}B=A^TB
$$
由于 $A$ 是正交的，$B$ 的列是正交的，于是
$$
C^TC=B^TAA^TB=B^TB=I_q
$$
即 $C$ 的列也是正交的。于是
$$
\mathrm{tr}(C^TC)=\mathrm{tr}(I_q),\quad \sum_{j=1}^m\sum_{k=1}^q c_{jk}^2=q
$$

这样，矩阵 $C$ 可以认为是一个某 $m$ 阶正交矩阵 $D$ 的前 $q$ 列。正交矩阵 $D$ 的行也正交，因此满足
$$
\vec d_j^T\vec d_j=1,\quad j=1,2,\cdots,m
$$
其中 $\vec d_j^T$ 是 $D$ 的第 $j$ 行，由于矩阵 $D$ 的行包括矩阵 $C$ 的行的前 $q$ 个元素，所以
$$
\vec c_j^T\vec c_j\leqslant 1,\quad j=1,2,\cdots,m
$$
即
$$
\sum_{k=1}^qc_{jk}^2\leqslant 1,\quad 1,2,\cdots,m
$$
注意到在式 $(1)$ 中 $\sum_{k=1}^qc_{jk}^2$ 是 $\lambda_j$ 的系数，且这些系数之和是 $q$，并且这些系数小于等于 1，因为 $\lambda_1\geqslant\lambda_2\geqslant\cdots\geqslant\lambda_q\geqslant\cdots\geqslant\lambda_m$，显然，当能够找到 $c_{jk}$ 使得
$$
\sum_{k=1}^qc_{jk}^2=\begin{cases}1,&j=1,\cdots,q\\0,&j=q+1,\cdots,m\end{cases}\tag{2}
$$
时，$\displaystyle\sum_{j=1}^m\left(\sum_{k=1}^qc_{jk}^2\right)\lambda_i$ 最大。当 $B=A_q$ 时，有
$$
c_{jk}=\begin{cases}1,&1\leqslant j=k\leqslant q\\0,&other.\end{cases}
$$
满足式 $(2)$，所以当 $B=A_q$ 时，$\mathrm{tr}(\varSigma_y)$ 达到最大值。

上述定理说明，当 $A$ 的前 $q$ 列取 $x$ 的前 $q$ 个主成分时，能够最大限度的保留原有变量方差的信息。使用同样的方法可以证明，舍弃后的 $q$ 个主成分时，可以使原有变量信息的损失最少。

上面的定理作为选择 $k$ 个主成分的理论依据。具体选择 $k$ 的方法，通常是用方差贡献率。

**定义**：第 $k$ 个主成分 $y_k$ 的方差贡献率定义为 $y_k$ 的方差与所有方差之和的比，记为 $\eta_k$，
$$
\eta_k=\frac{\lambda_k}{\sum_{i=1}^m\lambda_i}
$$

通常取累计方差贡献比达到规定百分比以上，例如 70\%~80\%以上。累计方差贡献率反映了主成分保留的信息的比例，但是它不能反映某个原有变量 $x_i$ 保留信息的比例，这是通常使用 $k$ 个主成分 $y_1,y_2,\cdots,y_k$ 对原有变量 $x_i$ 的贡献率。

**定义**：$k$ 个主成分 $y_1,y_2,\cdots,y_k$ 对原有变量 $x_i$ 的贡献率定义为 $x_i$ 与 $(y_1,y_2,\cdots,y_k)$ 的相关系数的平方，记为
$$
v_i=\rho^2(x_i,(y_1,y_2,\cdots,y_k))
$$
计算公式如下：
$$
v_i=\rho^2(x_i,(y_1,y_2,\cdots,y_k))=\sum_{j=1}^k\rho^2(x_i,y_j)=\sum_{j=1}^k\frac{\lambda_j\alpha_{ij^2}}{\sigma_{ii}}
$$
### 规范化变量的总体主成分分析

在实际问题中，不同的变量可能有不同的量纲，直接求主成分分析可能会得到不合理的结果。为了消除这些影响，常常对各个随机变量实施规范化，使其均值为 0，方差为 1。

规范化方法为减去均值，除以方差。于是规范化向量为
$$
x_i^*=\frac{x_i-E(x_i)}{\sqrt{\mathrm{var}(x_i)}}
$$
根据定义可以得到，规范化随机变量的协方差矩阵就是相关矩阵 $R$。主成分分析通常在规范化随机变量的协方差矩阵即相关系数矩阵上进行。

## 样本主成分分析

前文叙述的是在样本总体上进行主成分分析，而在实际问题中，需要在观测数据上进行主成分分析。样本主成分分析的概念与总体主成分分析相同，因此这里着重于算法。

### 定义

设对 $m$ 维随机变量进行 $n$ 次独立观测，$x_1,x_2,\cdots,x_n$ 表示观测样本。使用样本矩阵 $X$ 表示，即
$$
X=\begin{bmatrix}x_1&x_2&\cdots&x_n\end{bmatrix}^T=\begin{bmatrix}x_{11}&x_{12}&\cdots&x_{1m}\\x_{21}&x_{22}&\cdots &x_{2m}\\\vdots&\vdots&&\vdots\\x_{n1}&x_{n2}&\cdots&x_{nm}\end{bmatrix}
$$
给定样本矩阵 $X$，可以估计样本的均值向量 $\bar x$ 为
$$
\bar x=\frac{1}{n}\sum_{j=1}^nx_j
$$
样本的协方差矩阵 $S$ 为
$$
\begin{aligned}
S&=[s_{ij}]*{m\times m}\\
s*{ij}&=\frac{1}{n-1}\sum_{k=1}^n(x_{ik}-\bar x_i)(x_{jk}-\bar x_j)
\end{aligned}
$$

样本的相关矩阵 $R$ 为
$$
R=[r_{ij}]*{m\times m},\quad r*{ij}=\frac{s_{ij}}{\sqrt{s_{ii}s_{jj}}}
$$

定义 $m$ 维向量 $x$ 到 $m$ 维向量 $y$ 的一个线性变换
$$
y=A^Tx
$$
其中 $A$ 是 $m\times m$ 矩阵，其中对于每一个 $x_i$ 有
$$
y_i=a_i^Tx_i
$$
则 $y_i$ 的样本均值与样本方差分别为
$$
\mathrm{var}(y_i)=a_i^TSa_i,\quad \bar y_i=a_i^T\bar x
$$
其中 $S$ 为 $x$ 的样本协方差矩阵。对于任意两个线性变换，有 $y_i$ 与 $y_j$ 的样本协方差为
$$
\mathrm{cov}(y_i,y_k)=a_i^TSa_k
$$

样本主成分分析的定义如下：给定样本矩阵 $X$，样本第一主成分 $y_1=\vec a_1^Tx$ 是在 $\vec a_1^T\vec a_1=1$ 的条件下，使得 $\vec a_1^Tx_j(j=1,2,\cdots,n)$ 的样本方差 $\vec a_1^TS\vec a_1$ 最大的 $x$ 的线性变换。样本第二主成分 $y_2=\vec a_2^Tx$ 是在 $\vec a_2^T\vec a_2=1$ 和 $\vec a_1^Tx_j(j=1,2,\cdots,n)$ 的样本协方差 $\vec a_1^TS\vec a_2=0$ 条件下，使得 $\vec a_2^Tx_j(j=1,2,\cdots,n)$ 的样本方差 $\vec a_2^TS\vec a_2$ 最大的 $x$ 的线性变换。一般地，样本第 $i$ 个主成分 $y_i=\vec a_i^Tx$ 是在 $\vec a_i^T\vec a_i=1$ 和 $\vec a_i^Tx_j$ 与 $\vec a_k^Tx_j(k<i,j=1,2,\cdots,n)$ 的样本协方差 $\vec a_k^TS\vec a_i=0$ 条件下，使得 $\vec a_i^Tx_j(j=1,2,\cdots,n)$ 的样本方差 $\vec a_i^TS\vec a_i$ 最大的 $x$ 的线性变换。

在使用样本主成分分析时，一般假设样本数据时规范化的，即对样本进行如下变换：
$$
x_{ij}^*=\frac{x_{ij}-\bar x_i}{\sqrt{s_{ii}}},\quad i=1,2,\cdots,m;\quad j=1,2,\cdots,n
$$
其中
$$
\begin{aligned}
\bar x_i&=\frac{1}{n}\sum_{j=1}^nx_{ij},\quad i=1,2,\cdots,m\\
s_{ii}&=\frac{1}{n-1}\sum_{j=1}^n(x_{ij}-\bar x_i)^2,\quad i=1,2,\cdots,m
\end{aligned}
$$

### 样本主成分分析算法

传统的主成分分析通过数据的协方差矩阵或相关矩阵的特征值分解进行，现在常用的方法是通过数据矩阵的奇异值分解。这里先介绍基于特征值分解的方法：

给定样本矩阵 $X$，利用数据的样本协方差矩阵或者相关矩阵的特征分解进行主成分分析。具体步骤如下：
1. 对观测数据进行规范化
2. 计算样本相关系数矩阵 $R$ (协方差矩阵)
   $$
R=[r_{ij}]*{m\times m}=\frac{1}{n-1}XX^T
$$
其中
$$
r_{}{ij}=\frac{1}{n-1}\sum_{l=1}^nx_{il}x_{jl},\quad i,j=1,2,\cdots,m
$$
3. 求样本相关矩阵的 $k$ 个特征值和 $k$ 个单位特征向量。即求解
   $$
|R-\lambda I|=0
$$
得到 $R$ 的 $m$ 个特征值
$$
\lambda_1\geqslant \lambda_2\geqslant \cdots\geqslant \lambda_m
$$
通过计算方差贡献率达到预定值的主成分的个数 $k$，求得前 $k$ 个特征值对应的单位特征向量。
4. 计算 $k$ 个主成分
   $$
y_i=\vec a_i^Tx_i,\quad i=1,2,\cdots,k
$$

主成分分析得到的结果可以用于其他机器学习方法的输入，比如，将样本点投影到以主成分为坐标轴的空间中，然后应用[[00_Inbox/机器学习/聚类|聚类]]算法，就可以对样本点进行聚类。

下面介绍一下基于奇异值分解的算法：输入数据为规范化后的数据。
1. 构造新的 $n\times m$ 矩阵
$$
X'=\frac{1}{\sqrt{n-1}}X^T
$$
使得 $X'$ 的每一列均值为零
2. 对矩阵 $X'$ 进行截断奇异值分解，得到
$$
X'=U\varSigma V^T
$$
得到 $k$ 个奇异值，奇异向量。矩阵 $V^T$ 和 $X$ 的乘积构成样本主成分矩阵。
3. 求 $k\times n$ 样本主成分矩阵
$$
Y=V^TX
$$
