---
aliases:
  - tsne
  - t-sne
tags:
  - 机器学习
  - 无监督学习
  - 降维
---

# t-SNE 降维

t-SNE 是机器学习中运用广泛的降维方法之一，其主要用于绘制高维度数据的分类图。区别于 [[00-笔记/机器学习/主成分分析|PCA]] 方法，t-SNE 方法更加注重于保留原始数据的局部特征，因此在进行可视化时，[[00-笔记/机器学习/主成分分析|PCA]] 方法容易出现拥挤现象，而 t-SNE 方法可以很好的避免。

t-SNE 方法的基本思想是将距离远近转换为了一个概率分布，每一个概率分布对应了一个样本间距离远近的关系。而降维前后的数据各自对应一个概率分布。

基本方法就是随机生成同等数量的低维数据，然后通过梯度下降法最小化损失函数来更新低维数据，最后得到降维结果。根据这个思想，可以发现 t-SNE 降维的结果不是唯一的，每次得到的结果依赖于初始生成的值。

## 定义概率分布

t-SNE 降维将高维数据与低维数据对应到了两个概率分布上，我们首先要解决的问题是如何定义概率分布使其可以保留样本数据间的距离关系。t-SNE 通过统计中的 $t$ 分布对每个数据点 $i$ 定义了条件概率分布
$$
P(j\mid i) = \frac{S(x_i,x_j)}{\sum_{k\ne i} S(x_i,x_k)}\quad j\ne i,i = 1,2,\cdots,n
$$
其中 $S(x_i,x_j)$ 是样本点 $x_i$ 与 $x_j$ 的相似度度量，距离越近相似度越大。假设有 $n$ 个数据，那么我们将得到 $n$ 个条件概率分布。同样的，假设降维后的数据为 $z_i$，那么可以得到
$$
Q(j\mid i) = \frac{S'(z_i,z_j)}{\sum_{k\ne i} S'(z_i,z_k)}\quad j\ne i,i = 1,2,\cdots,n
$$
由于进行了规范化，消除了单位的影响，因此高维度和低维度的数据的相似度度量可以是不一样的。

## 损失函数

我们的最终目标是优化低维数据使其概率分布与高维数据相同，因此我们需要定义损失函数并对其进行优化。衡量两个概率分布距离的方式可以使用 [[00-笔记/实践知识/熵、相对熵与交叉熵#相对熵|KL散度]]。

假设对于已知的离散条件概率分布 $P(j\mid i)$ 与 $Q(j\mid i)$，离散的 KL 散度定义为
$$
\mathrm{KL}(P\parallel Q)=-\sum_{i=1}^nP(x_i)\log\frac{Q(x_i)}{P(x_i)}
$$
由于 KL 散度描述的是两个概率分布之间的差异，因此我们可以通过最小化 KL 散度来使低维数据的概率分布逼近高维数据的概率分布。需要注意的是，由于 KL 散度不是对称的，因此最小化 $\mathrm{KL}(P\parallel Q)$ 与 $\mathrm{KL}(Q\parallel P)$ 得到的结果是不一致的。对于最小化 KL 散度而言，通常是在右边的值逼近左边的值。如下图所示：
![[Assets/Pasted image 20230817110851.png]]
- 左图：$p$ 在左边，因此 $p$ 的峰值处的 $q$ 值也要尽量大，导致得到的 $q$ 的峰值在中心。
- 右图：$q$ 在左边，因此 $q$ 的峰值处的 $q$ 值要尽量大，导致得到的 $q$ 的峰值与 $p$ 的一个峰值重合。

利用 KL 散度定义的损失函数如下：
$$
\begin{aligned}
L(z_1,\cdots,z_n)&=\sum_{i=1}^n\mathrm{KL}\left(P\left(j\mid i\right)\parallel Q(j\mid i)\right)\\&=-\sum_{i=1}
^n\sum_{j=1}^nP(j\mid i)\log\frac{Q(j\mid i)}{P(j\mid i)}
\end{aligned}
$$

我们的优化目标就是最小化损失函数
$$
z_1^*,\cdots,z_n^*=\arg\min_{z_1,\cdots,z_n}L(z_1,\cdots,z_n)
$$

## 相似度的定义方式

相似度的定义方式是开放性的，可以针对问题进行定义，下面介绍在一般情况下的定义方式。在高维数据中，t-SNE 方法使用高斯距离
$$
S(x_i,x_j)=\exp\left(\frac{-\|x_i-x_j\|_2^2}{2\sigma^2}\right)
$$
这样的好处是可以将高维数据中的距离放大，以减轻拥挤现象。其中 $\sigma$ 是一个超参数，通常可以通过搜索的方式找到最优。根据之前概率分布的定义，我们可以发现这一步实际是使用高斯分布来描述高维数据的概率分布。

t-SNE 在低维数据中定义的距离函数为
$$
S'(z_i,z_j)=\left[1 + \|z_i-z_j\|_2^2\right]^{-1}
$$
上述定义同样是为了更好的将高维数据中的距离放大，以减轻拥挤问题。通过高维数据中的指数，低维数据中的反比例函数，最终可以将距离缩放到一个较为合适的大小。同样，这里实际上是使用 $t$ 分布来模拟低维数据的概率分布。

## 学习

直接使用梯度下降学习得到最终的结果，由于推导梯度的过程较为繁琐，下面直接给出最终的梯度：
$$
\frac{\partial L}{\partial z_i}=4\sum_{j=1}^n(z_i-z_j)[P(j\mid i) - Q(j\mid i)]\left(1+\|z_i-z_j\|_2^2\right)^{-1}
$$
于是可以使用梯度下降法来求解降维结果，此外，还可以在梯度下降中添加随时间降低的噪声数据来跳出局部最优值。

## 与 PCA 的区别

PCA 与 t-SNE 都是常用的降维方法，但是相对于 PCA 方法，t-SNE 不能用于可视化，而不可以用于数据转换。因此 t-SNE 方法是针对当前数据的，对于新增的数据并不使用。

此外，t-SNE 方法在数据维度非常高的时候计算量很大，因此对于维度过高的数据我们通常先使用 PCA 方法将其维度降低至 10 维后再使用 t-SNE 进行可视化。
