# 微调预训练语言模型

目前自然语言处理的基本范式是 pretrain + fine-tuning，因此这里记录一些微调时的遇到的问题。

## 预训练模型

大多数我们需要的预训练模型都可以在 Huggingface 与 torch hub 上找到，例如 Bert、gpt2 等模型。[Hugging Face](https://huggingface.co/) 基于 Transformers 库，基本调用接口为
```python
from transformer import AutoTokenizer, AutoModel

model = AutoModel.from_pretrained("model-name")
tokenizer = AutoTokenizer.from_pretrained("model-name")
```
其中 model 的名字可以在 hugging face 上查询。hugging face 上配备的详细的使用文档，可以在使用时进行查询。这里主要记录一下 tokenizer 的使用。在实际使用中，我们常使用的它的__call__方法，其参数如下：
```python
def __call__(
	text : str | list[str] | list[list[str]],
	text_pair : str | list[str] | list[list[str]],
	text_target : str | list[str] | list[list[str]],
	text_pair_target : str | list[str] | list[list[str]],
	padding : bool = False,
	truncation : bool = False,
	max_length : int,
	return_tensors : str
)
```
其中：
1. text：要编码的句子，或者句子的列表，或者 token 列表的列表。
2. text_pair：要编码的句子对，分别作为两个参数输入。
3. text_target：要作为目标的句子
4. text_pair_target：要作为目标的句子
5. padding：填充，默认为 False,
6. truncation：截断，默认为 False
7. max_length：最大长度
8. return_tensors：可以是 `pt` (pytorch)、`tf` (tensorflow)、`np` (numpy)。