# 熵、相对熵与交叉熵

熵、相对熵与交叉熵是信息论中的概念。下面从信息量开始。

## 信息量

信息量是对事件发生概率的度量，一个事件发生的概率越低，那么事件包含的信息量越大。对于[[02_Areas/概率论与数理统计/一维随机变量及其分布|随机变量]] $X$，事件 $X=x_0$ 的信息量定义如下
$$
I(x_0)=\log (\frac{1}{p_{x_0}})=-\log p_{x_0}
$$
其中 $p_{x_0}$ 是事件 $x_0$ 发生的概率。

## 信息熵

信息熵又称为平均自信息，信息熵定义了信息的不确定性，一个系统的不确定性越大，那么这个系统的熵越大。信息熵的定义如下
$$
H(X)=E[I(x_i)]=-\sum_{i=1}^np_i\log p_i
$$
信息熵的物理含义是传输 $X$ 发生事件的最小比特数。信息熵在机器学习中也有提及，但是没有直接使用，使用更加频繁的概念是下面的相对熵和交叉熵。

## 交叉熵

交叉熵的定义为
$$
H(p,q)=-\sum_{i=1}^np_i\log q_i
$$
交叉熵是机器学习分类任务中常用的一种损失函数，可以证明，最小化交叉熵理论上等价于极大似然估计。交叉熵的物理含义可以理解为传输错误编码需要的比特，直观的理解，最小化交叉熵就是最小化错误。

## 相对熵

相对熵也称 **KL 散度** (KL divergence)，用于衡量两个概率分布之间的差异。对于两个概率分布 $p(x)$ 和 $q(x)$，可以得到相对熵
$$
\mathrm{KL}(p\parallel q)=-\int p(x)\ln q(x)\mathrm{d}x-(-\int p(x)\ln p(x)\mathrm dx)
$$
由于在相对熵中，$p(x)$ 与 $q(x)$ 的地位对等的，因此相对熵不满足交换律。此外，根据相对熵的定义，一定有
$$
\mathrm{KL}(p\parallel q) \geqslant 0
$$
从定义上看，相对熵其实就是交叉熵减去自信息熵，即
$$
\mathrm{KL}(p\parallel q)=H(p,q)-H(p)
$$
从物理意义上看，相对熵表示相对于最优的编码，使用错误的编码会浪费多少比特。在机器学习中，相对熵常用于衡量两个概率分布的距离，相对熵越小，则两个概率分布越接近。

同时，相对熵可以视为广义上的交叉熵，交叉熵用于目标确定的监督学习任务，由于监督学习中 $p$ 是确定的，因此 $H(p)=0$，所以此时相对熵就退化为了交叉熵。