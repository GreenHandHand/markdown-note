---
tags:
  - 深度学习
---
# 深度学习概念与感知机
---
本章主要介绍一些概念与感知机（单层神经网络）。
## 概念

### 机器学习

下面是来自 [[00-笔记/深度学习/机器学习简介|机器学习]] 中的一些基本术语：

|名称|表示|含义|
|:----:|:---:|:---:|
|训练集 (training dataset/training set)|x_train|训练数据集|
|样本数据 (sample)|x|训练集的每行数据|
|数据点 (data point)|x|同样本数据|
|数据实例 (data instance)|x|同样本数据|
|标签 (label)|y_label|预测的目标|
|目标 (target)|y_label|同标签|
|特征 (feather)|x|预测依据的自变量|
|协变量 (covariate)|x|同特征|

### 损失函数

神经网络的目标是拟合 (fitting) 数据，因此我们需要确定拟合程度的度量。**损失函数 (loss function)** 是一种可以量化目标值的实际值与预测值之间的差距的函数，我们将通过损失函数求出的目标值与实际预测值之间的差异称为**损失 (loss)**。

通常我们会选择非负数作为损失，且数值越小损失越小，完美预测时损失为 0。常见的损失函数有均分误差 (MSE)，交叉熵 (Cross Entropy)等。

### 优化算法

**梯度下降 (Gradient Descent)** 是一种常用于优化模型参数的方法，是神经网络优化算法的基础。它的思想是通过计算函数的导数（梯度），找到使目标函数值最小化的方向，然后不断迭代更新参数，使得函数值逐渐趋近于最小值。在训练中，每次通过计算损失函数对参数求梯度来确定更新参数的方向和大小。

基本的梯度下降算法有三种：批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）和小批量梯度下降（Mini-Batch Gradient Descent）。其中，批量梯度下降使用全部数据进行计算，更新速度较为缓慢；随机梯度下降使用单个样本进行计算，收敛速度较快但不稳定；小批量梯度下降则折中了前两者。

梯度下降方法是深度学习领域中最基础、最核心的优化方法之一，具有广泛的应用。在深度学习中，我们使用梯度下降算法及其各种变种、动量更新（Momentum）、自适应学习率（Adaptive Learning Rate）和二阶优化算法（Second-order Optimization）等。

#### 小批量梯度下降

我们通常使用下面的公式更新神经网络：
$$
(\theta_0,\theta_1,\cdots)\leftarrow(\theta_0,\theta_1,\cdots)-\frac{\eta}{|B|}\sum_x \mathrm{grad}_\theta(x)
$$
由此引出几个在深度学习中更加常用的术语：

|名称|表示|含义|
|:---:|:---:|:---:|
|批量大小 (batch size)|batch_size|每个批量中数据的数量 |
|轮 (epoch)|epoch|一轮完整的训练|
|学习率 (learning rate)|learning_rate| $\eta$ |
|超参数 (hyperparameter)|无|不是通过模型得到的，需要事先由使用者确定的参数|
|调参 (hyperparametertuning)|无|选择超参数的过程|
|验证数据集 (validation dataset)|validation_set|评估超参数效果的数据集|
|泛化 (generalization)|无|模型在未经训练的数据中的表现水平|
|预测 (prediction)|predict|在给定特征的情况下估计目标|

### 神经网络

神经网络是一种模拟人脑结构和功能的计算模型，由多个神经元组成，可以用于各种机器学习任务（如分类、回归等）。其基本结构包括输入层、隐藏层和输出层，其中每个神经元接收来自上一层神经元的信号，并根据内部权重和偏置值进行加权求和，然后将结果通过一个激活函数输出给下一层神经元。

在神经网络中，我们通常使用下面的术语：

|名称|表示|含义|
|:---:|:---:|:---:|
|输入层|input_layer|神经网络输入层|
|隐藏层|hidden_layer|神经网络隐含层|
|输出层|output_layer|神经网络输出层|
|维度 (dimensionality)|dim|输入数据的特征数量|
|全连接层 (fully-connected layer)|fc 或 linear|每个节点都与下一节点相连|
|稠密层 (dense layer)|同全连接层|同全连接层|
|激活函数 (activate function)|activate_function|激活函数|
|权重 (weight)|weight|神经网络中神经元的连接程度|
|偏置 (bias)|bias|神经单元的偏移量|
|独热编码 (one-hot)|one_hot|将目标分类设为 1，其余分类为 0 的向量，用于分类任务|


## 感知机

感知机是只有一层输入层，一层输出层，没有隐藏层的神经网络。

### [[00-笔记/机器学习/线性模型|线性回归]]

如果没有激活函数，则感知机可以看作是输入数据的加权求和的结果，同线性回归算法。这里以线性回归为例子介绍在 pytorch 中计算神经网络的基本步骤。

在 [[00-笔记/深度学习/预备知识|pytorch]] 中，计算神经网络通常使用以下步骤：
1. 准备数据集：预处理数据，并将数据加载到 pytorch 中
2. 定义神经网络的结构：通过继承 `torch.nn.Module` 类或者使用 `torch.nn.Sequential` 方法定义神经网络的结构
3. 定义损失函数
4. 定义优化器并指定初始学习率
5. 训练模型：对于每个批次，通过计算损失，进行梯度反向传播，再调用优化器的 `step` 方法对模型进行更新
6. 在验证集或者测试集上评估模型的性能

下面完整的使用 pytorch 编写了一个线性回归模型，用于参考：
```python
# 导入所需的包，下面为习惯简称
import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils import data
from sklearn.model_selection import train_test_split
import tqdm # 进度条
import sys

# 生成数据
w_true = torch.tensor([2.1, 3.2, 4.3, 5.4, 6.5]).reshape(-1, 1)
b_true = 2.35
feature = torch.normal(0, 10, size = (1000, len(w_true)))
target = torch.mm(feature, w_true) + torch.normal(0, 0.001, size=(1000, 1)) + b_true

# 定义超参数
EPOCH_NUM = 10
LEARNING_RATE = 0.0003
BATCH_SIZE = 10
INPUT_SIZE = len(w_true)
OUTPUT_SIZE = 1

# 数据处理
def load_data(data_arrays, batch_size, is_train = True):
    """
    返回数据迭代器
    data_arrays: 数据
    batch_size: 批次大小
    is_train: 默认True, 是否打乱数据
    """
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, is_train)

X_train, X_test, y_train, y_test = train_test_split(feature, target, test_size = 0.3)
data_iter = load_data((X_train, y_train), BATCH_SIZE)

# 定义网络结构
class LinearNet(nn.Module):
    def __init__(self):
        super(LinearNet, self).__init__()
        self.fc = nn.Linear(INPUT_SIZE, OUTPUT_SIZE)
    
    def forward(self, x):
        return self.fc(x)
    
# 实例化
net = LinearNet()

# 定义损失函数和优化器
loss_fun = nn.MSELoss()
optim = torch.optim.SGD(net.parameters(), lr = LEARNING_RATE)

# 进行下降
for epoch in range(EPOCH_NUM):
    with tqdm.tqdm(total = len(y_train), desc=f'epoch {epoch}', file=sys.stdout) as process_bar:
        for X, y in data_iter:
            loss = loss_fun(net(X), y)
            optim.zero_grad()
            loss.backward()
            optim.step()

            process_bar.set_postfix({'loss':loss})
            process_bar.update(len(y))

# 评估准确性
with torch.no_grad():
    print('the error of w_true is :', w_true.reshape(-1) - net.fc.weight.data.reshape(-1))
    print('the error of b_ture is :', b_true - net.fc.bias.data)
```

### Softmax 回归

Softmax 回归就是在线性回归的基础上引入 Softmax 函数，使用线性回归来预测概率。Softmax 回归同样只有一层输入层，一层输出层，并且在输出层中使用 Softmax 函数将输出结果限制在 $(\,0,1\,)$ 中。

Softmax 函数表达式如下：
$$
\hat{y}=\mathrm{softmax}(o)\Leftrightarrow \hat{y}_j=\frac{\exp(o_j)}{\sum_k\exp(o_k)}
$$
由于 Softmax 中含有指数运算，所以在处理某些数值特别大的数据是，$\exp(o_j)$ 可能会发生上溢 (overflow) 的现象，使分母中或分子变成 inf 或者 nan ，最后得到的 $\hat{y}$ 数值为 0。解决上述问题的一个方法是在进行 Softmax 计算之前先将所有的 $o_j$ 减去一个 $\max(o_k)$，常数的移动不会改变 Softmax 的返回值。但是在执行了加法步骤与规范化步骤后，可能使得有些 $o_j-max(o_k)$ 会有较大的负值，导致 $\exp(o_j-\max(o_k))$ 有接近零的值，发生下溢 (underflow)，最终使得 $\hat{y}$ 为零，使得 $\log(\hat{y})=-\inf$，反向传播几步后，将会面对满屏 nan。

交叉熵损失表达式如下：
$$
Loss = \sum_{i=1}^nH(y^{(i)},\hat{y}^{(i)}),\quad H(y,\hat y)=\sum_{i=1}^q y_i\log\hat y_i
$$

结合 Softmax 函数与交叉熵损失，可以给出一个较好的解决方案，即避免在梯度下降时计算指数。具体的，我们直接通过数据计算交叉熵的值
$$
\begin{align}
\log \hat{y}&=\log\left(\frac{\exp(o_j-\max(o_k))}{\sum_k\exp(o_k-\max(o_k))}\right)\\&=\log(\exp(o_j-\max(o_k)))-\log\left(\sum_k\exp(o_k-\max(o_k))\right)\\&=o_j-\max(o_k)-\log\left(\sum_k\exp(o_k-\max(o_k))\right)
\end{align}
$$

于是避免了下溢出的现象。这种方法称为 LogSumExp 技巧。交叉熵可以作为损失函数原因可以通过极大似然估计来导出。

#### 交叉熵损失

因为 Softmax 函数得到的结果可以视为条件概率，所以我们计算准确的的条件概率，将其与 Softmax 得到的结果进行比较。现在设 $p=\{p_1,p_2,\cdots,p_k\}$ 为表示每一个元素为目标分类正确概率的向量，于是目标值 $q=\{q_1,q_2,\cdots,q_k\}$ 应当为真实目标的独热编码，可以得到
$$
P(Y|X;\theta)=\prod_{i=1}^nP(y^{(i)}|x^{(i)};\theta)=\prod_{i=1}^n\prod _{j=1}^{k} (p^{(i)}_j)^{q^{(i)}_j}
$$
上式即模型参数 $\theta$ 的似然，根据极大似然估计，我们最大化上式，即最小化其负对数，所以上式变为
$$
\begin{align}\theta^*&=\arg\min_\theta -\sum_{i=1}^n\sum_{j=1}^kq^{(i)}_j\log p^{(i)}_j
\end{align}
$$
当我们不知道真实的分布是，只能得到样本值。设 $x$ 的预测分布为 $P(x)$ ，而 $x$ 的真实分布为 $Q(x)$，根据大数定理，有
$$
\begin{align}
\theta^*&=
\arg\min_\theta-\sum_{i=1}^n\log P(x^{(i)})\\
&=\arg\min_\theta-\frac{1}{n}\sum_{i=1}^n\log P(x^{(i)})\\
&\approx\arg\min_\theta -E[\,\log P(X)\,]\\
&=\arg\min_\theta -\sum_{i=1}^nQ(x^{(i)})\log P(x^{(i)})
\end{align}
$$
上式的形式与最大似然估计的形式相同，因此在样本量足够时我们可以使用交叉熵作为损失函数来估计参数，由于 $q$ 是独热编码，因此除了正确预测项，其余项都消失了。

在信息论中，将下面的式子称为熵 (entropy)，代表了信息的不确定性，当事件发生的确定性越大，熵越小
$$
H(p)=\sum_{j}-p_j\log p_j
$$
而下面的式子称为交叉熵 (cross-entropy)，用于衡量两个概率之间的距离
$$
H(q,p)=\sum_{j}-q_j\log p_j
$$
交叉熵损失函数就可以表示如下：
$$
Loss=\sum_{i=1}^nH(y^{(i)},\hat{y}^{(i)})
$$
交叉熵损失广泛运用于分类问题中，相较于均方误差损失函数，交叉熵有以下优点：
1.  相比于传统的平方误差损失函数，交叉熵可以更好地处理分类问题。在分类任务中，我们通常对每个类别计算预测概率分布，并使用交叉熵来度量预测分布与真实分布之间的距离。由于交叉熵对错分样本的区别进行了加权处理，因此可以更好地惩罚错误分类的样本，并且收敛速度较快。
2.  交叉熵损失函数的梯度计算相对简单，容易实现和高效计算。特别是对于基于反向传播算法的深度学习模型，梯度计算是非常重要的环节，交叉熵可以大大简化计算过程，提高训练效率。
3.  交叉熵可以处理多分类问题，通过修改损失函数的形式，将其扩展到多分类情况下。同时，它也可以处理样本不平衡的情况，即某些样本类别相对较少的问题。