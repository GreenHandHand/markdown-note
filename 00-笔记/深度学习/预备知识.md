---
tags:
  - 深度学习
---
# 预备知识

这里包含了一些学习深度学习所必要的知识，包含数据的储存、操作和预处理。

## 数据操作

### 张量的创建

下面列出一些对于张量的创建操作：
```python
import torch
x = torch.arange(12) # 创建一个行向量，从0到11
x.shape # 获得x的形状
x.numel() # 获得x的元素数量
x.reshape(3, 4) # 将x的形状变化为(3, 4)
x.reshape(3, -1) # 当可以计算出来剩余值时，该值可以省略为-1
x = torch.ones(3, 3, 5) # 创建一个全1张量
x = torch.randn(3, 4) # 按标准正态分布随机创建张量
x = torch.tensor(list) # 按列表创建张量
```

### 张量的计算

对于普通的运算符，直接对张量使用可以理解为每个元素间进行操作。

广播机制：当两个运算的张量形状不匹配时，广播机制可以将两个张量的形状进行复制拓展以匹配他们的形状。

### 索引和切片

```python
x[-1] # 可以使用-1来对最后一个元素进行索引
x[1:3] # 切片索引
x[1, 3] = 9 # 利用索引来改变张量的值
x[0:2, :] = 12 # 对张量的多个值进行赋值
```

### 节省内存

```python
Y = Y + X # 这样做会额外分配一个内存给Y，容易造成不必要的内存浪费
Z[:] = Y + X # 这样不会分配额外的内存给Z
Y += X # 这样也不会
```

### 转换为其他 python 对象

```python
A = X.numpy() # 将X转换为np.ndarray对象
B = torch.tensor(A) # 将A转换为Tensor对象
X = torch.tensor([3])
X.item(), float(X), int(X) # 获得大小为1的张量的值
```

## 数据预处理

使用 pandas 库进行数据预处理是 python 中最常用的方法

### 读取数据集

```python
import pandas as pd
data = pd.read_csv("filepath") # 使用pandas来读取数据
```

### 处理缺失值

```python
data.fillna(data.mean()) # 使用fillna方法来填补缺失值
inputs = pd.get_dummies(data, dummy_na = True) # 将data中的离散值的类别转换为01的形式

output[1]:
    Alley_Pave    Alley_nan
0            1            0
1            1            0
2            0            1
```

### 转换成张量的格式

```python
# 使用torch.tensor来将dataFrame中的列转换为张量
X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)
```

## 线性代数

### 标量

```python
x = torch.tensor(3.0) # 输入一个值以创建一个标量
```

### 向量

```python
x = torch.arange(4) # 创建一个向量
x[3] # 使用索引来获得向量中的任意元素
len(x) # 返回x的长度，int型
x.shape # 返回x的长度，torch.Size类型
```

### 矩阵

```python
A = torch.arange(20).reshape(5, 4) # 创建一个矩阵
A.T # 矩阵的转置
```

### 张量

```python
A = torch.arange(24).reshape(2, 3, 4) # 创建一个张量，即多维矩阵
A + A # 对应元素相加，对于乘法、减法、除法等有相同的性质
A * 3 # 与标量运算相当于对每个元素进行运算
```

### 降维

对张量进行集合运算即可将张量降维，可以使用一些参数来指定维数。

```python
x.sum() # 返回x中所有值的和
x.sum(axis = 0) # 将x中的每一行求和
x.mean() # 求均值
x.sum(axis=1, keepdims=True) # 在降维后保存轴数不变
x.cumsum(axis=0) # 沿行计算x的元素的累计总和，该函数不会降低维数
```

### 点积

```python
x = torch.ones(4, dtype = torch.float32)
y = torch.arange(4, dtype = torch.float32)
torch.dot(x, y) # 计算向量x与y的点积，即向量内积
torch.sum(x * y) # 也可以计算内积
```

### 矩阵-向量积

假设矩阵 $A$ 的每一行都是一个向量，即：
$$
A = \begin{bmatrix}a_1^T\\a_2^T\\\vdots\\ a_m^T\end{bmatrix}
$$
则矩阵-向量积为：
$$
Ax = \begin{bmatrix}a_1^Tx\\a_2^Tx\\\vdots\\a_m^Tx\end{bmatrix}
$$

```python
torch.mv(A, x) # 计算矩阵-向量积 Ax
```

### 矩阵-矩阵乘法

```python
A = torch.ones(3, 4)
B = torch.ones(4, 5)
torch.mm(A, B) # 计算矩阵乘法 AB
```

### 范数

范数表示向量的大小，满足下面的性质：
- $f(\alpha x) = |\alpha|f(x)$
- $f(x+y)\le f(x) +f(y)$
- $f(x) \ge 0$

深度学习中常使用二范数：
$$
|\!|x|\!|_2 = \sqrt{\sum_{i=1}^nx_i^2}
$$
二范数常省略下标，记为 $||x||$。
对于矩阵，二范数使用 Forbenius norm，即：
$$
|\!|X|\!|_F=\sqrt{\sum_{i=1}^m\sum_{i=1}^nx_{ij}^2}
$$

```python
u = torch.tensor([3.0, -4.0])
torch.norm(u) # 计算u的二范数
torch.abs().sum() # 计算u的一范数
torch.norm(torch.ones(4, 9)) # 计算矩阵的二范数
```

### 微积分

深度学习中通过自动微分来加快求导过程。根据设计好的模型，系统会构建一个计算图，通过反向传播算法来获得每个参数的偏导数。

```python
x = torch.arange(4)
x.requires_grad_(True)
x.grad # 默认值为空
y = 2 * torch.dot(x, x)
y.backward() # 调用反向传播函数计算梯度
x.grad # 得到x的梯度
```

对于非标量的反向传播，需要传入一个 gardient 参数，指明微分函数关于 self 的梯度。

### 分离计算

```python
x.grad.zero_()
y = x * x
u = y.detach() # 该操作将u看作常数，不列入反向传播的计算
z = u * x
```

### Python 控制流的梯度计算

使用自动微分的一个好处是可以计算 python 控制流的导数，我们可以任意的使用 if-else 和循环语句，系统将会帮助我们求出梯度的值。

## 概率

概率的公理、定义，详细见概率论，这里不过多赘述。

## 查阅文档

在 Juypter 中，我们可以使用 ? 命令在另一个窗口中显示文档，例如 `list?` 将创建同 `help(list)` 的内容，并在新的浏览器窗口显示它。如果我们使用 `list??`，将会显示它的 python 代码。