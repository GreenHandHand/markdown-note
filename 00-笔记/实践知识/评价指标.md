```python
import micropip
await micropip.install('numpy')
await micropip.install('scikit-learn')
await micropip.install('matplotlib')
await micropip.install('seaborn')
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics
```

# 评价指标

评价指标 (evaluation metrics) 是评估结果的工具。现实中，不同的问题通常对应着不同的评价指标，人们甚至会更加具体的问题来建立相应的指标。

这里介绍一些最常用的一般评价指标。这里从分类开始，首先考虑二分类，之后再推广到多分类中。之后再讨论一些回归任务中的评价指标。

## 分类任务

### 混淆矩阵

混淆矩阵（Confusion Matrix）是一种用于评估模型性能的常用工具。在二分类任务中，混淆矩阵是一个二维表格，展示如下：

|            | y_pred (-)    | y_pred (+)     |
| ---------- | ------------- | -------------- |
| y_true (-) | False Negative | True Negative |
| y_true (+) | False Positive | True Positive               |
具体的，每一个元素代表：
- True Negative (TN) 表示模型将负类别正确地预测为负类别的样本数。
- False Positive (FP) 表示模型将负类别错误地预测为正类别的样本数。
- False Negative (FN) 表示模型将正类别错误地预测为负类别的样本数。
- True Positive (TP) 表示模型将正类别正确地预测为正类别的样本数。

在数学中，我们将 FP 称为第一类错误，将 FN 称为第二类错误。

或者对于多分类任务，我们可以得到的一个更大的混淆矩阵：
![[Assets/Pasted image 20230731111732.png]]

使用混淆矩阵而不是直接使用正确性（accuracy）的原因是，混淆矩阵可以提供更详细、更全面的分类性能评估，尤其是在面对不平衡数据集或具有多个类别的情况下。混淆矩阵提供了一种更细致、更全面的方式来评估模型的分类性能，特别适用于处理类别不平衡、多类别或需要对不同类型的误分类进行分析的情况。

sklearn 中直接提供了混淆矩阵的计算接口。并且我们通过 seaborn 中的热力图将其绘制出来。
```python
y_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]
y_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]

cm = metrics.confusion_matrix(y_true, y_pred)
fig = plt.figure()
sns.heatmap(cm, annot=True, cbar=False)
plt.show()

print(cm)
```

上述值称为一级指标，不能直接看出模型的优劣。因此我们通常计算二级指标与三级指标来对模型进行评估。

### 二级指标

### 正确率/准确率 (Accuracy)

正确率就是判断结果正确的数目占所有的数目的比重。其计算方式为：
$$
Accuracy=\frac{\text{TP+TN}}{\text{TP+TN+FP+FN}}
$$

简单来说，正确率就是计算
$$
\text{正确率ACC}=\frac{\text{对角线之和}}{\text{矩阵元素之和}}
$$
sklearn 中提供了独立的接口函数，即
```python
l1 = [0, 1, 1, 1, 0, 0, 0, 1]
l2 = [0, 1, 0, 1, 0, 1, 0, 0]
print(metrics.accuracy_score(l1, l2))
```

正确率是我们在分类任务中最为常用的指标，因为它的含义直观。但是在样本类别不均匀的时候，仅使用正确率可能会导致错误的结果。假设在样本大小为 100 的样本中包含了 90 个正类，我们的模型不论输入什么，都只会输出正类的预测，当我们对其计算正确率时，得到的正确率将会是 90\%，这样的结果显然是不合理的。为了得到正确的预测，就涉及到了混淆矩阵中的其他值。

### 精确率 (Precision)

对于二分类而言，精确率表示预测为 Positive 的结果中，模型预测正确的占比。即
$$
Precision=\frac{\text{TP}}{\text{TP+FP}}
$$
对于多分类而言，每一类都可以计算其精确率。每一类的精确率表示为所有预测为该类的结果中，预测正确的占比。简单来说，每一类的精确率就是计算：
$$
\text{类别i的精确率PPV}=\frac{混淆矩阵第i行第i列}{混淆矩阵第i列之和}
$$

### 召回率/灵敏度 (Recall/Sensitivity)

对于二分类而言，召回率表示真实类别为 Positive 的结果中，模型预测正确的占比。即
$$
Recall=\frac{\text{TP}}{\text{TP+FN}}
$$
对于多分类而言，每一类都可以计算其召回率。每一类的召回率表示为所有真实值为该类的结果中，预测正确的占比。简单来说，每一类的召回率就是计算：
$$
\text{类别i的召回率TPR}=\frac{混淆矩阵第i行第i列}{混淆矩阵第i行之和}
$$

### 特异度 (Specificity)

对于二分类而言，特异度表示真实值为 Negative 的结果中，模型预测正确的占比。即
$$
Specificity=\frac{\text{TN}}{\text{TN+FP}}
$$
对于多分类而言，每一类都可以计算其特异度。每一类的特异度为所有负样本中，预测为负的占比。简单来说，计算特异度就是计算：
$$
\tiny\text{类别i的特异度TNR}=\frac{混淆矩阵不包含第i行与第i列的其他元素之和}{混淆矩阵不包含第i行的其他元素之和}
$$

### F1 score

F1 分数是综合衡量精确率和召回率的指标，F1 越大，说明模型越好。其计算方式为
$$
\text{F1}=\frac{2}{\frac{1}{Precision}+\frac{1}{Recall}}=\frac{2\cdot Precision\cdot Recall}{Precision+Recall}
$$
该函数在 sklearn 中有直接的接口。
```python
y_true = [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]
y_pred = [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1]
print(metrics.f1_score(y_true, y_pred))
```

### ROC 曲线与 AUC

ROC 曲线 (Receiver Operating Characteristic) 与 AUC (Area Under ROC Curve) 也是常用的评价指标。对于一组不同的超参数，我们可以绘制他们的 precision-recall 曲线，一个分类性能好的模型应当同时具有较高的精确率与召回率，但是实际上我们得到的曲线一般的是一个反比例函数的曲线。一个完美的曲线形状应该为
![[Assets/Pasted image 20240116215757.png]]

与上面的曲线类似的，我们还可以绘制 tpr-fpr 曲线，其中 tpr 就是召回率，fpr 为 1-tnr，这样得到的曲线有一个更加著名的名字，即 ROC 曲线 (Receiver Operating Characteristic)，同时，可以得到相应的一个评价指标为 AUC (Area Under ROC Curve)，AUC 有以下几个性质：
1. AUC=1 表示完美的模型，大多数情况下，出现这样的值表明要么数据出现问题，要么在验证时使用了错误的方法。
2. AUC=0 表示模型预测非常差，预测几乎都是错的。出现这种情况可能是由于训练集与测试集的正例与反例反了。
3. AUC=0.5 表示模型正在随机预测。如果 AUC 曲线低于了 0.5，说明模型的预测正确率低于随机预测，大多数情况下，翻转预测结果，你的预测准确率可能会大于 0.5。

这里用一个例子说明 AUC 的意义，假设使用一个二分类模型的 AUC 为 0.85，那么当你有一个正例与一个反例，模型预测正例分数大于反例分数的概率为 0.85。

### 对数损失

对数损失 (log loss) 也是一种常用于二分类的指标，其计算方式如下所示：
$$
\tiny LogLoss = -(target\times \log(prediction)  +(1-target)\times \log(1-prediction))
$$
其中 $target$ 是 0 或者 1，$prediction$ 是一个预测的概率值。上式实际上就是交叉熵损失在二分类情况下的特殊情况。

### 推广到多分类

之前的所有的都可以非常容易的推广到多分类任务中。但是还有几个问题需要考虑，以计算精确率为例，我们可以通过混淆矩阵得到每一类的 TP 和 FP，那么一个模型的精确率可以使用下面的方式计算：
1. 宏观平均精度 (Macro averaged precision)：独立计算每一类的精度，然后计算它们的均值。
2. 微观平均精度 (Micro averaged precision)：计算广义的 TP 与 FP，并使用它们计算精度。
3. 加权精度 (Weighted precision)：类似与宏观平均精度，但是每一类的权重取决于这一类的样本数量。

在 sklearn 中，提供了每一种方法的接口，只需要指定参数 average 即可。
```python
y_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]
y_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]
print('macro:', metrics.precision_score(y_true, y_pred, average='macro'))
print('micro', metrics.precision_score(y_true, y_pred, average='micro'))
print('weighted', metrics.precision_score(y_true, y_pred, average='weighted'))
```

类似的，我们可以使用相同的方式计算召回率、f1 分数等其他指标。

### 多标签分类

之前讨论的所有评价指标都是针对单类别的任务。现实中，存在另一种分类任务，称为多标签分类任务 (multi-label classification)。在多标签分类任务中，每个样本都可以联系到一个以上的类别。例如，给定一张图片，预测其中的物体。在这样的问题中，指标需要对比的对象从两个值变成了两个集合。

对于多标签分类任务，使用的评价指标将会有一些变化。这里了解一些常用的指标。

#### Precision at k

precision at k or P@K 定义为预测类别列表中前 k 个在真实类别列表中的命中次数除以 k。使用代码可以更加清晰的描述：
```python
def pk(y_true, y_pred):
	if k == 0: return 0
	y_pred = y_pred[:k]
	pred_set = set(true_set)
	true_set = set(true_set)
	common_values = pred_set.intersection(true_set)
	return len(common_values) / len(y_pred[:k])
```

类似的，我们可以定义平均值 average precision at k 简写为 AP@k 定义为
$$
\text{AP@k} = \sum_{i=1}^k \text{P@i}
$$
最后，定义 mean average precision at k 简写为 MAP@k 定义为整个数据集中的 AP@k 的均值。

#### 多标签对数损失

对数损失的推广非常直观。根据对数损失的定义，我们可以将目标类别转换为多个 one-hot 向量的和，并使用相同的方式计算损失。

## 回归

### 常见误差函数

回归任务中的评价指标都比较直接。首先是误差 (error)，描述为
$$
\text{Error} = \text{True Value} - \text{Predicted Value}
$$
对误差取绝对值，称为绝对误差：
$$
\text{Absolute Error}=|\text{True Value}-\text{Predicted Value}|
$$
于是可以得到第一个评价指标为**平均绝对误差** (mean absolute error, MAE)，就是对整个数据集中的绝对误差取平均值。
$$
loss = \frac{1}{N}\sum_{i=1}^{N}|y_i-\hat y_i|
$$
类似的，常用的还有平均平方误差 (mean squared error, MSE)，定义为
$$
loss = \frac{1}{N}\sum_{i=1}^{N}(y_i-\hat y_i)^2
$$
均方根误差 (root mean squared error, RMSE) 是在回归任务中使用最多的评价指标
$$
\text{RMSE}=\sqrt{\text{MSE}}
$$

在对数尺度上，定义了平方对数误差 (squared logarithmic error, SLE) 对结果先取对数再作差。得到相应的均方对数误差 (mean squared logarithmic error, MSLE) 为
$$
loss = \frac{1}{N}\sum_{i=1}^{N}\left(\log(1+y_i)-\log(1+\hat y_i)\right)^2
$$
对其取根号得到均方根对数误差 (root mean squared logarithmic error, RMSLE)。 

最后，可以使用不同的误差定义方式来计算评价指标，例如百分比误差 (Percentage error, PE)：
$$
\small \text{Percentage Error} = \frac{(\text{True Value}- \text{Predicted Value})}{\text{True Value}} *100
$$
可以定义类似的 MAPE, MSPE 等。

### 其他指标

回归的一个好处是通用的评价指标较少，并且相较于分类任务更好理解。最后这里介绍一些更加复杂的评价指标。

R<sup>2</sup> 也称为**拟合系数** (coefficient of determination)，其计算方式为
$$
R^2=1-\frac{\sum_{i=1}^{N}(y_{t_i} - y_{p_i})^2}{\sum_{i=1}^N(y_{t_i}-y_{t_{mean}})}
$$
R<sup>2</sup> 越接近 1 表明模型拟合数据效果越好，越接近 0 表明模型效果越差。当模型进行荒谬的预测时，R<sup>2</sup> 也可以是负数。

**二次加权卡帕** (quadratic weighted kappa)，也被称为 QWK 或者科恩卡帕 (Cohen's kappa)。其可以描述两个分数之间的一致性，该函数在 sklearn 中有很好的实现，其具体的计算方式这里不介绍。
```python
y_true = [1, 2, 3, 1, 2, 3, 1, 2, 3]
y_pred = [2, 1, 3, 1, 2, 3, 3, 1, 2]
print(metrics.cohen_kappa_score(y_true, y_pred, weights='quadratic'))
```
一个大于 0.85 的 QWK 被视为一个非常好的模型。

马修相关系数 (Matthew's Correlation Coefficient, MCC) 是范围在 -1 到 1 之间的数，1 代表完美的预测，-1 代表完全错误的预测，0 代表随机的预测。其计算方式为
$$
\tiny \text{MCC}=\frac{TP\times TN-FP\times FN}{\sqrt{(TP+FP)\times (FN+TN)\times (FP+TN)\times (TP+FN)}}
$$

## 对于无监督算法

上面提到的所有评价指标都是建立在有监督的情况下，对于无监督的问题，我们最好的选择是手动建立一个测试数据集，并进行标注，使用监督学习中的评价指标来对其进行评估。